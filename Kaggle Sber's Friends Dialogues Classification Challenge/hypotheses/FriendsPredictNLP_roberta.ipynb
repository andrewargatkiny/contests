{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":304,"status":"ok","timestamp":1636037468369,"user":{"displayName":"Andrew Argatkiny","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04422905123959661601"},"user_tz":-420},"id":"jfX9Mo2IRKkS","outputId":"72476e94-f6d5-4f3f-9289-95aba0fefb20"},"outputs":[{"name":"stdout","output_type":"stream","text":["Python 3.9.7\n"]}],"source":["! python -V"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformers\n","  Downloading transformers-4.12.3-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 626 kB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/andrew/anaconda3/envs/nn/lib/python3.9/site-packages (from transformers) (1.21.2)\n","Collecting filelock\n","  Downloading filelock-3.3.2-py3-none-any.whl (9.7 kB)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (661 kB)\n","\u001b[K     |████████████████████████████████| 661 kB 9.8 MB/s \n","\u001b[?25hCollecting regex!=2019.12.17\n","  Downloading regex-2021.11.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (762 kB)\n","\u001b[K     |████████████████████████████████| 762 kB 9.7 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.1.0-py3-none-any.whl (59 kB)\n","\u001b[K     |████████████████████████████████| 59 kB 4.7 MB/s \n","\u001b[?25hCollecting requests\n","  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n","\u001b[K     |████████████████████████████████| 62 kB 712 kB/s \n","\u001b[?25hCollecting tqdm>=4.27\n","  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n","\u001b[K     |████████████████████████████████| 76 kB 3.7 MB/s \n","\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 9.8 MB/s \n","\u001b[?25hCollecting packaging>=20.0\n","  Downloading packaging-21.2-py3-none-any.whl (40 kB)\n","\u001b[K     |████████████████████████████████| 40 kB 4.5 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 10.4 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /home/andrew/anaconda3/envs/nn/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Collecting pyparsing<3,>=2.0.2\n","  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 4.4 MB/s \n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /home/andrew/anaconda3/envs/nn/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n","Collecting charset-normalizer~=2.0.0\n","  Downloading charset_normalizer-2.0.7-py3-none-any.whl (38 kB)\n","Collecting urllib3<1.27,>=1.21.1\n","  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n","\u001b[K     |████████████████████████████████| 138 kB 10.4 MB/s \n","\u001b[?25hCollecting idna<4,>=2.5\n","  Downloading idna-3.3-py3-none-any.whl (61 kB)\n","\u001b[K     |████████████████████████████████| 61 kB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: joblib in /home/andrew/anaconda3/envs/nn/lib/python3.9/site-packages (from sacremoses->transformers) (1.1.0)\n","Collecting click\n","  Downloading click-8.0.3-py3-none-any.whl (97 kB)\n","\u001b[K     |████████████████████████████████| 97 kB 4.7 MB/s \n","\u001b[?25hRequirement already satisfied: six in /home/andrew/anaconda3/envs/nn/lib/python3.9/site-packages (from sacremoses->transformers) (1.16.0)\n","Installing collected packages: urllib3, pyparsing, idna, charset-normalizer, tqdm, requests, regex, pyyaml, packaging, filelock, click, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyparsing\n","    Found existing installation: pyparsing 3.0.4\n","    Uninstalling pyparsing-3.0.4:\n","      Successfully uninstalled pyparsing-3.0.4\n","Successfully installed charset-normalizer-2.0.7 click-8.0.3 filelock-3.3.2 huggingface-hub-0.1.0 idna-3.3 packaging-21.2 pyparsing-2.4.7 pyyaml-6.0 regex-2021.11.2 requests-2.26.0 sacremoses-0.0.46 tokenizers-0.10.3 tqdm-4.62.3 transformers-4.12.3 urllib3-1.26.7\n"]}],"source":["! pip install transformers"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":28083,"status":"ok","timestamp":1636037505252,"user":{"displayName":"Andrew Argatkiny","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04422905123959661601"},"user_tz":-420},"id":"JplCyr0VKrhP"},"outputs":[],"source":["import random \n","import time\n","import copy\n","\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import torch\n","import torch.nn as nn\n","from torch.cuda.amp import autocast, GradScaler\n","\n","from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n","from transformers import BertTokenizer, BertModel, AdamW\n","from transformers import RobertaTokenizer, RobertaModel\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","\n","from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["torch.cuda.is_available()"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":209,"referenced_widgets":["adef346525b5428cb77ce85b67b7de02","632a7169fbd74159b11d92ab6cbacfa7","f32062e1b8f74b2ba3ebb65244b8345d","21bf688381ef4a8495a3a6981a84ef8d","d601368d185f4c2f8de3b222976c42ef","a825221a71434b82be79352ad867c68a","52e9ba2a4d974b15ae4b6ab1232dc256","6c7f0e73512743efb870026d6677531e","939389df612a4f23b8ab42d6dccd6c8d","769e20765ef64788b8de218a9853c99e","32acd792c0bf45419a87561dc4ccdb4c","79f29a0c5f01482dbbb7700bd7b573a7","d469fb245e2244848f21d12c9897e0e8","4718ae66701049229c66c8506b3efe1b","bddc64019a20420589041d59a8ac1e86","158f6d85beec4b33bdc422ee9e115808","d70b870ad108499e8651b57c9e9be34b","65cbb712ea2f4dab90ba6600d8440c0b","830958dc67714696a237fc7a051931bc","2d185bee7972497296fc31ba5c0d54d6","faaf710ce2cc471c9370811df2f54ba6","2e6a2c5306764e31bea3a9e37121ff8c","a56c41e3f2744ba6b2f5cd408a86e6d7","d15f42cd6e9d427c99f12b45ba2b04e1","8e60d43dcd7e4df9a554b7b8ed28c56d","ebbe8ae171e94413be8fcc944c05b28b","c280900ff8ba4cc8a2dbf30daad33a4b","192f33925dd045d792b38a2444794a10","a7c1b532fc64473eab127d4f73c4ffd7","d5ed4335e6bb418aaebd5316b616f54c","6502e372ae624b618f6248ee2ea778b6","4e0b4d1769974ba88e49d4fb02e2c4a2","317c759beaf6494db054eae9162ccfb2","a89df2c0f77a445eb9a12550fcaed50d","ee730afc554f486591437c1c9de33490","6cd8338fff6a4f469d417911e8b8142b","edd582160bd349c68d35b5461b9771c2","f1868ee80eff44af8596b13875f7cddd","7148c15a118641d3a9488fd981cc3bf3","54feb1dd180f4bf8b47c18713336d41c","58c863f270ac42c3b195987575a8bc9a","784667efc3604b52a7a3e90afeeba909","5e3622f69800454d9704f414226d5869","6c7b0a0b13b3450fa83c21af7c8525a2","e9d9038ad82440aebdb0fb714add647d","85aa7a96758841a3bb30895cf30d9912","717056921033432aa1612afdd7aab763","f010001c717b439797f738bb4353b74d","0f3925eaafb9438ab0f6b27c356cfe99","58564c36a43e4f3ab99a0e153cb5e480","8d900d48dde840e0824e409582df0dd2","5fa6528c3aa5487e87edb8797ebf465e","d3ce46e504bd4daf9ac3763a7cdc222f","487d23c4e2b64adda4a945df313d1ed1","82b0759acdb845ea9859d2f96ce0e5d3","e87825600783430391ed6af6be32c278","b9b60187d34644bda94b224017bc450d","73c9f7d056704fd2ac03cb76d6f36df5","972de0d98911484dad5f2c7b560d973c","0344eb1638a44fc0bd8be3d2b5aafc99","df27936694364ed583b70df9a0870a08","7d83773323bd4e90bc86003c4220a662","f543e115e78f40f7bc9813a76b5ba862","269978945c9c48d280cd1eb1ddf2f7fc","91647e75ea5e4c16a11046fda73ba026","e2cbc04f6abc4b45b5374f424dd88efa"]},"executionInfo":{"elapsed":4421,"status":"ok","timestamp":1636037509668,"user":{"displayName":"Andrew Argatkiny","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04422905123959661601"},"user_tz":-420},"id":"o24cy8ORGJjU","outputId":"2c57e157-d12e-41c3-9631-cd59f18b134b"},"outputs":[],"source":["#CHECKPOINT = \"sberbank-ai/sbert_large_nlu_ru\"\n","MAX_LEN = 256\n","TRAIN_BATCH_SIZE = 16\n","VALID_BATCH_SIZE = 16\n","EPOCHS = 5\n","LEARNING_RATE = 2e-05\n","#tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n","CHECKPOINT = \"roberta-base\"\n","tokenizer = RobertaTokenizer.from_pretrained(CHECKPOINT)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1636037511385,"user":{"displayName":"Andrew Argatkiny","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04422905123959661601"},"user_tz":-420},"id":"iuShp08vL62l","outputId":"885c5ad6-61d9-40c9-ae33-5be1da37a048"},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/andrew/ml/friends-classification\n","mkdir: cannot create directory ‘models’: File exists\n","example_eng_ru.csv\n","fb_model_translate_en_ru_2_friend_response_test.csv\n","fb_model_translate_en_ru_2_friend_response_train.csv\n","fb_model_translate_en_ru_2_friend_response_val.csv\n","fb_model_translate_en_ru_2_other_speaker_test.csv\n","fb_model_translate_en_ru_2_other_speaker_train.csv\n","fb_model_translate_en_ru_2_other_speaker_val.csv\n","fb_model_translate_ru_en_1_friend_response_test.csv\n","fb_model_translate_ru_en_1_friend_response_train.csv\n","fb_model_translate_ru_en_1_friend_response_val.csv\n","fb_model_translate_ru_en_1_other_speaker_test.csv\n","fb_model_translate_ru_en_1_other_speaker_train.csv\n","fb_model_translate_ru_en_1_other_speaker_val.csv\n","helsinki_model_translate_ru_en_1_friend_response_test.csv\n","helsinki_model_translate_ru_en_1_friend_response_train.csv\n","helsinki_model_translate_ru_en_1_friend_response_val.csv\n","helsinki_model_translate_ru_en_1_other_speaker_test.csv\n","helsinki_model_translate_ru_en_1_other_speaker_train.csv\n","helsinki_model_translate_ru_en_1_other_speaker_val.csv\n","models\n","submission1.csv\n","test.csv\n","test_data_eng_fb_model.csv\n","test_data_eng_google_model.csv\n","test_data_eng_helsinki_model.csv\n","test_data_rus_fb_model.csv\n","train_data.csv\n","train_data_eng_fb_model.csv\n","train_data_eng_google_model.csv\n","train_data_eng_helsinki_model.csv\n","train_data_rus_fb_model.csv\n","val_data.csv\n","val_data_eng_fb_model.csv\n","val_data_eng_google_model.csv\n","val_data_eng_helsinki_model.csv\n","val_data_rus_fb_model.csv\n"]}],"source":["%cd friends-classification/\n","!mkdir models\n","! ls"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":459,"status":"ok","timestamp":1636037511841,"user":{"displayName":"Andrew Argatkiny","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04422905123959661601"},"user_tz":-420},"id":"acv7o-iyMo2n","outputId":"e1180782-7199-4b5b-ef05-41f2f3d4d114"},"outputs":[{"name":"stdout","output_type":"stream","text":["РОСС       0.176569\n","РЕЙЧЕЛ     0.176089\n","ЧЕНДЛЕР    0.170568\n","ДЖОУИ      0.166287\n","МОНИКА     0.160525\n","ФИБИ       0.149962\n","Name: label, dtype: float64\n","\n","РОСС       0.176746\n","РЕЙЧЕЛ     0.176026\n","ЧЕНДЛЕР    0.170626\n","ДЖОУИ      0.166307\n","МОНИКА     0.160547\n","ФИБИ       0.149748\n","Name: label, dtype: float64\n"]}],"source":["df_train = pd.read_csv('train_data.csv').rename({'Category': 'label'}, axis=1)\n","df_train.other_speaker.fillna('', inplace=True)\n","df_val = pd.read_csv('val_data.csv')\n","df_val.other_speaker.fillna('', inplace=True)\n","df_test = pd.read_csv('test.csv')\n","df_test.other_speaker.fillna('', inplace=True)\n","\n","# Encoding target variable\n","names_to_cats = LabelEncoder()\n","df_train['label_code'] = names_to_cats.fit_transform(df_train.label)\n","df_val['label_code'] = names_to_cats.transform(df_val.label)\n","df_fb_train = pd.read_csv('train_data_rus_fb_model.csv')\n","df_full = pd.concat([df_train, df_val])\n","print(df_train[\"label\"].value_counts()/df_train.shape[0])\n","print()\n","print(df_val[\"label\"].value_counts()/df_val.shape[0])"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":459,"status":"ok","timestamp":1636037511841,"user":{"displayName":"Andrew Argatkiny","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04422905123959661601"},"user_tz":-420},"id":"acv7o-iyMo2n","outputId":"e1180782-7199-4b5b-ef05-41f2f3d4d114"},"outputs":[{"name":"stdout","output_type":"stream","text":["РОСС       0.176569\n","РЕЙЧЕЛ     0.176089\n","ЧЕНДЛЕР    0.170568\n","ДЖОУИ      0.166287\n","МОНИКА     0.160525\n","ФИБИ       0.149962\n","Name: label, dtype: float64\n","\n","РОСС       0.176746\n","РЕЙЧЕЛ     0.176026\n","ЧЕНДЛЕР    0.170626\n","ДЖОУИ      0.166307\n","МОНИКА     0.160547\n","ФИБИ       0.149748\n","Name: label, dtype: float64\n"]}],"source":["df_train = pd.read_csv('train_data_eng_fb_model.csv')\n","df_train.other_speaker.fillna('', inplace=True)\n","df_val = pd.read_csv('val_data_eng_fb_model.csv')\n","df_val.other_speaker.fillna('', inplace=True)\n","df_test = pd.read_csv('test_data_eng_fb_model.csv')\n","df_test.other_speaker.fillna('', inplace=True)\n","\n","df_train2 = pd.read_csv('train_data_eng_helsinki_model.csv')\n","df_train2.other_speaker.fillna('', inplace=True)\n","df_val2 = pd.read_csv('val_data_eng_helsinki_model.csv')\n","df_val2.other_speaker.fillna('', inplace=True)\n","df_test2 = pd.read_csv('test_data_eng_helsinki_model.csv')\n","df_test2.other_speaker.fillna('', inplace=True)\n","\n","df_train3 = pd.read_csv('train_data_eng_google_model.csv')\n","df_train3.other_speaker.fillna('', inplace=True)\n","df_val3 = pd.read_csv('val_data_eng_google_model.csv')\n","df_val3.other_speaker.fillna('', inplace=True)\n","df_test3 = pd.read_csv('test_data_eng_google_model.csv')\n","df_test3.other_speaker.fillna('', inplace=True)\n","\n","# Encoding target variable\n","names_to_cats = LabelEncoder()\n","df_train['label_code'] = names_to_cats.fit_transform(df_train.label)\n","df_val['label_code'] = names_to_cats.transform(df_val.label)\n","df_fb_train = pd.read_csv('train_data_rus_fb_model.csv')\n","df_full = pd.concat([df_train, df_val])\n","print(df_train[\"label\"].value_counts()/df_train.shape[0])\n","print()\n","print(df_val[\"label\"].value_counts()/df_val.shape[0])"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["df_train_2X = pd.concat([df_train, df_train2])\n","df_train_3X = pd.concat([df_train, df_train2, df_train3])\n","df_val_3X = pd.concat([df_val, df_val2, df_val3])"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":436},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1636037511842,"user":{"displayName":"Andrew Argatkiny","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04422905123959661601"},"user_tz":-420},"id":"q87GsjIbOl_e","outputId":"382b8f47-59e7-47df-8e3c-c137d3c72bc1"},"outputs":[{"name":"stdout","output_type":"stream","text":["(24993, 5) (2778, 5) (3086, 3)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>other_speaker</th>\n","      <th>friend_response</th>\n","      <th>label</th>\n","      <th>label_code</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>Good, and we don't eat at all?</td>\n","      <td>Come on, it's time to be serious, to fight the...</td>\n","      <td>МОНИКА</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>All right, all right, listen, I know I'm Mr. I...</td>\n","      <td>Good, here it is! Get off it!</td>\n","      <td>РОСС</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>Yeah. You know, I have all these feelings, and...</td>\n","      <td>Well, I saw a pretty big pigeon.</td>\n","      <td>ДЖОУИ</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>My God! Just a few minutes ago, and now I am.</td>\n","      <td>Wait, you can't give birth here! That is, I di...</td>\n","      <td>МОНИКА</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>Not as he is, just not as he is.</td>\n","      <td>Look, this is an artist formerly known as Chan...</td>\n","      <td>РОСС</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2773</th>\n","      <td>2773</td>\n","      <td>What's going on tonight?</td>\n","      <td>This is our first official date. Our first date.</td>\n","      <td>РЕЙЧЕЛ</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2774</th>\n","      <td>2774</td>\n","      <td>Hey, aren't you dressed up?</td>\n","      <td>Yes, and this time you better make sure that h...</td>\n","      <td>РЕЙЧЕЛ</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2775</th>\n","      <td>2775</td>\n","      <td>You were not there!</td>\n","      <td>No, but it's, you know, just a funny picture, ...</td>\n","      <td>РОСС</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2776</th>\n","      <td>2776</td>\n","      <td>You are talking on the phone!</td>\n","      <td>That was the fire part, we had a fire!</td>\n","      <td>РЕЙЧЕЛ</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2777</th>\n","      <td>2777</td>\n","      <td>I'm sorry.</td>\n","      <td>At least I earned ten dollars on my relationsh...</td>\n","      <td>РОСС</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2778 rows × 5 columns</p>\n","</div>"],"text/plain":["        Id                                      other_speaker  \\\n","0        0                     Good, and we don't eat at all?   \n","1        1  All right, all right, listen, I know I'm Mr. I...   \n","2        2  Yeah. You know, I have all these feelings, and...   \n","3        3      My God! Just a few minutes ago, and now I am.   \n","4        4                   Not as he is, just not as he is.   \n","...    ...                                                ...   \n","2773  2773                           What's going on tonight?   \n","2774  2774                        Hey, aren't you dressed up?   \n","2775  2775                                You were not there!   \n","2776  2776                      You are talking on the phone!   \n","2777  2777                                         I'm sorry.   \n","\n","                                        friend_response   label  label_code  \n","0     Come on, it's time to be serious, to fight the...  МОНИКА           1  \n","1                         Good, here it is! Get off it!    РОСС           3  \n","2                      Well, I saw a pretty big pigeon.   ДЖОУИ           0  \n","3     Wait, you can't give birth here! That is, I di...  МОНИКА           1  \n","4     Look, this is an artist formerly known as Chan...    РОСС           3  \n","...                                                 ...     ...         ...  \n","2773   This is our first official date. Our first date.  РЕЙЧЕЛ           2  \n","2774  Yes, and this time you better make sure that h...  РЕЙЧЕЛ           2  \n","2775  No, but it's, you know, just a funny picture, ...    РОСС           3  \n","2776             That was the fire part, we had a fire!  РЕЙЧЕЛ           2  \n","2777  At least I earned ten dollars on my relationsh...    РОСС           3  \n","\n","[2778 rows x 5 columns]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["print(df_train.shape, df_val.shape, df_test.shape)\n","df_val"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":264,"status":"ok","timestamp":1636041805080,"user":{"displayName":"Andrew Argatkiny","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04422905123959661601"},"user_tz":-420},"id":"6fwxjlLEGzIT"},"outputs":[],"source":["class FriendsDataset(Dataset):\n","    def __init__(self, dataframe, tokenizer, max_length=512, padding='max_length', \n","                 with_labels=True):\n","\n","        self.dataframe = dataframe  # pandas dataframe\n","        #Initialize the tokenizer\n","        self.tokenizer = tokenizer  \n","        self.padding = padding\n","        self.max_length = max_length\n","        \n","        self.with_labels = with_labels \n","        if 'label' not in self.dataframe.columns:\n","          self.with_labels = False\n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","\n","    def __getitem__(self, index):\n","\n","        # Selecting sentence1 and sentence2 at the specified index in the data frame\n","        sent1 = self.dataframe.other_speaker.iloc[index]\n","        sent2 = self.dataframe.friend_response.iloc[index]\n","\n","        # Tokenize the pair of sentences to get token ids, attention masks and token type ids\n","        encoded_pair = self.tokenizer(sent1, sent2, \n","                                      padding=self.padding,  # Pad to max_length\n","                                      truncation=True,  # Truncate to max_length\n","                                      max_length=self.max_length,  \n","                                      return_tensors='pt')  # Return torch.Tensor objects\n","        \n","        token_ids = encoded_pair['input_ids'].squeeze(0)  # tensor of token ids\n","        attn_masks = encoded_pair['attention_mask'].squeeze(0)  # binary tensor with \"0\" for padded values and \"1\" for the other values\n","        #token_type_ids = encoded_pair['token_type_ids'].squeeze(0)  # binary tensor with \"0\" for the 1st sentence tokens & \"1\" for the 2nd sentence tokens\n","\n","        if self.with_labels:  # True if the dataset has labels\n","            label = self.dataframe.label_code.iloc[index]\n","            return token_ids, attn_masks,  label  \n","        else:\n","            return token_ids, attn_masks#, token_type_ids"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":282},"executionInfo":{"elapsed":544,"status":"ok","timestamp":1636011627375,"user":{"displayName":"Andrew Argatkiny","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04422905123959661601"},"user_tz":-420},"id":"ERDB6JyobclV","outputId":"0fe39def-fc13-40f3-fb72-c8dcfd8ecafe"},"outputs":[{"data":{"text/plain":["<AxesSubplot:>"]},"execution_count":10,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["seq_len = [len(i.split()) for i in df_train.other_speaker.fillna('')]\n","\n","pd.Series(seq_len).hist(bins = 30)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":469},"executionInfo":{"elapsed":644,"status":"ok","timestamp":1636011628318,"user":{"displayName":"Andrew Argatkiny","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04422905123959661601"},"user_tz":-420},"id":"8vdS5P-2az0b","outputId":"5e99479f-a945-443c-97a5-ec54b965acef"},"outputs":[{"name":"stdout","output_type":"stream","text":["30857 number of all dialogs in train, validation and test\n"]},{"data":{"text/plain":["13         1\n","16         2\n","17         5\n","18        12\n","19        24\n","       ...  \n","266    29799\n","267    29821\n","268    29842\n","269    29851\n","270    29872\n","Name: seq_len, Length: 256, dtype: int64"]},"execution_count":11,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["dfs_all = pd.concat([df_train, df_val, df_test])\n","dfs_all['seq_len'] = dfs_all.apply(lambda row: \n","                                   len(row['other_speaker'] + row['friend_response']), axis=1)\n","print(len(dfs_all), \"number of all dialogs in train, validation and test\")\n","[len(i.split()) for i in dfs_all.other_speaker]\n","dfs_all['seq_len'].hist(bins = 30)\n","dfs_all['seq_len'].value_counts().sort_index(ascending=True).cumsum().head(256)"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1636037511843,"user":{"displayName":"Andrew Argatkiny","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04422905123959661601"},"user_tz":-420},"id":"jLmUmq46a0D6"},"outputs":[],"source":["class SentencePairClassifier(nn.Module):\n","\n","    def __init__(self, model=CHECKPOINT, freeze_model=True):\n","        super(SentencePairClassifier, self).__init__()\n","        #  Instantiating BERT-based model object\n","        # self.pretrained_layer = AutoModel.from_pretrained(CHECKPOINT)\n","        #self.pretrained_layer = BertModel.from_pretrained(CHECKPOINT)\n","        self.pretrained_layer = RobertaModel.from_pretrained(CHECKPOINT)\n","\n","        hidden_size = self.pretrained_layer.config.hidden_size\n","\n","        # Freeze model layers and only train the classification layer weights\n","        if freeze_model:\n","            for p in self.pretrained_layer.parameters():\n","                p.requires_grad = False\n","            print('All parameters frozen')\n","        # Classification layer\n","        self.cls_layer = nn.Linear(hidden_size, 6)\n","\n","        self.dropout = nn.Dropout(p=0.1)\n","\n","    @autocast()  # run in mixed precision\n","    def forward(self, input_ids, attn_masks):\n","        '''\n","        Inputs:\n","            -input_ids : Tensor  containing token ids\n","            -attn_masks : Tensor containing attention masks to be used to focus on non-padded values\n","            -token_type_ids : Tensor containing token type ids to be used to identify sentence1 and sentence2\n","        '''\n","\n","        # Feeding the inputs to the BERT-based model to obtain contextualized representations\n","        output = self.pretrained_layer(input_ids, attn_masks)\n","\n","        # Feeding to the classifier layer the last layer hidden-state of the [CLS] token further processed by a\n","        # Linear Layer and a Tanh activation. The Linear layer weights were trained from the sentence order prediction (ALBERT) or next sentence prediction (BERT)\n","        # objective during pre-training.\n","        logits = self.cls_layer(self.dropout(output.pooler_output))\n","\n","        return logits"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1636037511843,"user":{"displayName":"Andrew Argatkiny","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04422905123959661601"},"user_tz":-420},"id":"JKiUIjwGiWEv"},"outputs":[],"source":["def set_seed(seed):\n","    \"\"\" Set all seeds to make results reproducible \"\"\"\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    #os.environ['PYTHONHASHSEED'] = str(seed)\n","    \n","\n","def evaluate_loss(net, device, criterion, dataloader):\n","    net.eval()\n","    n_correct = 0\n","    mean_loss = 0\n","    count = 0\n","\n","    with torch.no_grad():\n","        for it, (seq, attn_masks, labels) in enumerate(tqdm(dataloader)):\n","            seq, attn_masks, labels = \\\n","                seq.to(device), attn_masks.to(device), labels.to(device)\n","            logits = net(seq, attn_masks)\n","            mean_loss += criterion(logits.squeeze(-1), labels).item()\n","            count += 1\n","            max_logits, argmax_idx = torch.max(logits.data, dim=1)\n","            n_correct += calcuate_accu(argmax_idx, labels)\n","    del logits\n","    return mean_loss / count, n_correct / len(dataloader.dataset)\n","  \n","# Function to calcuate the accuracy of the model\n","def calcuate_accu(big_idx, targets):\n","    n_correct = (big_idx==targets).sum().item()\n","    return n_correct"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":271,"status":"ok","timestamp":1636040507768,"user":{"displayName":"Andrew Argatkiny","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04422905123959661601"},"user_tz":-420},"id":"NNBTL2IC8PMJ"},"outputs":[],"source":["def train_bert(net, criterion, opti, lr, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate):\n","\n","    best_loss = np.Inf\n","    best_acc = 0\n","    best_ep = 1\n","    n_iterations = len(train_loader)\n","    batch_size = train_loader.batch_size\n","    print_every = 1000 // batch_size  # print the training loss this many times per epoch\n","    print_eval_iters = 10000 // batch_size\n","    scaler = GradScaler()\n","\n","    for ep in range(epochs):\n","        net.train()\n","        curr_loss = 0.0\n","        curr_n_correct = 0.\n","        trailing_loss = 0.\n","        trailing_n_correct = 0.\n","        curr_n_tr_examples = 0\n","        trainling_n_tr_examples = 0\n","\n","        for it, (seq, attn_masks, labels) in enumerate(tqdm(train_loader)):\n","            # Converting to cuda tensors\n","            seq, attn_masks,labels = \\\n","                seq.to(device), attn_masks.to(device), labels.to(device)\n","  \n","            # Enables autocasting for the forward pass (model + loss)\n","            with autocast():\n","                # Obtaining the logits from the model\n","                pooled = net(seq, attn_masks)\n","\n","                # Computing loss\n","                loss = criterion(pooled.squeeze(-1), labels)\n","                #print(loss, type(loss))\n","                loss = loss / iters_to_accumulate  # Normalize the loss because it is averaged\n","                # Computing accuracy\n","                #print(pooled.squeeze(-1), labels)\n","                curr_loss += loss.item() \n","                big_val, big_idx = torch.max(pooled.data, dim=1)\n","                n_correct = calcuate_accu(big_idx, labels)\n","                curr_n_correct += n_correct\n","\n","            trailing_loss += loss.item() \n","            trailing_n_correct += n_correct\n","            curr_n_tr_examples += labels.size(0)\n","            trainling_n_tr_examples += labels.size(0)\n","\n","            # Backpropagating the gradients\n","            # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n","            scaler.scale(loss).backward()\n","\n","            if (it + 1) % iters_to_accumulate == 0:\n","                # Optimization step\n","                # scaler.step() first unscales the gradients of the optimizer's assigned params.\n","                # If these gradients do not contain infs or NaNs, opti.step() is then called,\n","                # otherwise, opti.step() is skipped.\n","                scaler.step(opti)\n","                # Updates the scale for next iteration.\n","                scaler.update()\n","                # Adjust the learning rate based on the number of iterations.\n","                lr_scheduler.step()\n","                # Clear gradients\n","                opti.zero_grad()\n","\n","            if (it + 1) % print_every == 0:  # Print training loss information\n","                print()\n","                print(\"Batch {}/{} of epoch {} complete. Loss per last {} samples:: {} \"\n","                      .format(it+1, n_iterations, ep+1, curr_n_tr_examples, curr_loss / print_every))\n","                accu_step = (curr_n_correct*100) / curr_n_tr_examples \n","                #print(f\"Training Loss per 5000 steps: {loss_step}\")\n","                print(f\"Training Accuracy per last {curr_n_tr_examples} samples: {accu_step}\")\n","                curr_loss = 0.0\n","                curr_n_tr_examples = 0\n","                curr_n_correct = 0\n","\n","\n","            if (it + 1) % print_eval_iters == 0 or it ==  n_iterations - 1:\n","                del pooled, loss\n","                print(\"Epoch {}, batch {} complete! Training Loss : {}\"\n","                .format(ep+1, it+1, trailing_loss / (it+1)))\n","                print(\"Epoch {}, batch {} complete! Training Accuracy : {}\"\n","                .format(ep+1, it+1, trailing_n_correct / trainling_n_tr_examples))\n","                val_loss, val_accuracy = evaluate_loss(net, device, criterion, val_loader)  # Compute validation loss\n","                #print()\n","                print(\"Epoch {}, batch {} complete! Validation Loss : {}\".format(ep+1, it+1, val_loss))\n","                print(\"Epoch {}, batch {} complete! Validation Accuracy : {}\".format(ep+1, it+1,val_accuracy))\n","                net.train()\n","                #if val_loss < best_loss:\n","                if val_accuracy > best_acc:\n","                    print(\"Validation loss changed from {} to {}\".format(best_loss, val_loss))\n","                    print(\"Best validation accuracy improved from {} to {}\".format(best_acc, val_accuracy))\n","                    print()\n","                    #net_copy = copy.deepcopy(net)  # save a copy of the model\n","                    best_loss = val_loss\n","                    best_acc = val_accuracy\n","                    best_ep = ep + 1\n","                    # Saving the model\n","                    path_to_model='models/{}_lr_{}_val_acc_{}_ep_{}.pt'.format(time.ctime(), lr, round(best_acc, 4), best_ep)\n","                    torch.save(net.state_dict(), path_to_model)\n","                    print(\"The model has been saved in {}\".format(path_to_model))\n","\n","    torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["3d8a373f8b1a4da3841d55ed5fd62935","5435b27f9fab4fcd9f0cf0fe1f11db71","b968066f8f924c309860b111ae0816a4","98ff0c5dbe8049c588ef631dfbc08ef8","121075183d2441ddb2e99f849d2e9686","77b7765f0fe04c8a976230fb2eb01065","88b743167cd44b318de6a3998f466556","ef630b6fc27b4750839918ef7ac13c6c","e55762599f664a0fa5288de998af11c1","e3aeb80a52bf4ccdb69330b3ebce4355","7f4f04df6382427288c4e9422e4fbf7d"]},"executionInfo":{"elapsed":1839309,"status":"ok","timestamp":1636039351577,"user":{"displayName":"Andrew Argatkiny","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04422905123959661601"},"user_tz":-420},"id":"tV_3pDzJ81v9","outputId":"5413e47a-47a4-4216-9f14-592c6d734425"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading training data...\n","Reading validation data...\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["cuda:0\n"]}],"source":["from transformers import get_linear_schedule_with_warmup\n","from transformers import get_constant_schedule\n","#  Set all seeds to make reproducible results\n","set_seed(1)\n","\n","# Creating instances of training and validation set\n","print(\"Reading training data...\")\n","train_set = FriendsDataset(dataframe=df_train_3X, tokenizer=tokenizer, max_length=MAX_LEN)\n","#train_set = FriendsDataset(dataframe=df_full, tokenizer=tokenizer, max_length=MAX_LEN)\n","\n","print(\"Reading validation data...\")\n","val_set = FriendsDataset(dataframe=df_val_3X, tokenizer=tokenizer, max_length=MAX_LEN)\n","# Creating instances of training and validation dataloaders\n","train_loader = DataLoader(train_set, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=2)\n","val_loader = DataLoader(val_set, batch_size=VALID_BATCH_SIZE, shuffle=False, num_workers=2)\n","\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","net = SentencePairClassifier(model=CHECKPOINT, freeze_model=False)\n","print(device)\n","\n","if torch.cuda.device_count() > 1:  # if multiple GPUs\n","    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n","    net = nn.DataParallel(net)\n","\n","net.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","opti = AdamW(net.parameters(), lr=LEARNING_RATE)#, weight_decay=1e-2)\n","num_warmup_steps = 0 # The number of steps for the warmup phase.\n","iters_to_accumulate = 2\n","num_training_steps = EPOCHS * len(train_loader)  # The total number of training steps\n","t_total = (len(train_loader) // iters_to_accumulate) * EPOCHS  # Necessary to take into account Gradient accumulation\n","#lr_scheduler = get_linear_schedule_with_warmup(optimizer=opti, num_warmup_steps=num_warmup_steps, num_training_steps=t_total)\n","lr_scheduler = get_constant_schedule(optimizer=opti)\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  1%|▏         | 62/4687 [00:43<53:08,  1.45it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 62/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8985477570564516 \n","Training Accuracy per last 992 samples: 17.741935483870968\n"]},{"name":"stderr","output_type":"stream","text":["  3%|▎         | 124/4687 [01:26<53:02,  1.43it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 124/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8961629559916835 \n","Training Accuracy per last 992 samples: 18.548387096774192\n"]},{"name":"stderr","output_type":"stream","text":["  4%|▍         | 186/4687 [02:08<53:55,  1.39it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 186/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8963819934475806 \n","Training Accuracy per last 992 samples: 18.245967741935484\n"]},{"name":"stderr","output_type":"stream","text":["  5%|▌         | 246/4687 [02:50<51:24,  1.44it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","  5%|▌         | 248/4687 [02:52<51:48,  1.43it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 248/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8948826943674395 \n","Training Accuracy per last 992 samples: 18.850806451612904\n"]},{"name":"stderr","output_type":"stream","text":["  7%|▋         | 310/4687 [03:35<51:08,  1.43it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 310/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8943397768082157 \n","Training Accuracy per last 992 samples: 17.54032258064516\n"]},{"name":"stderr","output_type":"stream","text":["  8%|▊         | 372/4687 [04:18<51:16,  1.40it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 372/4687 of epoch 1 complete. Loss per last 992 samples:: 0.896913097750756 \n","Training Accuracy per last 992 samples: 16.633064516129032\n"]},{"name":"stderr","output_type":"stream","text":["  9%|▉         | 434/4687 [05:02<50:20,  1.41it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 434/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8975884222215221 \n","Training Accuracy per last 992 samples: 16.431451612903224\n"]},{"name":"stderr","output_type":"stream","text":[" 11%|█         | 496/4687 [05:43<44:24,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 496/4687 of epoch 1 complete. Loss per last 992 samples:: 0.894837410219254 \n","Training Accuracy per last 992 samples: 19.254032258064516\n"]},{"name":"stderr","output_type":"stream","text":[" 12%|█▏        | 557/4687 [06:22<43:25,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 12%|█▏        | 558/4687 [06:22<43:53,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 558/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8945489698840726 \n","Training Accuracy per last 992 samples: 18.044354838709676\n"]},{"name":"stderr","output_type":"stream","text":[" 13%|█▎        | 620/4687 [07:02<42:57,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 620/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8943427301222279 \n","Training Accuracy per last 992 samples: 16.733870967741936\n"]},{"name":"stderr","output_type":"stream","text":[" 13%|█▎        | 624/4687 [07:04<43:14,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 625 complete! Training Loss : 0.895867919921875\n","Epoch 1, batch 625 complete! Training Accuracy : 0.178\n"]},{"name":"stderr","output_type":"stream","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","100%|██████████| 521/521 [01:59<00:00,  4.35it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 625 complete! Validation Loss : 1.7919097138915547\n","Epoch 1, batch 625 complete! Validation Accuracy : 0.16162706983441325\n","Validation loss changed from inf to 1.7919097138915547\n","Best validation accuracy improved from 0 to 0.16162706983441325\n","\n"]},{"name":"stderr","output_type":"stream","text":[" 13%|█▎        | 625/4687 [09:05<41:35:03, 36.85s/it]"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Wed Nov 17 13:21:05 2021_lr_2e-05_val_acc_0.1616_ep_1.pt\n"]},{"name":"stderr","output_type":"stream","text":[" 13%|█▎        | 629/4687 [09:08<10:32:13,  9.35s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 15%|█▍        | 682/4687 [09:42<42:20,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 682/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8991945328251008 \n","Training Accuracy per last 992 samples: 16.028225806451612\n"]},{"name":"stderr","output_type":"stream","text":[" 15%|█▌        | 726/4687 [10:10<41:57,  1.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 16%|█▌        | 744/4687 [10:21<41:45,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 744/4687 of epoch 1 complete. Loss per last 992 samples:: 0.897123767483619 \n","Training Accuracy per last 992 samples: 17.137096774193548\n"]},{"name":"stderr","output_type":"stream","text":[" 17%|█▋        | 806/4687 [11:01<41:11,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 806/4687 of epoch 1 complete. Loss per last 992 samples:: 0.893797843686996 \n","Training Accuracy per last 992 samples: 20.866935483870968\n"]},{"name":"stderr","output_type":"stream","text":[" 19%|█▊        | 868/4687 [11:40<40:21,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 868/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8967043968939012 \n","Training Accuracy per last 992 samples: 18.75\n"]},{"name":"stderr","output_type":"stream","text":[" 19%|█▊        | 874/4687 [11:44<40:56,  1.55it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 20%|█▉        | 930/4687 [12:20<39:42,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 930/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8904694587953629 \n","Training Accuracy per last 992 samples: 20.967741935483872\n"]},{"name":"stderr","output_type":"stream","text":[" 21%|██        | 983/4687 [12:53<39:05,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 21%|██        | 992/4687 [12:59<39:29,  1.56it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 992/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8840856244487147 \n","Training Accuracy per last 992 samples: 21.975806451612904\n"]},{"name":"stderr","output_type":"stream","text":[" 22%|██▏       | 1054/4687 [13:39<38:27,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1054/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8899883147208921 \n","Training Accuracy per last 992 samples: 21.27016129032258\n"]},{"name":"stderr","output_type":"stream","text":[" 24%|██▍       | 1116/4687 [14:18<37:45,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1116/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8860988001669606 \n","Training Accuracy per last 992 samples: 22.883064516129032\n"]},{"name":"stderr","output_type":"stream","text":[" 25%|██▌       | 1175/4687 [14:56<36:58,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 25%|██▌       | 1178/4687 [14:58<38:49,  1.51it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1178/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8837944769090221 \n","Training Accuracy per last 992 samples: 22.379032258064516\n"]},{"name":"stderr","output_type":"stream","text":[" 26%|██▋       | 1240/4687 [15:37<36:33,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1240/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8806686401367188 \n","Training Accuracy per last 992 samples: 22.278225806451612\n"]},{"name":"stderr","output_type":"stream","text":[" 27%|██▋       | 1249/4687 [15:43<36:25,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 1250 complete! Training Loss : 0.8930215454101562\n","Epoch 1, batch 1250 complete! Training Accuracy : 0.19125\n"]},{"name":"stderr","output_type":"stream","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","100%|██████████| 521/521 [02:00<00:00,  4.34it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 1250 complete! Validation Loss : 1.7646072006957774\n","Epoch 1, batch 1250 complete! Validation Accuracy : 0.21754259659227262\n","Validation loss changed from 1.7919097138915547 to 1.7646072006957774\n","Best validation accuracy improved from 0.16162706983441325 to 0.21754259659227262\n","\n"]},{"name":"stderr","output_type":"stream","text":[" 27%|██▋       | 1250/4687 [17:44<35:16:01, 36.94s/it]"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Wed Nov 17 13:29:44 2021_lr_2e-05_val_acc_0.2175_ep_1.pt\n"]},{"name":"stderr","output_type":"stream","text":[" 28%|██▊       | 1302/4687 [18:17<35:51,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1302/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8841976042716734 \n","Training Accuracy per last 992 samples: 20.967741935483872\n"]},{"name":"stderr","output_type":"stream","text":[" 28%|██▊       | 1322/4687 [18:30<35:43,  1.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 29%|██▉       | 1364/4687 [18:57<35:14,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1364/4687 of epoch 1 complete. Loss per last 992 samples:: 0.867322983280305 \n","Training Accuracy per last 992 samples: 26.713709677419356\n"]},{"name":"stderr","output_type":"stream","text":[" 30%|███       | 1426/4687 [19:36<34:35,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1426/4687 of epoch 1 complete. Loss per last 992 samples:: 0.875853753858997 \n","Training Accuracy per last 992 samples: 23.991935483870968\n"]},{"name":"stderr","output_type":"stream","text":[" 30%|███       | 1428/4687 [19:37<34:53,  1.56it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 31%|███▏      | 1471/4687 [20:05<34:31,  1.55it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 32%|███▏      | 1479/4687 [20:10<33:53,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 32%|███▏      | 1488/4687 [20:16<35:13,  1.51it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1488/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8673132619550151 \n","Training Accuracy per last 992 samples: 23.588709677419356\n"]},{"name":"stderr","output_type":"stream","text":[" 33%|███▎      | 1550/4687 [20:55<33:19,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1550/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8685777110438193 \n","Training Accuracy per last 992 samples: 24.798387096774192\n"]},{"name":"stderr","output_type":"stream","text":[" 33%|███▎      | 1557/4687 [21:00<33:02,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 34%|███▍      | 1612/4687 [21:35<32:31,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1612/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8692088588591544 \n","Training Accuracy per last 992 samples: 25.0\n"]},{"name":"stderr","output_type":"stream","text":[" 36%|███▌      | 1674/4687 [22:14<31:52,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1674/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8592952605216734 \n","Training Accuracy per last 992 samples: 29.233870967741936\n"]},{"name":"stderr","output_type":"stream","text":[" 36%|███▌      | 1698/4687 [22:29<31:35,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 37%|███▋      | 1736/4687 [22:53<31:15,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1736/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8662808325982863 \n","Training Accuracy per last 992 samples: 25.302419354838708\n"]},{"name":"stderr","output_type":"stream","text":[" 38%|███▊      | 1781/4687 [23:22<30:35,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 38%|███▊      | 1798/4687 [23:33<30:37,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1798/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8555279393349925 \n","Training Accuracy per last 992 samples: 26.31048387096774\n"]},{"name":"stderr","output_type":"stream","text":[" 39%|███▊      | 1806/4687 [23:38<30:36,  1.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 40%|███▉      | 1860/4687 [24:12<29:55,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1860/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8449366169591104 \n","Training Accuracy per last 992 samples: 28.72983870967742\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|███▉      | 1874/4687 [24:21<29:45,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 1875 complete! Training Loss : 0.883684130859375\n","Epoch 1, batch 1875 complete! Training Accuracy : 0.21283333333333335\n"]},{"name":"stderr","output_type":"stream","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","100%|██████████| 521/521 [01:57<00:00,  4.43it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 1875 complete! Validation Loss : 1.7105251319577734\n","Epoch 1, batch 1875 complete! Validation Accuracy : 0.27513798896088315\n","Validation loss changed from 1.7646072006957774 to 1.7105251319577734\n","Best validation accuracy improved from 0.21754259659227262 to 0.27513798896088315\n","\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|████      | 1875/4687 [26:20<28:14:05, 36.15s/it]"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Wed Nov 17 13:38:20 2021_lr_2e-05_val_acc_0.2751_ep_1.pt\n"]},{"name":"stderr","output_type":"stream","text":[" 41%|████      | 1922/4687 [26:50<30:37,  1.50it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1922/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8483687985327936 \n","Training Accuracy per last 992 samples: 27.318548387096776\n"]},{"name":"stderr","output_type":"stream","text":[" 42%|████▏     | 1962/4687 [27:16<28:43,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 42%|████▏     | 1984/4687 [27:30<28:32,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1984/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8487681727255544 \n","Training Accuracy per last 992 samples: 28.830645161290324\n"]},{"name":"stderr","output_type":"stream","text":[" 44%|████▎     | 2046/4687 [28:09<27:51,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2046/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8553660608107044 \n","Training Accuracy per last 992 samples: 28.326612903225808\n"]},{"name":"stderr","output_type":"stream","text":[" 44%|████▍     | 2070/4687 [28:25<27:37,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 45%|████▍     | 2108/4687 [28:49<27:09,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2108/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8372029335268082 \n","Training Accuracy per last 992 samples: 30.846774193548388\n"]},{"name":"stderr","output_type":"stream","text":[" 45%|████▌     | 2131/4687 [29:03<26:49,  1.59it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 46%|████▋     | 2170/4687 [29:28<26:32,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2170/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8423856304537866 \n","Training Accuracy per last 992 samples: 30.342741935483872\n"]},{"name":"stderr","output_type":"stream","text":[" 48%|████▊     | 2232/4687 [30:07<25:57,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2232/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8412671242990801 \n","Training Accuracy per last 992 samples: 30.342741935483872\n"]},{"name":"stderr","output_type":"stream","text":[" 48%|████▊     | 2247/4687 [30:16<25:35,  1.59it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 48%|████▊     | 2265/4687 [30:28<25:24,  1.59it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 49%|████▊     | 2279/4687 [30:37<25:14,  1.59it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 49%|████▉     | 2294/4687 [30:46<25:13,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2294/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8451488248763546 \n","Training Accuracy per last 992 samples: 29.33467741935484\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 2356/4687 [31:25<24:32,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2356/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8232191147342804 \n","Training Accuracy per last 992 samples: 31.955645161290324\n"]},{"name":"stderr","output_type":"stream","text":[" 51%|█████     | 2381/4687 [31:41<24:42,  1.56it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 52%|█████▏    | 2418/4687 [32:05<24:03,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2418/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8417539904194493 \n","Training Accuracy per last 992 samples: 29.737903225806452\n"]},{"name":"stderr","output_type":"stream","text":[" 53%|█████▎    | 2480/4687 [32:44<23:20,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2480/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8304824213827809 \n","Training Accuracy per last 992 samples: 31.048387096774192\n"]},{"name":"stderr","output_type":"stream","text":[" 53%|█████▎    | 2499/4687 [32:56<23:03,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 2500 complete! Training Loss : 0.8729039962768554\n","Epoch 1, batch 2500 complete! Training Accuracy : 0.234425\n"]},{"name":"stderr","output_type":"stream","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","100%|██████████| 521/521 [01:57<00:00,  4.44it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 2500 complete! Validation Loss : 1.6853050773752398\n","Epoch 1, batch 2500 complete! Validation Accuracy : 0.28509719222462204\n","Validation loss changed from 1.7105251319577734 to 1.6853050773752398\n","Best validation accuracy improved from 0.27513798896088315 to 0.28509719222462204\n","\n"]},{"name":"stderr","output_type":"stream","text":[" 53%|█████▎    | 2500/4687 [34:55<21:54:26, 36.06s/it]"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Wed Nov 17 13:46:55 2021_lr_2e-05_val_acc_0.2851_ep_1.pt\n"]},{"name":"stderr","output_type":"stream","text":[" 54%|█████▍    | 2542/4687 [35:22<22:45,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2542/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8186922688638011 \n","Training Accuracy per last 992 samples: 31.149193548387096\n"]},{"name":"stderr","output_type":"stream","text":[" 56%|█████▌    | 2604/4687 [36:01<22:01,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2604/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8224147365939233 \n","Training Accuracy per last 992 samples: 31.552419354838708\n"]},{"name":"stderr","output_type":"stream","text":[" 57%|█████▋    | 2666/4687 [36:40<21:24,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2666/4687 of epoch 1 complete. Loss per last 992 samples:: 0.821492564293646 \n","Training Accuracy per last 992 samples: 31.149193548387096\n"]},{"name":"stderr","output_type":"stream","text":[" 58%|█████▊    | 2728/4687 [37:20<20:42,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2728/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8353887373401273 \n","Training Accuracy per last 992 samples: 30.846774193548388\n"]},{"name":"stderr","output_type":"stream","text":[" 60%|█████▉    | 2790/4687 [37:59<20:05,  1.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2790/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8256476925265405 \n","Training Accuracy per last 992 samples: 30.241935483870968\n"]},{"name":"stderr","output_type":"stream","text":[" 61%|██████    | 2852/4687 [38:38<19:29,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2852/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8303430618778351 \n","Training Accuracy per last 992 samples: 30.04032258064516\n"]},{"name":"stderr","output_type":"stream","text":[" 61%|██████    | 2861/4687 [38:44<19:18,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 62%|██████▏   | 2903/4687 [39:11<18:49,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 62%|██████▏   | 2914/4687 [39:18<18:45,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2914/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8376103370420395 \n","Training Accuracy per last 992 samples: 29.838709677419356\n"]},{"name":"stderr","output_type":"stream","text":[" 63%|██████▎   | 2976/4687 [39:57<18:07,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2976/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8204410306869014 \n","Training Accuracy per last 992 samples: 31.350806451612904\n"]},{"name":"stderr","output_type":"stream","text":[" 65%|██████▍   | 3038/4687 [40:37<17:30,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3038/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8275149253106886 \n","Training Accuracy per last 992 samples: 32.25806451612903\n"]},{"name":"stderr","output_type":"stream","text":[" 66%|██████▌   | 3100/4687 [41:16<16:47,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3100/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8183308570615707 \n","Training Accuracy per last 992 samples: 31.25\n"]},{"name":"stderr","output_type":"stream","text":[" 66%|██████▋   | 3111/4687 [41:23<16:36,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 67%|██████▋   | 3123/4687 [41:31<16:31,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 67%|██████▋   | 3124/4687 [41:31<16:48,  1.55it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 3125 complete! Training Loss : 0.8633118273925782\n","Epoch 1, batch 3125 complete! Training Accuracy : 0.25\n"]},{"name":"stderr","output_type":"stream","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","100%|██████████| 521/521 [01:57<00:00,  4.44it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 3125 complete! Validation Loss : 1.6903940738963532\n","Epoch 1, batch 3125 complete! Validation Accuracy : 0.2932565394768418\n","Validation loss changed from 1.6853050773752398 to 1.6903940738963532\n","Best validation accuracy improved from 0.28509719222462204 to 0.2932565394768418\n","\n"]},{"name":"stderr","output_type":"stream","text":[" 67%|██████▋   | 3125/4687 [43:30<15:39:24, 36.08s/it]"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Wed Nov 17 13:55:30 2021_lr_2e-05_val_acc_0.2933_ep_1.pt\n"]},{"name":"stderr","output_type":"stream","text":[" 67%|██████▋   | 3146/4687 [43:43<16:48,  1.53it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 67%|██████▋   | 3162/4687 [43:54<16:09,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3162/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8257684400004726 \n","Training Accuracy per last 992 samples: 31.955645161290324\n"]},{"name":"stderr","output_type":"stream","text":[" 68%|██████▊   | 3209/4687 [44:23<15:33,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 69%|██████▉   | 3224/4687 [44:33<15:29,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3224/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8353860301356162 \n","Training Accuracy per last 992 samples: 31.955645161290324\n"]},{"name":"stderr","output_type":"stream","text":[" 70%|██████▉   | 3270/4687 [45:02<15:00,  1.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 70%|███████   | 3286/4687 [45:12<14:50,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3286/4687 of epoch 1 complete. Loss per last 992 samples:: 0.808866685436618 \n","Training Accuracy per last 992 samples: 35.181451612903224\n"]},{"name":"stderr","output_type":"stream","text":[" 71%|███████▏  | 3348/4687 [45:52<14:10,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3348/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8068798434349799 \n","Training Accuracy per last 992 samples: 31.955645161290324\n"]},{"name":"stderr","output_type":"stream","text":[" 73%|███████▎  | 3410/4687 [46:31<13:33,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3410/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8205977101479808 \n","Training Accuracy per last 992 samples: 31.25\n"]},{"name":"stderr","output_type":"stream","text":[" 74%|███████▍  | 3472/4687 [47:10<12:53,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3472/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8228953423038605 \n","Training Accuracy per last 992 samples: 31.350806451612904\n"]},{"name":"stderr","output_type":"stream","text":[" 75%|███████▌  | 3534/4687 [47:50<12:12,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3534/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8219756464804372 \n","Training Accuracy per last 992 samples: 31.552419354838708\n"]},{"name":"stderr","output_type":"stream","text":[" 77%|███████▋  | 3596/4687 [48:29<11:34,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3596/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8187055587768555 \n","Training Accuracy per last 992 samples: 32.66129032258065\n"]},{"name":"stderr","output_type":"stream","text":[" 77%|███████▋  | 3619/4687 [48:44<11:16,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 78%|███████▊  | 3658/4687 [49:09<10:53,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3658/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8159642988635648 \n","Training Accuracy per last 992 samples: 32.86290322580645\n"]},{"name":"stderr","output_type":"stream","text":[" 79%|███████▉  | 3720/4687 [49:48<10:15,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3720/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8256806404359879 \n","Training Accuracy per last 992 samples: 31.754032258064516\n"]},{"name":"stderr","output_type":"stream","text":[" 80%|███████▉  | 3749/4687 [50:07<09:53,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 3750 complete! Training Loss : 0.8562859662373861\n","Epoch 1, batch 3750 complete! Training Accuracy : 0.26161666666666666\n"]},{"name":"stderr","output_type":"stream","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","100%|██████████| 521/521 [01:59<00:00,  4.36it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 3750 complete! Validation Loss : 1.6587748170585412\n","Epoch 1, batch 3750 complete! Validation Accuracy : 0.2980561555075594\n","Validation loss changed from 1.6903940738963532 to 1.6587748170585412\n","Best validation accuracy improved from 0.2932565394768418 to 0.2980561555075594\n","\n"]},{"name":"stderr","output_type":"stream","text":[" 80%|████████  | 3750/4687 [52:08<9:33:48, 36.74s/it]"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Wed Nov 17 14:04:08 2021_lr_2e-05_val_acc_0.2981_ep_1.pt\n"]},{"name":"stderr","output_type":"stream","text":[" 81%|████████  | 3782/4687 [52:28<09:36,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3782/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8159817726381363 \n","Training Accuracy per last 992 samples: 32.358870967741936\n"]},{"name":"stderr","output_type":"stream","text":[" 82%|████████▏ | 3844/4687 [53:07<08:57,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3844/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8157059761785692 \n","Training Accuracy per last 992 samples: 35.28225806451613\n"]},{"name":"stderr","output_type":"stream","text":[" 83%|████████▎ | 3906/4687 [53:47<08:16,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3906/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8013658215922694 \n","Training Accuracy per last 992 samples: 34.17338709677419\n"]},{"name":"stderr","output_type":"stream","text":[" 85%|████████▍ | 3968/4687 [54:26<07:36,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3968/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8029436911306074 \n","Training Accuracy per last 992 samples: 33.87096774193548\n"]},{"name":"stderr","output_type":"stream","text":[" 86%|████████▌ | 4030/4687 [55:06<06:57,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 4030/4687 of epoch 1 complete. Loss per last 992 samples:: 0.7793368677939138 \n","Training Accuracy per last 992 samples: 35.28225806451613\n"]},{"name":"stderr","output_type":"stream","text":[" 87%|████████▋ | 4092/4687 [55:45<06:18,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 4092/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8152619946387506 \n","Training Accuracy per last 992 samples: 31.955645161290324\n"]},{"name":"stderr","output_type":"stream","text":[" 88%|████████▊ | 4121/4687 [56:04<05:57,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 88%|████████▊ | 4128/4687 [56:08<06:00,  1.55it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 89%|████████▊ | 4154/4687 [56:25<05:38,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 4154/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8017000075309507 \n","Training Accuracy per last 992 samples: 34.979838709677416\n"]},{"name":"stderr","output_type":"stream","text":[" 89%|████████▊ | 4159/4687 [56:28<05:42,  1.54it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 90%|████████▉ | 4216/4687 [57:05<05:00,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 4216/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8165579765073715 \n","Training Accuracy per last 992 samples: 33.971774193548384\n"]},{"name":"stderr","output_type":"stream","text":[" 91%|█████████▏| 4278/4687 [57:44<04:19,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 4278/4687 of epoch 1 complete. Loss per last 992 samples:: 0.7870605222640499 \n","Training Accuracy per last 992 samples: 32.86290322580645\n"]},{"name":"stderr","output_type":"stream","text":[" 93%|█████████▎| 4340/4687 [58:24<03:40,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 4340/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8082992492183563 \n","Training Accuracy per last 992 samples: 33.16532258064516\n"]},{"name":"stderr","output_type":"stream","text":[" 93%|█████████▎| 4374/4687 [58:45<03:18,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 4375 complete! Training Loss : 0.8488553974696568\n","Epoch 1, batch 4375 complete! Training Accuracy : 0.27254285714285714\n"]},{"name":"stderr","output_type":"stream","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","100%|██████████| 521/521 [01:59<00:00,  4.35it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 4375 complete! Validation Loss : 1.6625161198416507\n","Epoch 1, batch 4375 complete! Validation Accuracy : 0.3050155987520998\n","Validation loss changed from 1.6587748170585412 to 1.6625161198416507\n","Best validation accuracy improved from 0.2980561555075594 to 0.3050155987520998\n","\n"]},{"name":"stderr","output_type":"stream","text":[" 93%|█████████▎| 4375/4687 [1:00:47<3:11:21, 36.80s/it]"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Wed Nov 17 14:12:47 2021_lr_2e-05_val_acc_0.305_ep_1.pt\n"]},{"name":"stderr","output_type":"stream","text":[" 94%|█████████▍| 4402/4687 [1:01:04<03:01,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 4402/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8078832626342773 \n","Training Accuracy per last 992 samples: 33.66935483870968\n"]},{"name":"stderr","output_type":"stream","text":[" 94%|█████████▍| 4428/4687 [1:01:21<02:44,  1.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 95%|█████████▌| 4464/4687 [1:01:43<02:22,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 4464/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8111228327597341 \n","Training Accuracy per last 992 samples: 33.971774193548384\n"]},{"name":"stderr","output_type":"stream","text":[" 97%|█████████▋| 4526/4687 [1:02:23<01:42,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 4526/4687 of epoch 1 complete. Loss per last 992 samples:: 0.7942359062933153 \n","Training Accuracy per last 992 samples: 36.59274193548387\n"]},{"name":"stderr","output_type":"stream","text":[" 98%|█████████▊| 4588/4687 [1:03:02<01:02,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 4588/4687 of epoch 1 complete. Loss per last 992 samples:: 0.7800774420461347 \n","Training Accuracy per last 992 samples: 35.98790322580645\n"]},{"name":"stderr","output_type":"stream","text":[" 99%|█████████▉| 4650/4687 [1:03:42<00:23,  1.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 4650/4687 of epoch 1 complete. Loss per last 992 samples:: 0.7980221163841986 \n","Training Accuracy per last 992 samples: 33.16532258064516\n"]},{"name":"stderr","output_type":"stream","text":[" 99%|█████████▉| 4658/4687 [1:03:47<00:18,  1.56it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","100%|█████████▉| 4686/4687 [1:04:05<00:00,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 4687 complete! Training Loss : 0.845512201924911\n","Epoch 1, batch 4687 complete! Training Accuracy : 0.27758439029594956\n"]},{"name":"stderr","output_type":"stream","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","100%|██████████| 521/521 [01:59<00:00,  4.36it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 4687 complete! Validation Loss : 1.6460307701535508\n","Epoch 1, batch 4687 complete! Validation Accuracy : 0.3062155027597792\n","Validation loss changed from 1.6625161198416507 to 1.6460307701535508\n","Best validation accuracy improved from 0.3050155987520998 to 0.3062155027597792\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4687/4687 [1:06:06<00:00,  1.18it/s]\n"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Wed Nov 17 14:18:06 2021_lr_2e-05_val_acc_0.3062_ep_1.pt\n"]},{"name":"stderr","output_type":"stream","text":["  1%|          | 30/4687 [00:19<49:26,  1.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","  1%|▏         | 62/4687 [00:39<48:57,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 62/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7830881303356539 \n","Training Accuracy per last 992 samples: 36.08870967741935\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 93/4687 [00:59<48:29,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","  3%|▎         | 124/4687 [01:19<48:23,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 124/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7584551534345073 \n","Training Accuracy per last 992 samples: 38.00403225806452\n"]},{"name":"stderr","output_type":"stream","text":["  4%|▍         | 186/4687 [01:58<47:47,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 186/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7571274388221002 \n","Training Accuracy per last 992 samples: 40.725806451612904\n"]},{"name":"stderr","output_type":"stream","text":["  5%|▌         | 248/4687 [02:38<47:13,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 248/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7470036475889145 \n","Training Accuracy per last 992 samples: 40.32258064516129\n"]},{"name":"stderr","output_type":"stream","text":["  7%|▋         | 310/4687 [03:17<46:25,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 310/4687 of epoch 2 complete. Loss per last 992 samples:: 0.780768209888089 \n","Training Accuracy per last 992 samples: 36.79435483870968\n"]},{"name":"stderr","output_type":"stream","text":["  8%|▊         | 372/4687 [03:57<45:47,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 372/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7774555760045205 \n","Training Accuracy per last 992 samples: 36.99596774193548\n"]},{"name":"stderr","output_type":"stream","text":["  9%|▉         | 416/4687 [04:25<45:12,  1.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","  9%|▉         | 434/4687 [04:36<44:57,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 434/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7750078631985572 \n","Training Accuracy per last 992 samples: 36.99596774193548\n"]},{"name":"stderr","output_type":"stream","text":[" 11%|█         | 496/4687 [05:16<45:57,  1.52it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 496/4687 of epoch 2 complete. Loss per last 992 samples:: 0.76623962771508 \n","Training Accuracy per last 992 samples: 41.12903225806452\n"]},{"name":"stderr","output_type":"stream","text":[" 12%|█▏        | 558/4687 [05:56<43:47,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 558/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7605160743959488 \n","Training Accuracy per last 992 samples: 39.516129032258064\n"]},{"name":"stderr","output_type":"stream","text":[" 13%|█▎        | 620/4687 [06:36<43:07,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 620/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7569977237332252 \n","Training Accuracy per last 992 samples: 40.725806451612904\n"]},{"name":"stderr","output_type":"stream","text":[" 13%|█▎        | 624/4687 [06:38<43:17,  1.56it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, batch 625 complete! Training Loss : 0.7664537155151367\n","Epoch 2, batch 625 complete! Training Accuracy : 0.3867\n"]},{"name":"stderr","output_type":"stream","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","100%|██████████| 521/521 [01:57<00:00,  4.44it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, batch 625 complete! Validation Loss : 1.6550503838771593\n","Epoch 2, batch 625 complete! Validation Accuracy : 0.310535157187425\n","Validation loss changed from 1.6460307701535508 to 1.6550503838771593\n","Best validation accuracy improved from 0.3062155027597792 to 0.310535157187425\n","\n"]},{"name":"stderr","output_type":"stream","text":[" 13%|█▎        | 625/4687 [08:37<40:46:28, 36.14s/it]"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Wed Nov 17 14:26:43 2021_lr_2e-05_val_acc_0.3105_ep_2.pt\n"]},{"name":"stderr","output_type":"stream","text":[" 14%|█▎        | 641/4687 [08:47<50:39,  1.33it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 14%|█▍        | 668/4687 [09:05<42:49,  1.56it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 15%|█▍        | 682/4687 [09:13<42:39,  1.56it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 682/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7745527759675057 \n","Training Accuracy per last 992 samples: 36.391129032258064\n"]},{"name":"stderr","output_type":"stream","text":[" 16%|█▌        | 734/4687 [09:46<41:54,  1.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 16%|█▌        | 744/4687 [09:53<41:53,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 744/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7679657167004 \n","Training Accuracy per last 992 samples: 40.725806451612904\n"]},{"name":"stderr","output_type":"stream","text":[" 17%|█▋        | 778/4687 [10:14<41:33,  1.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 17%|█▋        | 806/4687 [10:32<41:13,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 806/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7786529602543 \n","Training Accuracy per last 992 samples: 36.391129032258064\n"]},{"name":"stderr","output_type":"stream","text":[" 19%|█▊        | 868/4687 [11:12<40:35,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 868/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7815728956653226 \n","Training Accuracy per last 992 samples: 36.895161290322584\n"]},{"name":"stderr","output_type":"stream","text":[" 20%|█▉        | 930/4687 [11:51<40:01,  1.56it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 930/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7620827920975224 \n","Training Accuracy per last 992 samples: 40.020161290322584\n"]},{"name":"stderr","output_type":"stream","text":[" 21%|██        | 992/4687 [12:31<39:21,  1.56it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 992/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7501256389002646 \n","Training Accuracy per last 992 samples: 39.516129032258064\n"]},{"name":"stderr","output_type":"stream","text":[" 22%|██▏       | 1054/4687 [13:10<38:27,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1054/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7515151116155809 \n","Training Accuracy per last 992 samples: 38.50806451612903\n"]},{"name":"stderr","output_type":"stream","text":[" 23%|██▎       | 1079/4687 [13:26<37:59,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 24%|██▍       | 1116/4687 [13:50<37:49,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1116/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7576832925119708 \n","Training Accuracy per last 992 samples: 37.70161290322581\n"]},{"name":"stderr","output_type":"stream","text":[" 25%|██▌       | 1178/4687 [14:29<37:11,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1178/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7479874087918189 \n","Training Accuracy per last 992 samples: 39.11290322580645\n"]},{"name":"stderr","output_type":"stream","text":[" 26%|██▌       | 1213/4687 [14:51<36:42,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 26%|██▋       | 1237/4687 [15:06<36:27,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 26%|██▋       | 1240/4687 [15:08<36:49,  1.56it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1240/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7400698815622637 \n","Training Accuracy per last 992 samples: 39.91935483870968\n"]},{"name":"stderr","output_type":"stream","text":[" 27%|██▋       | 1249/4687 [15:14<36:19,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, batch 1250 complete! Training Loss : 0.763573681640625\n","Epoch 2, batch 1250 complete! Training Accuracy : 0.3865\n"]},{"name":"stderr","output_type":"stream","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","100%|██████████| 521/521 [01:59<00:00,  4.37it/s]\n"," 27%|██▋       | 1250/4687 [17:14<34:47:22, 36.44s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, batch 1250 complete! Validation Loss : 1.6644936120441458\n","Epoch 2, batch 1250 complete! Validation Accuracy : 0.3099352051835853\n"]},{"name":"stderr","output_type":"stream","text":[" 28%|██▊       | 1302/4687 [17:47<35:51,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1302/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7350474634478169 \n","Training Accuracy per last 992 samples: 40.32258064516129\n"]},{"name":"stderr","output_type":"stream","text":[" 29%|██▉       | 1364/4687 [18:27<35:18,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1364/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7376124474310106 \n","Training Accuracy per last 992 samples: 42.13709677419355\n"]},{"name":"stderr","output_type":"stream","text":[" 29%|██▉       | 1372/4687 [18:32<35:34,  1.55it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 30%|██▉       | 1389/4687 [18:43<34:46,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 30%|███       | 1426/4687 [19:06<34:40,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1426/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7460299845664732 \n","Training Accuracy per last 992 samples: 40.92741935483871\n"]},{"name":"stderr","output_type":"stream","text":[" 31%|███       | 1441/4687 [19:16<34:14,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 31%|███▏      | 1473/4687 [19:37<34:00,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 32%|███▏      | 1488/4687 [19:46<33:56,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1488/4687 of epoch 2 complete. Loss per last 992 samples:: 0.741836009487029 \n","Training Accuracy per last 992 samples: 40.32258064516129\n"]},{"name":"stderr","output_type":"stream","text":[" 33%|███▎      | 1550/4687 [20:26<33:18,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1550/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7332339132985761 \n","Training Accuracy per last 992 samples: 40.92741935483871\n"]},{"name":"stderr","output_type":"stream","text":[" 34%|███▍      | 1586/4687 [20:49<32:56,  1.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 34%|███▍      | 1612/4687 [21:05<32:39,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1612/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7686242749614101 \n","Training Accuracy per last 992 samples: 40.42338709677419\n"]},{"name":"stderr","output_type":"stream","text":[" 36%|███▌      | 1672/4687 [21:44<31:54,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 36%|███▌      | 1674/4687 [21:45<32:39,  1.54it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1674/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7500326864181026 \n","Training Accuracy per last 992 samples: 40.725806451612904\n"]},{"name":"stderr","output_type":"stream","text":[" 37%|███▋      | 1736/4687 [22:25<31:12,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1736/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7489033052998204 \n","Training Accuracy per last 992 samples: 39.71774193548387\n"]},{"name":"stderr","output_type":"stream","text":[" 38%|███▊      | 1798/4687 [23:04<30:35,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1798/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7414871800330377 \n","Training Accuracy per last 992 samples: 40.92741935483871\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|███▉      | 1860/4687 [23:43<29:56,  1.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1860/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7434848969982516 \n","Training Accuracy per last 992 samples: 39.818548387096776\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|███▉      | 1874/4687 [23:52<29:50,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, batch 1875 complete! Training Loss : 0.7571380521138509\n","Epoch 2, batch 1875 complete! Training Accuracy : 0.39293333333333336\n"]},{"name":"stderr","output_type":"stream","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","100%|██████████| 521/521 [01:58<00:00,  4.39it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, batch 1875 complete! Validation Loss : 1.6619331813819578\n","Epoch 2, batch 1875 complete! Validation Accuracy : 0.3132949364050876\n","Validation loss changed from 1.6550503838771593 to 1.6619331813819578\n","Best validation accuracy improved from 0.310535157187425 to 0.3132949364050876\n","\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|████      | 1875/4687 [25:53<28:31:32, 36.52s/it]"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Wed Nov 17 14:43:59 2021_lr_2e-05_val_acc_0.3133_ep_2.pt\n"]},{"name":"stderr","output_type":"stream","text":[" 41%|████      | 1922/4687 [26:23<29:18,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1922/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7320367751582977 \n","Training Accuracy per last 992 samples: 40.92741935483871\n"]},{"name":"stderr","output_type":"stream","text":[" 42%|████▏     | 1984/4687 [27:02<28:37,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1984/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7496835647090789 \n","Training Accuracy per last 992 samples: 39.516129032258064\n"]},{"name":"stderr","output_type":"stream","text":[" 44%|████▎     | 2046/4687 [27:42<28:01,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2046/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7494564210214922 \n","Training Accuracy per last 992 samples: 40.32258064516129\n"]},{"name":"stderr","output_type":"stream","text":[" 44%|████▍     | 2062/4687 [27:52<27:59,  1.56it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 45%|████▍     | 2108/4687 [28:21<27:22,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2108/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7489196792725594 \n","Training Accuracy per last 992 samples: 39.11290322580645\n"]},{"name":"stderr","output_type":"stream","text":[" 46%|████▋     | 2170/4687 [29:01<26:42,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2170/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7337824452307916 \n","Training Accuracy per last 992 samples: 43.75\n"]},{"name":"stderr","output_type":"stream","text":[" 48%|████▊     | 2232/4687 [29:40<26:03,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2232/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7387240240650792 \n","Training Accuracy per last 992 samples: 39.818548387096776\n"]},{"name":"stderr","output_type":"stream","text":[" 49%|████▉     | 2294/4687 [30:19<25:22,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2294/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7487140624753891 \n","Training Accuracy per last 992 samples: 39.41532258064516\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 2356/4687 [30:59<24:42,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2356/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7430664723919284 \n","Training Accuracy per last 992 samples: 38.608870967741936\n"]},{"name":"stderr","output_type":"stream","text":[" 51%|█████▏    | 2404/4687 [31:29<24:15,  1.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 52%|█████▏    | 2418/4687 [31:39<24:08,  1.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2418/4687 of epoch 2 complete. Loss per last 992 samples:: 0.733828729198825 \n","Training Accuracy per last 992 samples: 40.12096774193548\n"]},{"name":"stderr","output_type":"stream","text":[" 53%|█████▎    | 2480/4687 [32:18<23:21,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2480/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7192505790341285 \n","Training Accuracy per last 992 samples: 43.24596774193548\n"]},{"name":"stderr","output_type":"stream","text":[" 53%|█████▎    | 2499/4687 [32:30<23:06,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, batch 2500 complete! Training Loss : 0.7526733331680298\n","Epoch 2, batch 2500 complete! Training Accuracy : 0.3964\n"]},{"name":"stderr","output_type":"stream","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","100%|██████████| 521/521 [01:58<00:00,  4.40it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, batch 2500 complete! Validation Loss : 1.668396113243762\n","Epoch 2, batch 2500 complete! Validation Accuracy : 0.3230141588672906\n","Validation loss changed from 1.6619331813819578 to 1.668396113243762\n","Best validation accuracy improved from 0.3132949364050876 to 0.3230141588672906\n","\n"]},{"name":"stderr","output_type":"stream","text":[" 53%|█████▎    | 2500/4687 [34:30<22:08:40, 36.45s/it]"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Wed Nov 17 14:52:36 2021_lr_2e-05_val_acc_0.323_ep_2.pt\n"]},{"name":"stderr","output_type":"stream","text":[" 54%|█████▍    | 2541/4687 [34:56<22:37,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 54%|█████▍    | 2542/4687 [34:57<22:57,  1.56it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2542/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7310873462307838 \n","Training Accuracy per last 992 samples: 43.649193548387096\n"]},{"name":"stderr","output_type":"stream","text":[" 54%|█████▍    | 2546/4687 [34:59<22:50,  1.56it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 56%|█████▌    | 2604/4687 [35:36<22:02,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2604/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7422285925957465 \n","Training Accuracy per last 992 samples: 39.61693548387097\n"]},{"name":"stderr","output_type":"stream","text":[" 57%|█████▋    | 2666/4687 [36:15<21:24,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2666/4687 of epoch 2 complete. Loss per last 992 samples:: 0.730851496419599 \n","Training Accuracy per last 992 samples: 41.63306451612903\n"]},{"name":"stderr","output_type":"stream","text":[" 58%|█████▊    | 2717/4687 [36:48<20:44,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 58%|█████▊    | 2728/4687 [36:55<20:44,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2728/4687 of epoch 2 complete. Loss per last 992 samples:: 0.721354523012715 \n","Training Accuracy per last 992 samples: 43.04435483870968\n"]},{"name":"stderr","output_type":"stream","text":[" 60%|█████▉    | 2790/4687 [37:34<20:06,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2790/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7309569081952495 \n","Training Accuracy per last 992 samples: 41.63306451612903\n"]},{"name":"stderr","output_type":"stream","text":[" 61%|██████    | 2852/4687 [38:13<19:23,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2852/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7582759857177734 \n","Training Accuracy per last 992 samples: 38.608870967741936\n"]},{"name":"stderr","output_type":"stream","text":[" 61%|██████▏   | 2871/4687 [38:25<19:09,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 62%|██████▏   | 2914/4687 [38:53<18:49,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2914/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7275115751451061 \n","Training Accuracy per last 992 samples: 42.03629032258065\n"]},{"name":"stderr","output_type":"stream","text":[" 63%|██████▎   | 2976/4687 [39:32<18:08,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2976/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7181802257414787 \n","Training Accuracy per last 992 samples: 42.23790322580645\n"]},{"name":"stderr","output_type":"stream","text":[" 64%|██████▎   | 2983/4687 [39:37<17:59,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 64%|██████▍   | 3014/4687 [39:56<17:41,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 65%|██████▍   | 3038/4687 [40:11<17:29,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3038/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7346097269365864 \n","Training Accuracy per last 992 samples: 41.935483870967744\n"]},{"name":"stderr","output_type":"stream","text":[" 66%|██████▌   | 3100/4687 [40:51<16:43,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3100/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7119222379499867 \n","Training Accuracy per last 992 samples: 43.75\n"]},{"name":"stderr","output_type":"stream","text":[" 66%|██████▌   | 3103/4687 [40:52<16:41,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 67%|██████▋   | 3124/4687 [41:06<16:30,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, batch 3125 complete! Training Loss : 0.7481919342041016\n","Epoch 2, batch 3125 complete! Training Accuracy : 0.40082\n"]},{"name":"stderr","output_type":"stream","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","100%|██████████| 521/521 [01:58<00:00,  4.38it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, batch 3125 complete! Validation Loss : 1.6971194217850287\n","Epoch 2, batch 3125 complete! Validation Accuracy : 0.32433405327573794\n","Validation loss changed from 1.668396113243762 to 1.6971194217850287\n","Best validation accuracy improved from 0.3230141588672906 to 0.32433405327573794\n","\n"]},{"name":"stderr","output_type":"stream","text":[" 67%|██████▋   | 3125/4687 [43:06<15:52:19, 36.58s/it]"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Wed Nov 17 15:01:12 2021_lr_2e-05_val_acc_0.3243_ep_2.pt\n"]},{"name":"stderr","output_type":"stream","text":[" 67%|██████▋   | 3162/4687 [43:30<16:08,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3162/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7140499930227956 \n","Training Accuracy per last 992 samples: 43.95161290322581\n"]},{"name":"stderr","output_type":"stream","text":[" 69%|██████▉   | 3224/4687 [44:09<15:30,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3224/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7242419950423702 \n","Training Accuracy per last 992 samples: 42.13709677419355\n"]},{"name":"stderr","output_type":"stream","text":[" 70%|███████   | 3286/4687 [44:49<14:52,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3286/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7203181789767358 \n","Training Accuracy per last 992 samples: 42.439516129032256\n"]},{"name":"stderr","output_type":"stream","text":[" 71%|███████▏  | 3348/4687 [45:28<14:13,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3348/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7038200132308468 \n","Training Accuracy per last 992 samples: 44.05241935483871\n"]},{"name":"stderr","output_type":"stream","text":[" 72%|███████▏  | 3375/4687 [45:45<13:51,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 73%|███████▎  | 3410/4687 [46:07<13:33,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3410/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7230272370000039 \n","Training Accuracy per last 992 samples: 43.145161290322584\n"]},{"name":"stderr","output_type":"stream","text":[" 73%|███████▎  | 3431/4687 [46:21<13:13,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 74%|███████▍  | 3472/4687 [46:47<12:52,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3472/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7040488873758624 \n","Training Accuracy per last 992 samples: 44.45564516129032\n"]},{"name":"stderr","output_type":"stream","text":[" 75%|███████▌  | 3534/4687 [47:26<12:13,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3534/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7187341336281069 \n","Training Accuracy per last 992 samples: 42.23790322580645\n"]},{"name":"stderr","output_type":"stream","text":[" 77%|███████▋  | 3596/4687 [48:06<11:34,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3596/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7171215088136734 \n","Training Accuracy per last 992 samples: 45.060483870967744\n"]},{"name":"stderr","output_type":"stream","text":[" 78%|███████▊  | 3658/4687 [48:45<10:55,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3658/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7029880631354547 \n","Training Accuracy per last 992 samples: 43.649193548387096\n"]},{"name":"stderr","output_type":"stream","text":[" 79%|███████▉  | 3720/4687 [49:24<10:15,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3720/4687 of epoch 2 complete. Loss per last 992 samples:: 0.7062036145117975 \n","Training Accuracy per last 992 samples: 42.84274193548387\n"]},{"name":"stderr","output_type":"stream","text":[" 80%|███████▉  | 3749/4687 [49:43<09:54,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, batch 3750 complete! Training Loss : 0.7426276889801026\n","Epoch 2, batch 3750 complete! Training Accuracy : 0.40595\n"]},{"name":"stderr","output_type":"stream","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","100%|██████████| 521/521 [01:57<00:00,  4.43it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, batch 3750 complete! Validation Loss : 1.7073161588291748\n","Epoch 2, batch 3750 complete! Validation Accuracy : 0.3249340052795776\n","Validation loss changed from 1.6971194217850287 to 1.7073161588291748\n","Best validation accuracy improved from 0.32433405327573794 to 0.3249340052795776\n","\n"]},{"name":"stderr","output_type":"stream","text":[" 80%|███████▉  | 3749/4687 [51:42<12:56,  1.21it/s]\n"]},{"ename":"RuntimeError","evalue":"[enforce fail at inline_container.cc:300] . unexpected pos 446702208 vs 446702096","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m~/anaconda3/envs/nn/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/nn/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0mnum_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melement_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m         \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_3189/367460629.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopti\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters_to_accumulate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_3189/1783964205.py\u001b[0m in \u001b[0;36mtrain_bert\u001b[0;34m(net, criterion, opti, lr, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate)\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0;31m# Saving the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                     \u001b[0mpath_to_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'models/{}_lr_{}_val_acc_{}_ep_{}.pt'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_ep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The model has been saved in {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/nn/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/nn/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:300] . unexpected pos 446702208 vs 446702096"]}],"source":["train_bert(net, criterion, opti, LEARNING_RATE, lr_scheduler, train_loader, val_loader, EPOCHS, iters_to_accumulate)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  1%|▏         | 62/4687 [00:43<54:12,  1.42it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 62/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8997984855405746 \n","Training Accuracy per last 992 samples: 18.245967741935484\n"]},{"name":"stderr","output_type":"stream","text":["  3%|▎         | 124/4687 [01:27<54:11,  1.40it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 124/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8957706574470766 \n","Training Accuracy per last 992 samples: 19.254032258064516\n"]},{"name":"stderr","output_type":"stream","text":["  4%|▍         | 186/4687 [02:11<52:31,  1.43it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 186/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8977075392200101 \n","Training Accuracy per last 992 samples: 18.951612903225808\n"]},{"name":"stderr","output_type":"stream","text":["  5%|▌         | 246/4687 [02:53<52:21,  1.41it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","  5%|▌         | 248/4687 [02:54<52:43,  1.40it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 248/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8957189744518649 \n","Training Accuracy per last 992 samples: 18.951612903225808\n"]},{"name":"stderr","output_type":"stream","text":["  7%|▋         | 310/4687 [03:38<52:12,  1.40it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 310/4687 of epoch 1 complete. Loss per last 992 samples:: 0.896025134671119 \n","Training Accuracy per last 992 samples: 16.633064516129032\n"]},{"name":"stderr","output_type":"stream","text":["  8%|▊         | 372/4687 [04:22<50:28,  1.42it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 372/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8999102192540323 \n","Training Accuracy per last 992 samples: 17.338709677419356\n"]},{"name":"stderr","output_type":"stream","text":["  9%|▉         | 434/4687 [05:05<49:26,  1.43it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 434/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8994224302230343 \n","Training Accuracy per last 992 samples: 16.028225806451612\n"]},{"name":"stderr","output_type":"stream","text":[" 11%|█         | 496/4687 [05:48<48:53,  1.43it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 496/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8978699714906754 \n","Training Accuracy per last 992 samples: 16.532258064516128\n"]},{"name":"stderr","output_type":"stream","text":[" 12%|█▏        | 557/4687 [06:31<48:13,  1.43it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 12%|█▏        | 558/4687 [06:32<48:47,  1.41it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 558/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8977134458480343 \n","Training Accuracy per last 992 samples: 16.028225806451612\n"]},{"name":"stderr","output_type":"stream","text":[" 13%|█▎        | 620/4687 [07:11<43:01,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 620/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8994559011151714 \n","Training Accuracy per last 992 samples: 15.524193548387096\n"]},{"name":"stderr","output_type":"stream","text":[" 13%|█▎        | 624/4687 [07:14<44:26,  1.52it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 625 complete! Training Loss : 0.89782041015625\n","Epoch 1, batch 625 complete! Training Accuracy : 0.1737\n"]},{"name":"stderr","output_type":"stream","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","100%|██████████| 521/521 [01:57<00:00,  4.42it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 625 complete! Validation Loss : 1.790923779390595\n","Epoch 1, batch 625 complete! Validation Accuracy : 0.1767458603311735\n","Validation loss changed from inf to 1.790923779390595\n","Best validation accuracy improved from 0 to 0.1767458603311735\n","\n"]},{"name":"stderr","output_type":"stream","text":[" 13%|█▎        | 625/4687 [09:13<40:53:26, 36.24s/it]"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Wed Nov 17 00:02:19 2021_lr_2e-05_val_acc_0.1767_ep_1.pt\n"]},{"name":"stderr","output_type":"stream","text":[" 13%|█▎        | 629/4687 [09:16<10:21:15,  9.19s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 15%|█▍        | 682/4687 [09:50<42:31,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 682/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8958380914503529 \n","Training Accuracy per last 992 samples: 17.943548387096776\n"]},{"name":"stderr","output_type":"stream","text":[" 15%|█▌        | 726/4687 [10:18<42:01,  1.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 16%|█▌        | 744/4687 [10:29<41:44,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 744/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8977326423891129 \n","Training Accuracy per last 992 samples: 16.633064516129032\n"]},{"name":"stderr","output_type":"stream","text":[" 17%|█▋        | 806/4687 [11:09<41:06,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 806/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8963485225554435 \n","Training Accuracy per last 992 samples: 19.153225806451612\n"]},{"name":"stderr","output_type":"stream","text":[" 19%|█▊        | 868/4687 [11:48<40:28,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 868/4687 of epoch 1 complete. Loss per last 992 samples:: 0.9004098215410786 \n","Training Accuracy per last 992 samples: 16.330645161290324\n"]},{"name":"stderr","output_type":"stream","text":[" 19%|█▊        | 874/4687 [11:52<40:27,  1.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 20%|█▉        | 930/4687 [12:28<39:57,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 930/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8980442170173891 \n","Training Accuracy per last 992 samples: 16.93548387096774\n"]},{"name":"stderr","output_type":"stream","text":[" 21%|██        | 983/4687 [13:01<39:03,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 21%|██        | 992/4687 [13:07<39:16,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 992/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8969185121597782 \n","Training Accuracy per last 992 samples: 19.153225806451612\n"]},{"name":"stderr","output_type":"stream","text":[" 22%|██▏       | 1054/4687 [13:46<38:35,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1054/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8957765640751008 \n","Training Accuracy per last 992 samples: 17.741935483870968\n"]},{"name":"stderr","output_type":"stream","text":[" 24%|██▍       | 1116/4687 [14:26<37:46,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1116/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8961329306325605 \n","Training Accuracy per last 992 samples: 17.741935483870968\n"]},{"name":"stderr","output_type":"stream","text":[" 25%|██▌       | 1175/4687 [15:03<37:07,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 25%|██▌       | 1178/4687 [15:05<37:27,  1.56it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1178/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8956259450604839 \n","Training Accuracy per last 992 samples: 17.036290322580644\n"]},{"name":"stderr","output_type":"stream","text":[" 26%|██▋       | 1240/4687 [15:44<36:34,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1240/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8969785628780242 \n","Training Accuracy per last 992 samples: 17.943548387096776\n"]},{"name":"stderr","output_type":"stream","text":[" 27%|██▋       | 1249/4687 [15:50<36:21,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 1250 complete! Training Loss : 0.8973997802734375\n","Epoch 1, batch 1250 complete! Training Accuracy : 0.17525\n"]},{"name":"stderr","output_type":"stream","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","100%|██████████| 521/521 [01:56<00:00,  4.46it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 1250 complete! Validation Loss : 1.782687664947217\n","Epoch 1, batch 1250 complete! Validation Accuracy : 0.19462443004559635\n","Validation loss changed from 1.790923779390595 to 1.782687664947217\n","Best validation accuracy improved from 0.1767458603311735 to 0.19462443004559635\n","\n"]},{"name":"stderr","output_type":"stream","text":[" 27%|██▋       | 1250/4687 [17:49<34:18:41, 35.94s/it]"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Wed Nov 17 00:10:54 2021_lr_2e-05_val_acc_0.1946_ep_1.pt\n"]},{"name":"stderr","output_type":"stream","text":[" 28%|██▊       | 1302/4687 [18:21<35:50,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1302/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8880187003843246 \n","Training Accuracy per last 992 samples: 19.556451612903224\n"]},{"name":"stderr","output_type":"stream","text":[" 28%|██▊       | 1322/4687 [18:34<35:30,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 29%|██▉       | 1364/4687 [19:01<37:45,  1.47it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1364/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8879564346805695 \n","Training Accuracy per last 992 samples: 23.588709677419356\n"]},{"name":"stderr","output_type":"stream","text":[" 30%|███       | 1426/4687 [19:40<34:43,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1426/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8893147130166331 \n","Training Accuracy per last 992 samples: 18.850806451612904\n"]},{"name":"stderr","output_type":"stream","text":[" 30%|███       | 1428/4687 [19:42<34:37,  1.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 31%|███▏      | 1471/4687 [20:09<33:54,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 32%|███▏      | 1479/4687 [20:14<33:41,  1.59it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 32%|███▏      | 1488/4687 [20:20<33:52,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1488/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8877735753213206 \n","Training Accuracy per last 992 samples: 20.766129032258064\n"]},{"name":"stderr","output_type":"stream","text":[" 33%|███▎      | 1550/4687 [20:59<33:05,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1550/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8886691677954889 \n","Training Accuracy per last 992 samples: 19.35483870967742\n"]},{"name":"stderr","output_type":"stream","text":[" 33%|███▎      | 1557/4687 [21:03<32:48,  1.59it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 34%|███▍      | 1612/4687 [21:38<32:28,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1612/4687 of epoch 1 complete. Loss per last 992 samples:: 0.890870371172505 \n","Training Accuracy per last 992 samples: 20.866935483870968\n"]},{"name":"stderr","output_type":"stream","text":[" 36%|███▌      | 1674/4687 [22:17<31:44,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1674/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8858305408108619 \n","Training Accuracy per last 992 samples: 22.580645161290324\n"]},{"name":"stderr","output_type":"stream","text":[" 36%|███▌      | 1698/4687 [22:32<33:29,  1.49it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 37%|███▋      | 1736/4687 [22:58<32:00,  1.54it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1736/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8921368506646925 \n","Training Accuracy per last 992 samples: 19.657258064516128\n"]},{"name":"stderr","output_type":"stream","text":[" 38%|███▊      | 1781/4687 [23:27<30:40,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 38%|███▊      | 1798/4687 [23:38<30:35,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1798/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8933051324659779 \n","Training Accuracy per last 992 samples: 21.169354838709676\n"]},{"name":"stderr","output_type":"stream","text":[" 39%|███▊      | 1806/4687 [23:43<30:31,  1.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 40%|███▉      | 1860/4687 [24:17<30:00,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1860/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8858573667464718 \n","Training Accuracy per last 992 samples: 22.278225806451612\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|███▉      | 1874/4687 [24:26<29:51,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 1875 complete! Training Loss : 0.8946413330078125\n","Epoch 1, batch 1875 complete! Training Accuracy : 0.18613333333333335\n"]},{"name":"stderr","output_type":"stream","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","100%|██████████| 521/521 [01:58<00:00,  4.39it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 1875 complete! Validation Loss : 1.7815067928262955\n","Epoch 1, batch 1875 complete! Validation Accuracy : 0.2047036237101032\n","Validation loss changed from 1.782687664947217 to 1.7815067928262955\n","Best validation accuracy improved from 0.19462443004559635 to 0.2047036237101032\n","\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|████      | 1875/4687 [26:26<28:31:12, 36.51s/it]"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Wed Nov 17 00:19:32 2021_lr_2e-05_val_acc_0.2047_ep_1.pt\n"]},{"name":"stderr","output_type":"stream","text":[" 41%|████      | 1922/4687 [26:56<29:18,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1922/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8939964540543095 \n","Training Accuracy per last 992 samples: 20.262096774193548\n"]},{"name":"stderr","output_type":"stream","text":[" 42%|████▏     | 1962/4687 [27:22<28:52,  1.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 42%|████▏     | 1984/4687 [27:36<28:40,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1984/4687 of epoch 1 complete. Loss per last 992 samples:: 0.889313482469128 \n","Training Accuracy per last 992 samples: 20.766129032258064\n"]},{"name":"stderr","output_type":"stream","text":[" 44%|████▎     | 2046/4687 [28:15<28:03,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2046/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8870775776524698 \n","Training Accuracy per last 992 samples: 20.967741935483872\n"]},{"name":"stderr","output_type":"stream","text":[" 44%|████▍     | 2070/4687 [28:30<27:41,  1.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 45%|████▍     | 2108/4687 [28:54<27:24,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2108/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8761852633568549 \n","Training Accuracy per last 992 samples: 24.193548387096776\n"]},{"name":"stderr","output_type":"stream","text":[" 45%|████▌     | 2131/4687 [29:09<28:20,  1.50it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 46%|████▋     | 2170/4687 [29:34<26:42,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2170/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8803905364005796 \n","Training Accuracy per last 992 samples: 21.975806451612904\n"]},{"name":"stderr","output_type":"stream","text":[" 48%|████▊     | 2232/4687 [30:13<26:05,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2232/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8863788727791079 \n","Training Accuracy per last 992 samples: 19.153225806451612\n"]},{"name":"stderr","output_type":"stream","text":[" 48%|████▊     | 2247/4687 [30:23<25:45,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 48%|████▊     | 2265/4687 [30:34<25:36,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 49%|████▊     | 2279/4687 [30:43<25:25,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 49%|████▉     | 2294/4687 [30:53<25:23,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2294/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8874143785045993 \n","Training Accuracy per last 992 samples: 20.967741935483872\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 2356/4687 [31:32<24:41,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2356/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8773342870896862 \n","Training Accuracy per last 992 samples: 23.487903225806452\n"]},{"name":"stderr","output_type":"stream","text":[" 51%|█████     | 2381/4687 [31:48<24:18,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 52%|█████▏    | 2418/4687 [32:12<24:07,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2418/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8810044565508443 \n","Training Accuracy per last 992 samples: 22.782258064516128\n"]},{"name":"stderr","output_type":"stream","text":[" 53%|█████▎    | 2480/4687 [32:51<23:25,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2480/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8859673161660472 \n","Training Accuracy per last 992 samples: 21.471774193548388\n"]},{"name":"stderr","output_type":"stream","text":[" 53%|█████▎    | 2499/4687 [33:03<23:05,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 2500 complete! Training Loss : 0.891921142578125\n","Epoch 1, batch 2500 complete! Training Accuracy : 0.194275\n"]},{"name":"stderr","output_type":"stream","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","100%|██████████| 521/521 [01:57<00:00,  4.43it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 2500 complete! Validation Loss : 1.7541368012236085\n","Epoch 1, batch 2500 complete! Validation Accuracy : 0.23758099352051837\n","Validation loss changed from 1.7815067928262955 to 1.7541368012236085\n","Best validation accuracy improved from 0.2047036237101032 to 0.23758099352051837\n","\n"]},{"name":"stderr","output_type":"stream","text":[" 53%|█████▎    | 2500/4687 [35:02<21:59:58, 36.21s/it]"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Wed Nov 17 00:28:08 2021_lr_2e-05_val_acc_0.2376_ep_1.pt\n"]},{"name":"stderr","output_type":"stream","text":[" 54%|█████▍    | 2542/4687 [35:29<22:47,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2542/4687 of epoch 1 complete. Loss per last 992 samples:: 0.867569338890814 \n","Training Accuracy per last 992 samples: 26.512096774193548\n"]},{"name":"stderr","output_type":"stream","text":[" 56%|█████▌    | 2604/4687 [36:09<22:19,  1.55it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2604/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8714471017160723 \n","Training Accuracy per last 992 samples: 22.379032258064516\n"]},{"name":"stderr","output_type":"stream","text":[" 57%|█████▋    | 2666/4687 [36:48<21:19,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2666/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8767431935956401 \n","Training Accuracy per last 992 samples: 22.580645161290324\n"]},{"name":"stderr","output_type":"stream","text":[" 58%|█████▊    | 2728/4687 [37:28<20:53,  1.56it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2728/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8858753327400454 \n","Training Accuracy per last 992 samples: 19.35483870967742\n"]},{"name":"stderr","output_type":"stream","text":[" 60%|█████▉    | 2790/4687 [38:07<20:08,  1.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2790/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8679041708669355 \n","Training Accuracy per last 992 samples: 25.100806451612904\n"]},{"name":"stderr","output_type":"stream","text":[" 61%|██████    | 2852/4687 [38:46<19:26,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2852/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8764837941815776 \n","Training Accuracy per last 992 samples: 22.47983870967742\n"]},{"name":"stderr","output_type":"stream","text":[" 61%|██████    | 2861/4687 [38:52<19:17,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 62%|██████▏   | 2903/4687 [39:19<18:51,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 62%|██████▏   | 2914/4687 [39:26<18:50,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2914/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8873160577589466 \n","Training Accuracy per last 992 samples: 20.66532258064516\n"]},{"name":"stderr","output_type":"stream","text":[" 63%|██████▎   | 2976/4687 [40:05<18:06,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 2976/4687 of epoch 1 complete. Loss per last 992 samples:: 0.875655758765436 \n","Training Accuracy per last 992 samples: 22.076612903225808\n"]},{"name":"stderr","output_type":"stream","text":[" 65%|██████▍   | 3038/4687 [40:45<17:27,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3038/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8788001768050655 \n","Training Accuracy per last 992 samples: 22.177419354838708\n"]},{"name":"stderr","output_type":"stream","text":[" 66%|██████▌   | 3100/4687 [41:24<16:49,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3100/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8626135549237651 \n","Training Accuracy per last 992 samples: 25.403225806451612\n"]},{"name":"stderr","output_type":"stream","text":[" 66%|██████▋   | 3111/4687 [41:31<16:36,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 67%|██████▋   | 3123/4687 [41:39<16:27,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 67%|██████▋   | 3124/4687 [41:39<16:34,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 3125 complete! Training Loss : 0.8883422717285157\n","Epoch 1, batch 3125 complete! Training Accuracy : 0.20112\n"]},{"name":"stderr","output_type":"stream","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","100%|██████████| 521/521 [01:57<00:00,  4.42it/s]\n"," 67%|██████▋   | 3125/4687 [43:38<15:36:33, 35.98s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 3125 complete! Validation Loss : 1.754974658109405\n","Epoch 1, batch 3125 complete! Validation Accuracy : 0.2299016078713703\n"]},{"name":"stderr","output_type":"stream","text":[" 67%|██████▋   | 3146/4687 [43:51<16:48,  1.53it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 67%|██████▋   | 3162/4687 [44:01<16:11,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3162/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8685407330912929 \n","Training Accuracy per last 992 samples: 22.278225806451612\n"]},{"name":"stderr","output_type":"stream","text":[" 68%|██████▊   | 3209/4687 [44:31<15:36,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 69%|██████▉   | 3224/4687 [44:41<15:27,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3224/4687 of epoch 1 complete. Loss per last 992 samples:: 0.88502687023532 \n","Training Accuracy per last 992 samples: 20.161290322580644\n"]},{"name":"stderr","output_type":"stream","text":[" 70%|██████▉   | 3270/4687 [45:10<14:59,  1.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 70%|███████   | 3286/4687 [45:20<14:49,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3286/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8749405645555065 \n","Training Accuracy per last 992 samples: 23.487903225806452\n"]},{"name":"stderr","output_type":"stream","text":[" 71%|███████▏  | 3348/4687 [46:00<14:12,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3348/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8652577554025958 \n","Training Accuracy per last 992 samples: 24.495967741935484\n"]},{"name":"stderr","output_type":"stream","text":[" 73%|███████▎  | 3410/4687 [46:39<13:34,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3410/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8798151323872228 \n","Training Accuracy per last 992 samples: 21.370967741935484\n"]},{"name":"stderr","output_type":"stream","text":[" 74%|███████▍  | 3472/4687 [47:18<12:52,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3472/4687 of epoch 1 complete. Loss per last 992 samples:: 0.896026365218624 \n","Training Accuracy per last 992 samples: 17.43951612903226\n"]},{"name":"stderr","output_type":"stream","text":[" 75%|███████▌  | 3534/4687 [47:58<12:11,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3534/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8961430211221019 \n","Training Accuracy per last 992 samples: 17.338709677419356\n"]},{"name":"stderr","output_type":"stream","text":[" 77%|███████▋  | 3596/4687 [48:37<11:32,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3596/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8947793283770161 \n","Training Accuracy per last 992 samples: 17.43951612903226\n"]},{"name":"stderr","output_type":"stream","text":[" 77%|███████▋  | 3619/4687 [48:52<11:14,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 78%|███████▊  | 3658/4687 [49:17<10:53,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3658/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8974584764049899 \n","Training Accuracy per last 992 samples: 18.75\n"]},{"name":"stderr","output_type":"stream","text":[" 79%|███████▉  | 3720/4687 [49:56<10:33,  1.53it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3720/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8988667149697581 \n","Training Accuracy per last 992 samples: 16.22983870967742\n"]},{"name":"stderr","output_type":"stream","text":[" 80%|███████▉  | 3749/4687 [50:15<09:52,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 3750 complete! Training Loss : 0.8882351837158203\n","Epoch 1, batch 3750 complete! Training Accuracy : 0.2001\n"]},{"name":"stderr","output_type":"stream","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","100%|██████████| 521/521 [01:57<00:00,  4.43it/s]\n"," 80%|████████  | 3750/4687 [52:13<9:21:25, 35.95s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 3750 complete! Validation Loss : 1.791203065019194\n","Epoch 1, batch 3750 complete! Validation Accuracy : 0.1771058315334773\n"]},{"name":"stderr","output_type":"stream","text":[" 81%|████████  | 3782/4687 [52:34<09:35,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3782/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8946287093623992 \n","Training Accuracy per last 992 samples: 17.137096774193548\n"]},{"name":"stderr","output_type":"stream","text":[" 82%|████████▏ | 3844/4687 [53:14<09:04,  1.55it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3844/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8954824632213961 \n","Training Accuracy per last 992 samples: 19.758064516129032\n"]},{"name":"stderr","output_type":"stream","text":[" 83%|████████▎ | 3906/4687 [53:53<08:17,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3906/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8973265617124496 \n","Training Accuracy per last 992 samples: 17.43951612903226\n"]},{"name":"stderr","output_type":"stream","text":[" 85%|████████▍ | 3968/4687 [54:33<07:38,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 3968/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8970548568233367 \n","Training Accuracy per last 992 samples: 16.330645161290324\n"]},{"name":"stderr","output_type":"stream","text":[" 86%|████████▌ | 4030/4687 [55:13<06:55,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 4030/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8976967104019657 \n","Training Accuracy per last 992 samples: 17.036290322580644\n"]},{"name":"stderr","output_type":"stream","text":[" 87%|████████▋ | 4092/4687 [55:52<06:16,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 4092/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8953227381552419 \n","Training Accuracy per last 992 samples: 16.330645161290324\n"]},{"name":"stderr","output_type":"stream","text":[" 88%|████████▊ | 4121/4687 [56:10<05:55,  1.59it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 88%|████████▊ | 4128/4687 [56:14<05:53,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 89%|████████▊ | 4154/4687 [56:31<05:36,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 4154/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8995193973664315 \n","Training Accuracy per last 992 samples: 16.93548387096774\n"]},{"name":"stderr","output_type":"stream","text":[" 89%|████████▊ | 4159/4687 [56:34<05:33,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 90%|████████▉ | 4216/4687 [57:10<04:57,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 4216/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8972891530682964 \n","Training Accuracy per last 992 samples: 17.036290322580644\n"]},{"name":"stderr","output_type":"stream","text":[" 91%|█████████▏| 4278/4687 [57:51<04:21,  1.56it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 4278/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8956564626386089 \n","Training Accuracy per last 992 samples: 17.036290322580644\n"]},{"name":"stderr","output_type":"stream","text":[" 93%|█████████▎| 4340/4687 [58:31<03:40,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 4340/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8983287196005544 \n","Training Accuracy per last 992 samples: 16.22983870967742\n"]},{"name":"stderr","output_type":"stream","text":[" 93%|█████████▎| 4374/4687 [58:52<03:18,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 4375 complete! Training Loss : 0.8894846112932477\n","Epoch 1, batch 4375 complete! Training Accuracy : 0.19597142857142857\n"]},{"name":"stderr","output_type":"stream","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","100%|██████████| 521/521 [01:57<00:00,  4.44it/s]\n"," 93%|█████████▎| 4375/4687 [1:00:50<3:06:13, 35.81s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 4375 complete! Validation Loss : 1.7911786978166986\n","Epoch 1, batch 4375 complete! Validation Accuracy : 0.1771058315334773\n"]},{"name":"stderr","output_type":"stream","text":[" 94%|█████████▍| 4402/4687 [1:01:07<03:02,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 4402/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8981997582220262 \n","Training Accuracy per last 992 samples: 15.625\n"]},{"name":"stderr","output_type":"stream","text":[" 94%|█████████▍| 4428/4687 [1:01:24<02:44,  1.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 95%|█████████▌| 4464/4687 [1:01:47<02:21,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 4464/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8946735012915826 \n","Training Accuracy per last 992 samples: 19.153225806451612\n"]},{"name":"stderr","output_type":"stream","text":[" 97%|█████████▋| 4526/4687 [1:02:28<01:53,  1.42it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 4526/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8974574919669859 \n","Training Accuracy per last 992 samples: 13.709677419354838\n"]},{"name":"stderr","output_type":"stream","text":[" 98%|█████████▊| 4588/4687 [1:03:09<01:03,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 4588/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8984173190209174 \n","Training Accuracy per last 992 samples: 18.649193548387096\n"]},{"name":"stderr","output_type":"stream","text":[" 99%|█████████▉| 4650/4687 [1:03:49<00:23,  1.55it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 4650/4687 of epoch 1 complete. Loss per last 992 samples:: 0.8980993455456149 \n","Training Accuracy per last 992 samples: 16.633064516129032\n"]},{"name":"stderr","output_type":"stream","text":[" 99%|█████████▉| 4658/4687 [1:03:54<00:18,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","100%|█████████▉| 4686/4687 [1:04:12<00:00,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 4687 complete! Training Loss : 0.8900047290266666\n","Epoch 1, batch 4687 complete! Training Accuracy : 0.1942143800264074\n"]},{"name":"stderr","output_type":"stream","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","100%|██████████| 521/521 [01:57<00:00,  4.42it/s]\n","100%|██████████| 4687/4687 [1:06:10<00:00,  1.18it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 4687 complete! Validation Loss : 1.790801943378119\n","Epoch 1, batch 4687 complete! Validation Accuracy : 0.1767458603311735\n"]},{"name":"stderr","output_type":"stream","text":["  1%|          | 30/4687 [00:19<49:24,  1.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","  1%|▏         | 62/4687 [00:39<49:05,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 62/4687 of epoch 2 complete. Loss per last 992 samples:: 0.896453857421875 \n","Training Accuracy per last 992 samples: 17.338709677419356\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 93/4687 [00:59<48:24,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","  3%|▎         | 124/4687 [01:18<48:20,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 124/4687 of epoch 2 complete. Loss per last 992 samples:: 0.896515877016129 \n","Training Accuracy per last 992 samples: 17.43951612903226\n"]},{"name":"stderr","output_type":"stream","text":["  4%|▍         | 186/4687 [01:58<47:44,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 186/4687 of epoch 2 complete. Loss per last 992 samples:: 0.8962963473412299 \n","Training Accuracy per last 992 samples: 17.943548387096776\n"]},{"name":"stderr","output_type":"stream","text":["  5%|▌         | 248/4687 [02:38<48:09,  1.54it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 248/4687 of epoch 2 complete. Loss per last 992 samples:: 0.8997374503843246 \n","Training Accuracy per last 992 samples: 17.137096774193548\n"]},{"name":"stderr","output_type":"stream","text":["  7%|▋         | 310/4687 [03:20<47:12,  1.55it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 310/4687 of epoch 2 complete. Loss per last 992 samples:: 0.8949392995526714 \n","Training Accuracy per last 992 samples: 17.943548387096776\n"]},{"name":"stderr","output_type":"stream","text":["  8%|▊         | 372/4687 [04:02<50:52,  1.41it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 372/4687 of epoch 2 complete. Loss per last 992 samples:: 0.8960310412991431 \n","Training Accuracy per last 992 samples: 17.338709677419356\n"]},{"name":"stderr","output_type":"stream","text":["  9%|▉         | 416/4687 [04:31<47:45,  1.49it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","  9%|▉         | 434/4687 [04:42<45:04,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 434/4687 of epoch 2 complete. Loss per last 992 samples:: 0.8989710653981855 \n","Training Accuracy per last 992 samples: 17.338709677419356\n"]},{"name":"stderr","output_type":"stream","text":[" 11%|█         | 496/4687 [05:22<44:15,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 496/4687 of epoch 2 complete. Loss per last 992 samples:: 0.8968176072643649 \n","Training Accuracy per last 992 samples: 18.245967741935484\n"]},{"name":"stderr","output_type":"stream","text":[" 12%|█▏        | 558/4687 [06:01<43:45,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 558/4687 of epoch 2 complete. Loss per last 992 samples:: 0.8973172095514113 \n","Training Accuracy per last 992 samples: 16.83467741935484\n"]},{"name":"stderr","output_type":"stream","text":[" 13%|█▎        | 620/4687 [06:40<43:00,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 620/4687 of epoch 2 complete. Loss per last 992 samples:: 0.8933912707913306 \n","Training Accuracy per last 992 samples: 18.346774193548388\n"]},{"name":"stderr","output_type":"stream","text":[" 13%|█▎        | 624/4687 [06:43<43:11,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, batch 625 complete! Training Loss : 0.896722412109375\n","Epoch 2, batch 625 complete! Training Accuracy : 0.1752\n"]},{"name":"stderr","output_type":"stream","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","100%|██████████| 521/521 [01:57<00:00,  4.44it/s]\n"," 13%|█▎        | 625/4687 [08:41<40:28:58, 35.88s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, batch 625 complete! Validation Loss : 1.7922377339251438\n","Epoch 2, batch 625 complete! Validation Accuracy : 0.1767458603311735\n"]},{"name":"stderr","output_type":"stream","text":[" 14%|█▎        | 641/4687 [08:51<50:35,  1.33it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 14%|█▍        | 668/4687 [09:08<42:36,  1.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 15%|█▍        | 682/4687 [09:17<42:27,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 682/4687 of epoch 2 complete. Loss per last 992 samples:: 0.8970317225302419 \n","Training Accuracy per last 992 samples: 16.83467741935484\n"]},{"name":"stderr","output_type":"stream","text":[" 16%|█▌        | 734/4687 [09:50<41:40,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 16%|█▌        | 744/4687 [09:56<41:36,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 744/4687 of epoch 2 complete. Loss per last 992 samples:: 0.9001578054120464 \n","Training Accuracy per last 992 samples: 16.733870967741936\n"]},{"name":"stderr","output_type":"stream","text":[" 17%|█▋        | 778/4687 [10:18<41:10,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 17%|█▋        | 806/4687 [10:36<40:58,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 806/4687 of epoch 2 complete. Loss per last 992 samples:: 0.8963888845136089 \n","Training Accuracy per last 992 samples: 18.14516129032258\n"]},{"name":"stderr","output_type":"stream","text":[" 19%|█▊        | 868/4687 [11:15<40:19,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 868/4687 of epoch 2 complete. Loss per last 992 samples:: 0.8970912810294859 \n","Training Accuracy per last 992 samples: 17.54032258064516\n"]},{"name":"stderr","output_type":"stream","text":[" 20%|█▉        | 930/4687 [11:54<39:38,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 930/4687 of epoch 2 complete. Loss per last 992 samples:: 0.8974368187689012 \n","Training Accuracy per last 992 samples: 16.733870967741936\n"]},{"name":"stderr","output_type":"stream","text":[" 21%|██        | 992/4687 [12:33<38:57,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 992/4687 of epoch 2 complete. Loss per last 992 samples:: 0.8976647161668346 \n","Training Accuracy per last 992 samples: 17.137096774193548\n"]},{"name":"stderr","output_type":"stream","text":[" 22%|██▏       | 1054/4687 [13:13<38:41,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1054/4687 of epoch 2 complete. Loss per last 992 samples:: 0.8960654966292843 \n","Training Accuracy per last 992 samples: 17.741935483870968\n"]},{"name":"stderr","output_type":"stream","text":[" 23%|██▎       | 1079/4687 [13:29<41:31,  1.45it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 24%|██▍       | 1116/4687 [13:54<42:01,  1.42it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1116/4687 of epoch 2 complete. Loss per last 992 samples:: 0.8979319910849294 \n","Training Accuracy per last 992 samples: 16.93548387096774\n"]},{"name":"stderr","output_type":"stream","text":[" 25%|██▌       | 1178/4687 [14:37<41:28,  1.41it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1178/4687 of epoch 2 complete. Loss per last 992 samples:: 0.895263671875 \n","Training Accuracy per last 992 samples: 18.548387096774192\n"]},{"name":"stderr","output_type":"stream","text":[" 26%|██▌       | 1213/4687 [15:01<39:04,  1.48it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 26%|██▋       | 1237/4687 [15:18<40:01,  1.44it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 26%|██▋       | 1240/4687 [15:20<39:18,  1.46it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1240/4687 of epoch 2 complete. Loss per last 992 samples:: 0.8968358193674395 \n","Training Accuracy per last 992 samples: 18.649193548387096\n"]},{"name":"stderr","output_type":"stream","text":[" 27%|██▋       | 1249/4687 [15:26<36:29,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, batch 1250 complete! Training Loss : 0.8969371337890625\n","Epoch 2, batch 1250 complete! Training Accuracy : 0.17515\n"]},{"name":"stderr","output_type":"stream","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","100%|██████████| 521/521 [01:57<00:00,  4.42it/s]\n"," 27%|██▋       | 1250/4687 [17:24<34:21:16, 35.98s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, batch 1250 complete! Validation Loss : 1.7921964971209212\n","Epoch 2, batch 1250 complete! Validation Accuracy : 0.17602591792656588\n"]},{"name":"stderr","output_type":"stream","text":[" 28%|██▊       | 1302/4687 [17:58<35:47,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1302/4687 of epoch 2 complete. Loss per last 992 samples:: 0.8963426159274194 \n","Training Accuracy per last 992 samples: 17.741935483870968\n"]},{"name":"stderr","output_type":"stream","text":[" 29%|██▉       | 1364/4687 [18:37<35:13,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1364/4687 of epoch 2 complete. Loss per last 992 samples:: 0.8959375196887601 \n","Training Accuracy per last 992 samples: 17.741935483870968\n"]},{"name":"stderr","output_type":"stream","text":[" 29%|██▉       | 1372/4687 [18:43<35:41,  1.55it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 29%|██▉       | 1380/4687 [18:48<35:13,  1.56it/s]"]}],"source":["train_bert(net, criterion, opti, LEARNING_RATE, lr_scheduler, train_loader, val_loader, EPOCHS, iters_to_accumulate)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  4%|▍         | 62/1563 [00:43<17:34,  1.42it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 62/1563 of epoch 1 complete. Loss per last 992 samples:: 0.9014552947013609 \n","Training Accuracy per last 992 samples: 16.733870967741936\n"]},{"name":"stderr","output_type":"stream","text":["  8%|▊         | 124/1563 [01:27<16:56,  1.42it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 124/1563 of epoch 1 complete. Loss per last 992 samples:: 0.8983720348727319 \n","Training Accuracy per last 992 samples: 15.524193548387096\n"]},{"name":"stderr","output_type":"stream","text":[" 12%|█▏        | 186/1563 [02:10<16:28,  1.39it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 186/1563 of epoch 1 complete. Loss per last 992 samples:: 0.8972532210811492 \n","Training Accuracy per last 992 samples: 17.842741935483872\n"]},{"name":"stderr","output_type":"stream","text":[" 16%|█▌        | 248/1563 [02:54<15:28,  1.42it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 248/1563 of epoch 1 complete. Loss per last 992 samples:: 0.8970366447202621 \n","Training Accuracy per last 992 samples: 16.431451612903224\n"]},{"name":"stderr","output_type":"stream","text":[" 20%|█▉        | 310/1563 [03:38<14:32,  1.44it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 310/1563 of epoch 1 complete. Loss per last 992 samples:: 0.8970617478893649 \n","Training Accuracy per last 992 samples: 18.346774193548388\n"]},{"name":"stderr","output_type":"stream","text":[" 24%|██▍       | 372/1563 [04:22<14:02,  1.41it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 372/1563 of epoch 1 complete. Loss per last 992 samples:: 0.8988563783707157 \n","Training Accuracy per last 992 samples: 17.237903225806452\n"]},{"name":"stderr","output_type":"stream","text":[" 28%|██▊       | 434/1563 [05:03<12:11,  1.54it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 434/1563 of epoch 1 complete. Loss per last 992 samples:: 0.897124259702621 \n","Training Accuracy per last 992 samples: 17.842741935483872\n"]},{"name":"stderr","output_type":"stream","text":[" 32%|███▏      | 496/1563 [05:43<11:30,  1.54it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 496/1563 of epoch 1 complete. Loss per last 992 samples:: 0.897978751890121 \n","Training Accuracy per last 992 samples: 16.532258064516128\n"]},{"name":"stderr","output_type":"stream","text":[" 36%|███▌      | 558/1563 [06:23<11:09,  1.50it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 558/1563 of epoch 1 complete. Loss per last 992 samples:: 0.8961939657888105 \n","Training Accuracy per last 992 samples: 17.036290322580644\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|███▉      | 620/1563 [07:04<10:02,  1.56it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 620/1563 of epoch 1 complete. Loss per last 992 samples:: 0.9000696982106855 \n","Training Accuracy per last 992 samples: 14.112903225806452\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|███▉      | 624/1563 [07:06<10:20,  1.51it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 625 complete! Training Loss : 0.89817236328125\n","Epoch 1, batch 625 complete! Training Accuracy : 0.1678\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 174/174 [00:38<00:00,  4.47it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 625 complete! Validation Loss : 1.7903589709051724\n","Epoch 1, batch 625 complete! Validation Accuracy : 0.17782577393808496\n","Validation loss changed from inf to 1.7903589709051724\n","Best validation accuracy improved from 0 to 0.17782577393808496\n","\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|███▉      | 625/1563 [07:47<3:16:47, 12.59s/it]"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Sun Nov 14 02:51:56 2021_lr_2e-05_val_acc_0.1778_ep_1.pt\n"]},{"name":"stderr","output_type":"stream","text":[" 44%|████▎     | 682/1563 [08:23<09:29,  1.55it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 682/1563 of epoch 1 complete. Loss per last 992 samples:: 0.8977803876323085 \n","Training Accuracy per last 992 samples: 18.044354838709676\n"]},{"name":"stderr","output_type":"stream","text":[" 48%|████▊     | 744/1563 [09:03<08:39,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 744/1563 of epoch 1 complete. Loss per last 992 samples:: 0.8975864533455141 \n","Training Accuracy per last 992 samples: 17.54032258064516\n"]},{"name":"stderr","output_type":"stream","text":[" 52%|█████▏    | 806/1563 [09:43<08:02,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 806/1563 of epoch 1 complete. Loss per last 992 samples:: 0.8975086827431956 \n","Training Accuracy per last 992 samples: 17.641129032258064\n"]},{"name":"stderr","output_type":"stream","text":[" 56%|█████▌    | 868/1563 [10:23<07:23,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 868/1563 of epoch 1 complete. Loss per last 992 samples:: 0.8954950148059476 \n","Training Accuracy per last 992 samples: 18.447580645161292\n"]},{"name":"stderr","output_type":"stream","text":[" 60%|█████▉    | 930/1563 [11:02<06:41,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 930/1563 of epoch 1 complete. Loss per last 992 samples:: 0.8969298331968246 \n","Training Accuracy per last 992 samples: 15.725806451612904\n"]},{"name":"stderr","output_type":"stream","text":[" 63%|██████▎   | 992/1563 [11:42<06:02,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 992/1563 of epoch 1 complete. Loss per last 992 samples:: 0.896819083921371 \n","Training Accuracy per last 992 samples: 18.346774193548388\n"]},{"name":"stderr","output_type":"stream","text":[" 67%|██████▋   | 1054/1563 [12:21<05:25,  1.56it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1054/1563 of epoch 1 complete. Loss per last 992 samples:: 0.896971671811996 \n","Training Accuracy per last 992 samples: 20.262096774193548\n"]},{"name":"stderr","output_type":"stream","text":[" 71%|███████▏  | 1116/1563 [13:01<04:44,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1116/1563 of epoch 1 complete. Loss per last 992 samples:: 0.8935975105531754 \n","Training Accuracy per last 992 samples: 19.254032258064516\n"]},{"name":"stderr","output_type":"stream","text":[" 75%|███████▌  | 1178/1563 [13:41<04:06,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1178/1563 of epoch 1 complete. Loss per last 992 samples:: 0.8950342978200605 \n","Training Accuracy per last 992 samples: 19.556451612903224\n"]},{"name":"stderr","output_type":"stream","text":[" 79%|███████▉  | 1240/1563 [14:22<03:43,  1.45it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1240/1563 of epoch 1 complete. Loss per last 992 samples:: 0.8959581928868448 \n","Training Accuracy per last 992 samples: 19.153225806451612\n"]},{"name":"stderr","output_type":"stream","text":[" 80%|███████▉  | 1249/1563 [14:28<03:21,  1.56it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 1250 complete! Training Loss : 0.897237158203125\n","Epoch 1, batch 1250 complete! Training Accuracy : 0.17575\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 174/174 [00:41<00:00,  4.21it/s]\n"," 80%|███████▉  | 1250/1563 [15:10<1:08:05, 13.05s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 1250 complete! Validation Loss : 1.7851057381465518\n","Epoch 1, batch 1250 complete! Validation Accuracy : 0.1771058315334773\n"]},{"name":"stderr","output_type":"stream","text":[" 83%|████████▎ | 1302/1563 [15:44<02:47,  1.56it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1302/1563 of epoch 1 complete. Loss per last 992 samples:: 0.896666988249748 \n","Training Accuracy per last 992 samples: 16.129032258064516\n"]},{"name":"stderr","output_type":"stream","text":[" 87%|████████▋ | 1364/1563 [16:25<02:11,  1.51it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1364/1563 of epoch 1 complete. Loss per last 992 samples:: 0.8940336165889617 \n","Training Accuracy per last 992 samples: 18.14516129032258\n"]},{"name":"stderr","output_type":"stream","text":[" 91%|█████████ | 1426/1563 [17:05<01:26,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1426/1563 of epoch 1 complete. Loss per last 992 samples:: 0.8930098010647681 \n","Training Accuracy per last 992 samples: 19.455645161290324\n"]},{"name":"stderr","output_type":"stream","text":[" 95%|█████████▌| 1488/1563 [17:45<00:50,  1.49it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1488/1563 of epoch 1 complete. Loss per last 992 samples:: 0.8877743136498236 \n","Training Accuracy per last 992 samples: 19.254032258064516\n"]},{"name":"stderr","output_type":"stream","text":[" 99%|█████████▉| 1550/1563 [18:26<00:08,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1550/1563 of epoch 1 complete. Loss per last 992 samples:: 0.8854441488942792 \n","Training Accuracy per last 992 samples: 23.487903225806452\n"]},{"name":"stderr","output_type":"stream","text":["100%|█████████▉| 1562/1563 [18:34<00:00,  1.54it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 1563 complete! Training Loss : 0.896057577981296\n","Epoch 1, batch 1563 complete! Training Accuracy : 0.17981034689713118\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 174/174 [00:40<00:00,  4.25it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 1563 complete! Validation Loss : 1.7667081986350575\n","Epoch 1, batch 1563 complete! Validation Accuracy : 0.22786177105831534\n","Validation loss changed from 1.7903589709051724 to 1.7667081986350575\n","Best validation accuracy improved from 0.17782577393808496 to 0.22786177105831534\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1563/1563 [19:16<00:00,  1.35it/s]\n"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Sun Nov 14 03:03:25 2021_lr_2e-05_val_acc_0.2279_ep_1.pt\n"]},{"name":"stderr","output_type":"stream","text":["  4%|▍         | 62/1563 [00:40<16:46,  1.49it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 62/1563 of epoch 2 complete. Loss per last 992 samples:: 0.881682611280872 \n","Training Accuracy per last 992 samples: 21.975806451612904\n"]},{"name":"stderr","output_type":"stream","text":["  8%|▊         | 124/1563 [01:20<15:18,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 124/1563 of epoch 2 complete. Loss per last 992 samples:: 0.8753209267893145 \n","Training Accuracy per last 992 samples: 23.991935483870968\n"]},{"name":"stderr","output_type":"stream","text":[" 12%|█▏        | 186/1563 [02:01<15:25,  1.49it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 186/1563 of epoch 2 complete. Loss per last 992 samples:: 0.8714835259222216 \n","Training Accuracy per last 992 samples: 24.495967741935484\n"]},{"name":"stderr","output_type":"stream","text":[" 16%|█▌        | 248/1563 [02:41<13:53,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 248/1563 of epoch 2 complete. Loss per last 992 samples:: 0.879303716844128 \n","Training Accuracy per last 992 samples: 22.076612903225808\n"]},{"name":"stderr","output_type":"stream","text":[" 20%|█▉        | 310/1563 [03:21<13:35,  1.54it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 310/1563 of epoch 2 complete. Loss per last 992 samples:: 0.8723070083125946 \n","Training Accuracy per last 992 samples: 24.193548387096776\n"]},{"name":"stderr","output_type":"stream","text":[" 24%|██▍       | 372/1563 [04:01<12:37,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 372/1563 of epoch 2 complete. Loss per last 992 samples:: 0.8737114937074723 \n","Training Accuracy per last 992 samples: 24.092741935483872\n"]},{"name":"stderr","output_type":"stream","text":[" 28%|██▊       | 434/1563 [04:41<12:27,  1.51it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 434/1563 of epoch 2 complete. Loss per last 992 samples:: 0.8793641367266255 \n","Training Accuracy per last 992 samples: 21.975806451612904\n"]},{"name":"stderr","output_type":"stream","text":[" 32%|███▏      | 496/1563 [05:21<11:24,  1.56it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 496/1563 of epoch 2 complete. Loss per last 992 samples:: 0.8628126575100806 \n","Training Accuracy per last 992 samples: 25.302419354838708\n"]},{"name":"stderr","output_type":"stream","text":[" 36%|███▌      | 558/1563 [06:01<10:52,  1.54it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 558/1563 of epoch 2 complete. Loss per last 992 samples:: 0.8547627848963584 \n","Training Accuracy per last 992 samples: 28.629032258064516\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|███▉      | 620/1563 [06:42<10:40,  1.47it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 620/1563 of epoch 2 complete. Loss per last 992 samples:: 0.8657262863651398 \n","Training Accuracy per last 992 samples: 25.907258064516128\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|███▉      | 624/1563 [06:44<10:36,  1.48it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, batch 625 complete! Training Loss : 0.8718375183105469\n","Epoch 2, batch 625 complete! Training Accuracy : 0.2426\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 174/174 [00:40<00:00,  4.26it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, batch 625 complete! Validation Loss : 1.7059368265086208\n","Epoch 2, batch 625 complete! Validation Accuracy : 0.2501799856011519\n","Validation loss changed from 1.7667081986350575 to 1.7059368265086208\n","Best validation accuracy improved from 0.22786177105831534 to 0.2501799856011519\n","\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|███▉      | 625/1563 [07:27<3:26:14, 13.19s/it]"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Sun Nov 14 03:10:53 2021_lr_2e-05_val_acc_0.2502_ep_2.pt\n"]},{"name":"stderr","output_type":"stream","text":[" 44%|████▎     | 682/1563 [08:04<09:44,  1.51it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 682/1563 of epoch 2 complete. Loss per last 992 samples:: 0.8508006680396295 \n","Training Accuracy per last 992 samples: 26.108870967741936\n"]},{"name":"stderr","output_type":"stream","text":[" 48%|████▊     | 744/1563 [08:45<08:44,  1.56it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 744/1563 of epoch 2 complete. Loss per last 992 samples:: 0.8581630952896611 \n","Training Accuracy per last 992 samples: 27.419354838709676\n"]},{"name":"stderr","output_type":"stream","text":[" 52%|█████▏    | 806/1563 [09:27<09:11,  1.37it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 806/1563 of epoch 2 complete. Loss per last 992 samples:: 0.846983140514743 \n","Training Accuracy per last 992 samples: 27.116935483870968\n"]},{"name":"stderr","output_type":"stream","text":[" 56%|█████▌    | 868/1563 [10:07<07:49,  1.48it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 868/1563 of epoch 2 complete. Loss per last 992 samples:: 0.866848191907329 \n","Training Accuracy per last 992 samples: 26.713709677419356\n"]},{"name":"stderr","output_type":"stream","text":[" 60%|█████▉    | 930/1563 [10:47<06:52,  1.53it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 930/1563 of epoch 2 complete. Loss per last 992 samples:: 0.8521508862895351 \n","Training Accuracy per last 992 samples: 25.504032258064516\n"]},{"name":"stderr","output_type":"stream","text":[" 63%|██████▎   | 992/1563 [11:28<06:10,  1.54it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 992/1563 of epoch 2 complete. Loss per last 992 samples:: 0.8448432799308531 \n","Training Accuracy per last 992 samples: 28.931451612903224\n"]},{"name":"stderr","output_type":"stream","text":[" 67%|██████▋   | 1054/1563 [12:08<05:23,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1054/1563 of epoch 2 complete. Loss per last 992 samples:: 0.8592927686629757 \n","Training Accuracy per last 992 samples: 24.092741935483872\n"]},{"name":"stderr","output_type":"stream","text":[" 71%|███████▏  | 1116/1563 [12:49<05:04,  1.47it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1116/1563 of epoch 2 complete. Loss per last 992 samples:: 0.8497996022624354 \n","Training Accuracy per last 992 samples: 26.91532258064516\n"]},{"name":"stderr","output_type":"stream","text":[" 75%|███████▌  | 1178/1563 [13:29<04:04,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1178/1563 of epoch 2 complete. Loss per last 992 samples:: 0.8451112316500756 \n","Training Accuracy per last 992 samples: 28.326612903225808\n"]},{"name":"stderr","output_type":"stream","text":[" 79%|███████▉  | 1240/1563 [14:08<03:22,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1240/1563 of epoch 2 complete. Loss per last 992 samples:: 0.8572365545457409 \n","Training Accuracy per last 992 samples: 24.092741935483872\n"]},{"name":"stderr","output_type":"stream","text":[" 80%|███████▉  | 1249/1563 [14:13<03:14,  1.62it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, batch 1250 complete! Training Loss : 0.8621230041503907\n","Epoch 2, batch 1250 complete! Training Accuracy : 0.25445\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 174/174 [00:38<00:00,  4.50it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, batch 1250 complete! Validation Loss : 1.6855637122844827\n","Epoch 2, batch 1250 complete! Validation Accuracy : 0.28833693304535635\n","Validation loss changed from 1.7059368265086208 to 1.6855637122844827\n","Best validation accuracy improved from 0.2501799856011519 to 0.28833693304535635\n","\n"]},{"name":"stderr","output_type":"stream","text":[" 80%|███████▉  | 1250/1563 [14:54<1:05:10, 12.49s/it]"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Sun Nov 14 03:18:20 2021_lr_2e-05_val_acc_0.2883_ep_2.pt\n"]},{"name":"stderr","output_type":"stream","text":[" 83%|████████▎ | 1302/1563 [15:27<02:45,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1302/1563 of epoch 2 complete. Loss per last 992 samples:: 0.8430861503847183 \n","Training Accuracy per last 992 samples: 28.326612903225808\n"]},{"name":"stderr","output_type":"stream","text":[" 87%|████████▋ | 1364/1563 [16:06<02:09,  1.54it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1364/1563 of epoch 2 complete. Loss per last 992 samples:: 0.8460490319036669 \n","Training Accuracy per last 992 samples: 25.504032258064516\n"]},{"name":"stderr","output_type":"stream","text":[" 91%|█████████ | 1426/1563 [16:45<01:26,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1426/1563 of epoch 2 complete. Loss per last 992 samples:: 0.8646796441847279 \n","Training Accuracy per last 992 samples: 24.495967741935484\n"]},{"name":"stderr","output_type":"stream","text":[" 95%|█████████▌| 1488/1563 [17:25<00:47,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1488/1563 of epoch 2 complete. Loss per last 992 samples:: 0.8666049895748016 \n","Training Accuracy per last 992 samples: 24.495967741935484\n"]},{"name":"stderr","output_type":"stream","text":[" 99%|█████████▉| 1550/1563 [18:04<00:08,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1550/1563 of epoch 2 complete. Loss per last 992 samples:: 0.845732165921119 \n","Training Accuracy per last 992 samples: 28.528225806451612\n"]},{"name":"stderr","output_type":"stream","text":["100%|█████████▉| 1562/1563 [18:11<00:00,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, batch 1563 complete! Training Loss : 0.8602302939328946\n","Epoch 2, batch 1563 complete! Training Accuracy : 0.2562317448885688\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 174/174 [00:38<00:00,  4.50it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, batch 1563 complete! Validation Loss : 1.6740526221264367\n","Epoch 2, batch 1563 complete! Validation Accuracy : 0.29049676025917925\n","Validation loss changed from 1.6855637122844827 to 1.6740526221264367\n","Best validation accuracy improved from 0.28833693304535635 to 0.29049676025917925\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1563/1563 [18:51<00:00,  1.38it/s]\n"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Sun Nov 14 03:22:17 2021_lr_2e-05_val_acc_0.2905_ep_2.pt\n"]},{"name":"stderr","output_type":"stream","text":["  4%|▍         | 62/1563 [00:39<15:52,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 62/1563 of epoch 3 complete. Loss per last 992 samples:: 0.8370809862690587 \n","Training Accuracy per last 992 samples: 29.33467741935484\n"]},{"name":"stderr","output_type":"stream","text":["  8%|▊         | 124/1563 [01:18<15:13,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 124/1563 of epoch 3 complete. Loss per last 992 samples:: 0.8343893789475963 \n","Training Accuracy per last 992 samples: 30.846774193548388\n"]},{"name":"stderr","output_type":"stream","text":[" 12%|█▏        | 186/1563 [01:57<14:32,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 186/1563 of epoch 3 complete. Loss per last 992 samples:: 0.8217866651473507 \n","Training Accuracy per last 992 samples: 30.443548387096776\n"]},{"name":"stderr","output_type":"stream","text":[" 16%|█▌        | 248/1563 [02:37<13:54,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 248/1563 of epoch 3 complete. Loss per last 992 samples:: 0.8490121287684287 \n","Training Accuracy per last 992 samples: 26.411290322580644\n"]},{"name":"stderr","output_type":"stream","text":[" 20%|█▉        | 310/1563 [03:16<13:15,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 310/1563 of epoch 3 complete. Loss per last 992 samples:: 0.8533172915058751 \n","Training Accuracy per last 992 samples: 27.419354838709676\n"]},{"name":"stderr","output_type":"stream","text":[" 24%|██▍       | 372/1563 [03:55<12:37,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 372/1563 of epoch 3 complete. Loss per last 992 samples:: 0.8271625580326203 \n","Training Accuracy per last 992 samples: 30.544354838709676\n"]},{"name":"stderr","output_type":"stream","text":[" 28%|██▊       | 434/1563 [04:35<11:58,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 434/1563 of epoch 3 complete. Loss per last 992 samples:: 0.8307319764168032 \n","Training Accuracy per last 992 samples: 28.528225806451612\n"]},{"name":"stderr","output_type":"stream","text":[" 32%|███▏      | 496/1563 [05:14<11:16,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 496/1563 of epoch 3 complete. Loss per last 992 samples:: 0.831485471417827 \n","Training Accuracy per last 992 samples: 29.43548387096774\n"]},{"name":"stderr","output_type":"stream","text":[" 36%|███▌      | 558/1563 [05:53<10:37,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 558/1563 of epoch 3 complete. Loss per last 992 samples:: 0.8240047577888735 \n","Training Accuracy per last 992 samples: 33.66935483870968\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|███▉      | 620/1563 [06:32<09:58,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 620/1563 of epoch 3 complete. Loss per last 992 samples:: 0.8130114155430948 \n","Training Accuracy per last 992 samples: 32.056451612903224\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|███▉      | 624/1563 [06:35<09:57,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 3, batch 625 complete! Training Loss : 0.8320869659423829\n","Epoch 3, batch 625 complete! Training Accuracy : 0.2984\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 174/174 [00:38<00:00,  4.50it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3, batch 625 complete! Validation Loss : 1.6741143588362069\n","Epoch 3, batch 625 complete! Validation Accuracy : 0.30093592512598993\n","Validation loss changed from 1.6740526221264367 to 1.6741143588362069\n","Best validation accuracy improved from 0.29049676025917925 to 0.30093592512598993\n","\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|███▉      | 625/1563 [07:15<3:15:12, 12.49s/it]"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Sun Nov 14 03:29:33 2021_lr_2e-05_val_acc_0.3009_ep_3.pt\n"]},{"name":"stderr","output_type":"stream","text":[" 44%|████▎     | 682/1563 [07:51<09:18,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 682/1563 of epoch 3 complete. Loss per last 992 samples:: 0.8221357560926869 \n","Training Accuracy per last 992 samples: 31.25\n"]},{"name":"stderr","output_type":"stream","text":[" 48%|████▊     | 744/1563 [08:30<08:40,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 744/1563 of epoch 3 complete. Loss per last 992 samples:: 0.830233097076416 \n","Training Accuracy per last 992 samples: 29.637096774193548\n"]},{"name":"stderr","output_type":"stream","text":[" 52%|█████▏    | 806/1563 [09:10<08:01,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 806/1563 of epoch 3 complete. Loss per last 992 samples:: 0.8296513095978768 \n","Training Accuracy per last 992 samples: 30.947580645161292\n"]},{"name":"stderr","output_type":"stream","text":[" 56%|█████▌    | 868/1563 [09:49<07:20,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 868/1563 of epoch 3 complete. Loss per last 992 samples:: 0.8374060046288275 \n","Training Accuracy per last 992 samples: 29.93951612903226\n"]},{"name":"stderr","output_type":"stream","text":[" 60%|█████▉    | 930/1563 [10:28<06:41,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 930/1563 of epoch 3 complete. Loss per last 992 samples:: 0.8101476546256773 \n","Training Accuracy per last 992 samples: 33.770161290322584\n"]},{"name":"stderr","output_type":"stream","text":[" 63%|██████▎   | 992/1563 [11:07<06:02,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 992/1563 of epoch 3 complete. Loss per last 992 samples:: 0.8192882999297111 \n","Training Accuracy per last 992 samples: 33.568548387096776\n"]},{"name":"stderr","output_type":"stream","text":[" 67%|██████▋   | 1054/1563 [11:47<05:23,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1054/1563 of epoch 3 complete. Loss per last 992 samples:: 0.8160183506627237 \n","Training Accuracy per last 992 samples: 34.274193548387096\n"]},{"name":"stderr","output_type":"stream","text":[" 71%|███████▏  | 1116/1563 [12:26<04:43,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1116/1563 of epoch 3 complete. Loss per last 992 samples:: 0.8300302874657416 \n","Training Accuracy per last 992 samples: 30.745967741935484\n"]},{"name":"stderr","output_type":"stream","text":[" 75%|███████▌  | 1178/1563 [13:05<04:03,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1178/1563 of epoch 3 complete. Loss per last 992 samples:: 0.8311678978704637 \n","Training Accuracy per last 992 samples: 30.04032258064516\n"]},{"name":"stderr","output_type":"stream","text":[" 79%|███████▉  | 1240/1563 [13:44<03:25,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1240/1563 of epoch 3 complete. Loss per last 992 samples:: 0.8247066620857485 \n","Training Accuracy per last 992 samples: 31.754032258064516\n"]},{"name":"stderr","output_type":"stream","text":[" 80%|███████▉  | 1249/1563 [13:50<03:18,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 3, batch 1250 complete! Training Loss : 0.8285538017272949\n","Epoch 3, batch 1250 complete! Training Accuracy : 0.30725\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 174/174 [00:38<00:00,  4.50it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3, batch 1250 complete! Validation Loss : 1.6476068606321839\n","Epoch 3, batch 1250 complete! Validation Accuracy : 0.3045356371490281\n","Validation loss changed from 1.6741143588362069 to 1.6476068606321839\n","Best validation accuracy improved from 0.30093592512598993 to 0.3045356371490281\n","\n"]},{"name":"stderr","output_type":"stream","text":[" 80%|███████▉  | 1250/1563 [14:30<1:05:09, 12.49s/it]"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Sun Nov 14 03:36:48 2021_lr_2e-05_val_acc_0.3045_ep_3.pt\n"]},{"name":"stderr","output_type":"stream","text":[" 83%|████████▎ | 1302/1563 [15:03<02:45,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1302/1563 of epoch 3 complete. Loss per last 992 samples:: 0.8240856047599546 \n","Training Accuracy per last 992 samples: 30.342741935483872\n"]},{"name":"stderr","output_type":"stream","text":[" 87%|████████▋ | 1364/1563 [15:42<02:06,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1364/1563 of epoch 3 complete. Loss per last 992 samples:: 0.8066854938384025 \n","Training Accuracy per last 992 samples: 33.66935483870968\n"]},{"name":"stderr","output_type":"stream","text":[" 91%|█████████ | 1426/1563 [16:22<01:26,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1426/1563 of epoch 3 complete. Loss per last 992 samples:: 0.8019378262181436 \n","Training Accuracy per last 992 samples: 34.475806451612904\n"]},{"name":"stderr","output_type":"stream","text":[" 95%|█████████▌| 1488/1563 [17:01<00:47,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1488/1563 of epoch 3 complete. Loss per last 992 samples:: 0.8319448655651461 \n","Training Accuracy per last 992 samples: 32.76209677419355\n"]},{"name":"stderr","output_type":"stream","text":[" 99%|█████████▉| 1550/1563 [17:40<00:08,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1550/1563 of epoch 3 complete. Loss per last 992 samples:: 0.8035376456476027 \n","Training Accuracy per last 992 samples: 33.971774193548384\n"]},{"name":"stderr","output_type":"stream","text":["100%|█████████▉| 1562/1563 [17:47<00:00,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 3, batch 1563 complete! Training Loss : 0.8253745099175686\n","Epoch 3, batch 1563 complete! Training Accuracy : 0.3122074180770616\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 174/174 [00:38<00:00,  4.50it/s]\n","100%|██████████| 1563/1563 [18:26<00:00,  1.41it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3, batch 1563 complete! Validation Loss : 1.6610710757902298\n","Epoch 3, batch 1563 complete! Validation Accuracy : 0.3023758099352052\n"]},{"name":"stderr","output_type":"stream","text":["  4%|▍         | 62/1563 [00:39<15:50,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 62/1563 of epoch 4 complete. Loss per last 992 samples:: 0.79054444836032 \n","Training Accuracy per last 992 samples: 34.57661290322581\n"]},{"name":"stderr","output_type":"stream","text":["  8%|▊         | 124/1563 [01:18<15:16,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 124/1563 of epoch 4 complete. Loss per last 992 samples:: 0.7894439081991872 \n","Training Accuracy per last 992 samples: 34.67741935483871\n"]},{"name":"stderr","output_type":"stream","text":[" 12%|█▏        | 186/1563 [01:58<15:07,  1.52it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 186/1563 of epoch 4 complete. Loss per last 992 samples:: 0.7951636622028966 \n","Training Accuracy per last 992 samples: 33.66935483870968\n"]},{"name":"stderr","output_type":"stream","text":[" 16%|█▌        | 248/1563 [02:38<13:55,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 248/1563 of epoch 4 complete. Loss per last 992 samples:: 0.7788480327975366 \n","Training Accuracy per last 992 samples: 37.70161290322581\n"]},{"name":"stderr","output_type":"stream","text":[" 20%|█▉        | 310/1563 [03:18<13:15,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 310/1563 of epoch 4 complete. Loss per last 992 samples:: 0.7889893900963568 \n","Training Accuracy per last 992 samples: 35.483870967741936\n"]},{"name":"stderr","output_type":"stream","text":[" 24%|██▍       | 372/1563 [03:57<12:35,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 372/1563 of epoch 4 complete. Loss per last 992 samples:: 0.7899654142318233 \n","Training Accuracy per last 992 samples: 35.181451612903224\n"]},{"name":"stderr","output_type":"stream","text":[" 28%|██▊       | 434/1563 [04:36<11:56,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 434/1563 of epoch 4 complete. Loss per last 992 samples:: 0.7795911142902989 \n","Training Accuracy per last 992 samples: 35.98790322580645\n"]},{"name":"stderr","output_type":"stream","text":[" 32%|███▏      | 496/1563 [05:15<11:16,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 496/1563 of epoch 4 complete. Loss per last 992 samples:: 0.7760935291167228 \n","Training Accuracy per last 992 samples: 36.895161290322584\n"]},{"name":"stderr","output_type":"stream","text":[" 36%|███▌      | 558/1563 [05:55<10:37,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 558/1563 of epoch 4 complete. Loss per last 992 samples:: 0.7917100229570943 \n","Training Accuracy per last 992 samples: 34.475806451612904\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|███▉      | 620/1563 [06:34<09:58,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 620/1563 of epoch 4 complete. Loss per last 992 samples:: 0.8037995676840505 \n","Training Accuracy per last 992 samples: 34.17338709677419\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|███▉      | 624/1563 [06:36<09:57,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 4, batch 625 complete! Training Loss : 0.788346353149414\n","Epoch 4, batch 625 complete! Training Accuracy : 0.3533\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 174/174 [00:38<00:00,  4.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4, batch 625 complete! Validation Loss : 1.649694683908046\n","Epoch 4, batch 625 complete! Validation Accuracy : 0.3102951763858891\n","Validation loss changed from 1.6476068606321839 to 1.649694683908046\n","Best validation accuracy improved from 0.3045356371490281 to 0.3102951763858891\n","\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|███▉      | 625/1563 [07:17<3:15:32, 12.51s/it]"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Sun Nov 14 03:48:01 2021_lr_2e-05_val_acc_0.3103_ep_4.pt\n"]},{"name":"stderr","output_type":"stream","text":[" 44%|████▎     | 682/1563 [07:53<09:17,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 682/1563 of epoch 4 complete. Loss per last 992 samples:: 0.7833750940138294 \n","Training Accuracy per last 992 samples: 36.59274193548387\n"]},{"name":"stderr","output_type":"stream","text":[" 48%|████▊     | 744/1563 [08:32<08:39,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 744/1563 of epoch 4 complete. Loss per last 992 samples:: 0.7675462538196195 \n","Training Accuracy per last 992 samples: 41.028225806451616\n"]},{"name":"stderr","output_type":"stream","text":[" 52%|█████▏    | 806/1563 [09:11<07:59,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 806/1563 of epoch 4 complete. Loss per last 992 samples:: 0.7874234107232863 \n","Training Accuracy per last 992 samples: 36.08870967741935\n"]},{"name":"stderr","output_type":"stream","text":[" 56%|█████▌    | 868/1563 [09:51<07:21,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 868/1563 of epoch 4 complete. Loss per last 992 samples:: 0.810103016514932 \n","Training Accuracy per last 992 samples: 31.85483870967742\n"]},{"name":"stderr","output_type":"stream","text":[" 60%|█████▉    | 930/1563 [10:30<06:41,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 930/1563 of epoch 4 complete. Loss per last 992 samples:: 0.7860250473022461 \n","Training Accuracy per last 992 samples: 35.98790322580645\n"]},{"name":"stderr","output_type":"stream","text":[" 63%|██████▎   | 992/1563 [11:09<06:01,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 992/1563 of epoch 4 complete. Loss per last 992 samples:: 0.7768266893202259 \n","Training Accuracy per last 992 samples: 37.903225806451616\n"]},{"name":"stderr","output_type":"stream","text":[" 67%|██████▋   | 1054/1563 [11:48<05:23,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1054/1563 of epoch 4 complete. Loss per last 992 samples:: 0.7899719515154439 \n","Training Accuracy per last 992 samples: 34.979838709677416\n"]},{"name":"stderr","output_type":"stream","text":[" 71%|███████▏  | 1116/1563 [12:27<04:43,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1116/1563 of epoch 4 complete. Loss per last 992 samples:: 0.7738912797743275 \n","Training Accuracy per last 992 samples: 35.08064516129032\n"]},{"name":"stderr","output_type":"stream","text":[" 75%|███████▌  | 1178/1563 [13:07<04:03,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1178/1563 of epoch 4 complete. Loss per last 992 samples:: 0.7704403938785676 \n","Training Accuracy per last 992 samples: 39.21370967741935\n"]},{"name":"stderr","output_type":"stream","text":[" 79%|███████▉  | 1240/1563 [13:46<03:24,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1240/1563 of epoch 4 complete. Loss per last 992 samples:: 0.7877446605313209 \n","Training Accuracy per last 992 samples: 35.58467741935484\n"]},{"name":"stderr","output_type":"stream","text":[" 80%|███████▉  | 1249/1563 [13:52<03:18,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 4, batch 1250 complete! Training Loss : 0.7859700691223145\n","Epoch 4, batch 1250 complete! Training Accuracy : 0.3584\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 174/174 [00:38<00:00,  4.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4, batch 1250 complete! Validation Loss : 1.6560198904454022\n","Epoch 4, batch 1250 complete! Validation Accuracy : 0.31929445644348453\n","Validation loss changed from 1.649694683908046 to 1.6560198904454022\n","Best validation accuracy improved from 0.3102951763858891 to 0.31929445644348453\n","\n"]},{"name":"stderr","output_type":"stream","text":[" 80%|███████▉  | 1250/1563 [14:32<1:05:07, 12.48s/it]"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Sun Nov 14 03:55:16 2021_lr_2e-05_val_acc_0.3193_ep_4.pt\n"]},{"name":"stderr","output_type":"stream","text":[" 83%|████████▎ | 1302/1563 [15:05<02:45,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1302/1563 of epoch 4 complete. Loss per last 992 samples:: 0.7778798534024146 \n","Training Accuracy per last 992 samples: 37.70161290322581\n"]},{"name":"stderr","output_type":"stream","text":[" 87%|████████▋ | 1364/1563 [15:44<02:06,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1364/1563 of epoch 4 complete. Loss per last 992 samples:: 0.7852672607667984 \n","Training Accuracy per last 992 samples: 38.00403225806452\n"]},{"name":"stderr","output_type":"stream","text":[" 91%|█████████ | 1426/1563 [16:23<01:27,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1426/1563 of epoch 4 complete. Loss per last 992 samples:: 0.7848409837292086 \n","Training Accuracy per last 992 samples: 36.29032258064516\n"]},{"name":"stderr","output_type":"stream","text":[" 95%|█████████▌| 1488/1563 [17:02<00:47,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1488/1563 of epoch 4 complete. Loss per last 992 samples:: 0.7946076854582755 \n","Training Accuracy per last 992 samples: 33.36693548387097\n"]},{"name":"stderr","output_type":"stream","text":[" 99%|█████████▉| 1550/1563 [17:42<00:08,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1550/1563 of epoch 4 complete. Loss per last 992 samples:: 0.7968610794313492 \n","Training Accuracy per last 992 samples: 36.895161290322584\n"]},{"name":"stderr","output_type":"stream","text":["100%|█████████▉| 1562/1563 [17:49<00:00,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 4, batch 1563 complete! Training Loss : 0.7862696064944764\n","Epoch 4, batch 1563 complete! Training Accuracy : 0.3597007162005362\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 174/174 [00:38<00:00,  4.51it/s]\n","100%|██████████| 1563/1563 [18:28<00:00,  1.41it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4, batch 1563 complete! Validation Loss : 1.6322737068965518\n","Epoch 4, batch 1563 complete! Validation Accuracy : 0.31893448524118073\n"]},{"name":"stderr","output_type":"stream","text":["  4%|▍         | 62/1563 [00:39<15:46,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 62/1563 of epoch 5 complete. Loss per last 992 samples:: 0.7208990743083339 \n","Training Accuracy per last 992 samples: 43.24596774193548\n"]},{"name":"stderr","output_type":"stream","text":["  8%|▊         | 124/1563 [01:18<15:08,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 124/1563 of epoch 5 complete. Loss per last 992 samples:: 0.7351894378662109 \n","Training Accuracy per last 992 samples: 42.03629032258065\n"]},{"name":"stderr","output_type":"stream","text":[" 12%|█▏        | 186/1563 [01:57<14:29,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 186/1563 of epoch 5 complete. Loss per last 992 samples:: 0.7447216741500362 \n","Training Accuracy per last 992 samples: 40.020161290322584\n"]},{"name":"stderr","output_type":"stream","text":[" 16%|█▌        | 248/1563 [02:36<13:51,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 248/1563 of epoch 5 complete. Loss per last 992 samples:: 0.7475824202260664 \n","Training Accuracy per last 992 samples: 41.229838709677416\n"]},{"name":"stderr","output_type":"stream","text":[" 20%|█▉        | 310/1563 [03:15<13:11,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 310/1563 of epoch 5 complete. Loss per last 992 samples:: 0.7272429158610683 \n","Training Accuracy per last 992 samples: 41.33064516129032\n"]},{"name":"stderr","output_type":"stream","text":[" 24%|██▍       | 372/1563 [03:54<12:31,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 372/1563 of epoch 5 complete. Loss per last 992 samples:: 0.7574374445023075 \n","Training Accuracy per last 992 samples: 39.818548387096776\n"]},{"name":"stderr","output_type":"stream","text":[" 28%|██▊       | 434/1563 [04:33<11:54,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 434/1563 of epoch 5 complete. Loss per last 992 samples:: 0.7418159823263845 \n","Training Accuracy per last 992 samples: 40.625\n"]},{"name":"stderr","output_type":"stream","text":[" 32%|███▏      | 496/1563 [05:12<11:13,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 496/1563 of epoch 5 complete. Loss per last 992 samples:: 0.734811705927695 \n","Training Accuracy per last 992 samples: 42.03629032258065\n"]},{"name":"stderr","output_type":"stream","text":[" 36%|███▌      | 558/1563 [05:51<10:35,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 558/1563 of epoch 5 complete. Loss per last 992 samples:: 0.7551292450197281 \n","Training Accuracy per last 992 samples: 39.11290322580645\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|███▉      | 620/1563 [06:30<09:57,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 620/1563 of epoch 5 complete. Loss per last 992 samples:: 0.7313059529950542 \n","Training Accuracy per last 992 samples: 41.229838709677416\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|███▉      | 624/1563 [06:33<09:55,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 5, batch 625 complete! Training Loss : 0.7401319396972657\n","Epoch 5, batch 625 complete! Training Accuracy : 0.4099\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 174/174 [00:38<00:00,  4.50it/s]\n"," 40%|███▉      | 625/1563 [07:12<3:11:11, 12.23s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 5, batch 625 complete! Validation Loss : 1.6772517061781609\n","Epoch 5, batch 625 complete! Validation Accuracy : 0.3174946004319654\n"]},{"name":"stderr","output_type":"stream","text":[" 44%|████▎     | 682/1563 [07:48<09:15,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 682/1563 of epoch 5 complete. Loss per last 992 samples:: 0.7416533808554372 \n","Training Accuracy per last 992 samples: 40.32258064516129\n"]},{"name":"stderr","output_type":"stream","text":[" 48%|████▊     | 744/1563 [08:27<08:36,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 744/1563 of epoch 5 complete. Loss per last 992 samples:: 0.7225907694908881 \n","Training Accuracy per last 992 samples: 42.74193548387097\n"]},{"name":"stderr","output_type":"stream","text":[" 52%|█████▏    | 806/1563 [09:07<07:59,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 806/1563 of epoch 5 complete. Loss per last 992 samples:: 0.7480504282059208 \n","Training Accuracy per last 992 samples: 40.725806451612904\n"]},{"name":"stderr","output_type":"stream","text":[" 56%|█████▌    | 868/1563 [09:47<07:27,  1.55it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 868/1563 of epoch 5 complete. Loss per last 992 samples:: 0.7381166181256694 \n","Training Accuracy per last 992 samples: 41.733870967741936\n"]},{"name":"stderr","output_type":"stream","text":[" 60%|█████▉    | 930/1563 [10:26<06:41,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 930/1563 of epoch 5 complete. Loss per last 992 samples:: 0.766685962677002 \n","Training Accuracy per last 992 samples: 39.21370967741935\n"]},{"name":"stderr","output_type":"stream","text":[" 63%|██████▎   | 992/1563 [11:05<06:01,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 992/1563 of epoch 5 complete. Loss per last 992 samples:: 0.741922716940603 \n","Training Accuracy per last 992 samples: 41.935483870967744\n"]},{"name":"stderr","output_type":"stream","text":[" 67%|██████▋   | 1054/1563 [11:45<05:23,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1054/1563 of epoch 5 complete. Loss per last 992 samples:: 0.7333791640497023 \n","Training Accuracy per last 992 samples: 41.83467741935484\n"]},{"name":"stderr","output_type":"stream","text":[" 71%|███████▏  | 1116/1563 [12:24<04:43,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1116/1563 of epoch 5 complete. Loss per last 992 samples:: 0.7485776332116896 \n","Training Accuracy per last 992 samples: 39.71774193548387\n"]},{"name":"stderr","output_type":"stream","text":[" 75%|███████▌  | 1178/1563 [13:03<04:03,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1178/1563 of epoch 5 complete. Loss per last 992 samples:: 0.7161989827309886 \n","Training Accuracy per last 992 samples: 43.649193548387096\n"]},{"name":"stderr","output_type":"stream","text":[" 79%|███████▉  | 1240/1563 [13:43<03:24,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1240/1563 of epoch 5 complete. Loss per last 992 samples:: 0.7464119695848034 \n","Training Accuracy per last 992 samples: 41.028225806451616\n"]},{"name":"stderr","output_type":"stream","text":[" 80%|███████▉  | 1249/1563 [13:48<03:19,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 5, batch 1250 complete! Training Loss : 0.7395848896026611\n","Epoch 5, batch 1250 complete! Training Accuracy : 0.41225\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 174/174 [00:38<00:00,  4.50it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5, batch 1250 complete! Validation Loss : 1.6663018588362069\n","Epoch 5, batch 1250 complete! Validation Accuracy : 0.32685385169186465\n","Validation loss changed from 1.6560198904454022 to 1.6663018588362069\n","Best validation accuracy improved from 0.31929445644348453 to 0.32685385169186465\n","\n"]},{"name":"stderr","output_type":"stream","text":[" 80%|███████▉  | 1250/1563 [14:29<1:05:18, 12.52s/it]"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Sun Nov 14 04:13:42 2021_lr_2e-05_val_acc_0.3269_ep_5.pt\n"]},{"name":"stderr","output_type":"stream","text":[" 83%|████████▎ | 1302/1563 [15:02<02:45,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1302/1563 of epoch 5 complete. Loss per last 992 samples:: 0.7463427205239573 \n","Training Accuracy per last 992 samples: 42.439516129032256\n"]},{"name":"stderr","output_type":"stream","text":[" 87%|████████▋ | 1364/1563 [15:41<02:06,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1364/1563 of epoch 5 complete. Loss per last 992 samples:: 0.7498249469264862 \n","Training Accuracy per last 992 samples: 38.306451612903224\n"]},{"name":"stderr","output_type":"stream","text":[" 91%|█████████ | 1426/1563 [16:20<01:26,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1426/1563 of epoch 5 complete. Loss per last 992 samples:: 0.7286069162430302 \n","Training Accuracy per last 992 samples: 41.33064516129032\n"]},{"name":"stderr","output_type":"stream","text":[" 95%|█████████▌| 1488/1563 [17:00<00:50,  1.49it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1488/1563 of epoch 5 complete. Loss per last 992 samples:: 0.7373768129656392 \n","Training Accuracy per last 992 samples: 40.82661290322581\n"]},{"name":"stderr","output_type":"stream","text":[" 99%|█████████▉| 1550/1563 [17:39<00:08,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1550/1563 of epoch 5 complete. Loss per last 992 samples:: 0.7297469569790748 \n","Training Accuracy per last 992 samples: 42.33870967741935\n"]},{"name":"stderr","output_type":"stream","text":["100%|█████████▉| 1562/1563 [17:47<00:00,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 5, batch 1563 complete! Training Loss : 0.7396369215317895\n","Epoch 5, batch 1563 complete! Training Accuracy : 0.4115152242627936\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 174/174 [00:38<00:00,  4.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5, batch 1563 complete! Validation Loss : 1.6704915364583333\n","Epoch 5, batch 1563 complete! Validation Accuracy : 0.32901367890568756\n","Validation loss changed from 1.6663018588362069 to 1.6704915364583333\n","Best validation accuracy improved from 0.32685385169186465 to 0.32901367890568756\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1563/1563 [18:27<00:00,  1.41it/s]"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Sun Nov 14 04:17:40 2021_lr_2e-05_val_acc_0.329_ep_5.pt\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["train_bert(net, criterion, opti, LEARNING_RATE, lr_scheduler, train_loader, val_loader, EPOCHS, iters_to_accumulate)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["net.load_state_dict(torch.load('models/Sun Nov 14 04:17:40 2021_lr_2e-05_val_acc_0.329_ep_5.pt'))"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"data":{"text/plain":["{'input_ids': [0, 31414, 232, 2], 'attention_mask': [1, 1, 1, 1]}"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer(\"Hello world\")"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["torch.save(net.state_dict(), 'models/final_model.pt')"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":[" 37%|███▋      | 71/193 [00:16<00:27,  4.43it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","100%|██████████| 193/193 [00:43<00:00,  4.41it/s]\n"]}],"source":["test_set = FriendsDataset(dataframe=df_test, tokenizer=tokenizer, max_length=MAX_LEN)\n","test_loader = DataLoader(test_set, batch_size=VALID_BATCH_SIZE, shuffle=False, num_workers=1)\n","def predict(net, device, dataloader):\n","    net.eval()\n","    predictions = []\n","\n","    with torch.no_grad():\n","        for it, (seq, attn_masks, token_type_ids) in enumerate(tqdm(dataloader)):\n","            seq, attn_masks, token_type_ids = \\\n","                seq.to(device), attn_masks.to(device), token_type_ids.to(device)\n","            logits = net(seq, attn_masks, token_type_ids)\n","            max_logits, argmax_idx = torch.max(logits.data, dim=1)\n","            predictions.extend(argmax_idx.tolist())\n","    del logits\n","    return predictions\n","preds = predict(net, device, test_loader)"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>other_speaker</th>\n","      <th>friend_response</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>Но я безработный, моя музыка - это все, что у ...</td>\n","      <td>Меня застрелят. Любой совет?</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>Посмотри. Пятьсот семнадцать коробок!</td>\n","      <td>Боже мой, как ты это сделал?</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>Хорошо. Хорошо. Помогло бы, если бы я подошел ...</td>\n","      <td>Это было бы очень полезно!</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>Росс, чего ты так долго?</td>\n","      <td>Простите, это как будто не для быстрого отдыха!</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>Так кто ты?</td>\n","      <td>Ну, наши имена действительно Моника и Чендлер....</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3081</th>\n","      <td>3081</td>\n","      <td>Дело не только в барабанах. Каждые пять минут ...</td>\n","      <td>Понимаете, именно так нормальные люди должны р...</td>\n","    </tr>\n","    <tr>\n","      <th>3082</th>\n","      <td>3082</td>\n","      <td>Кажется, я случайно использовал коробки Моники...</td>\n","      <td>Боже, все испорчено! Папа, она будет раздавлена!</td>\n","    </tr>\n","    <tr>\n","      <th>3083</th>\n","      <td>3083</td>\n","      <td>ну знаете, вот почему через несколько лет расп...</td>\n","      <td>Ой, это так здорово.</td>\n","    </tr>\n","    <tr>\n","      <th>3084</th>\n","      <td>3084</td>\n","      <td>Он переспал с тобой, а потом никогда тебе не з...</td>\n","      <td>А я просто хотела нового папу для Дэви и Бекки.</td>\n","    </tr>\n","    <tr>\n","      <th>3085</th>\n","      <td>3085</td>\n","      <td>Я знаю, разве он не классный? Так приятно нако...</td>\n","      <td>Ну, может он скоро уедет, как в классную поезд...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3086 rows × 3 columns</p>\n","</div>"],"text/plain":["        Id                                      other_speaker  \\\n","0        0  Но я безработный, моя музыка - это все, что у ...   \n","1        1              Посмотри. Пятьсот семнадцать коробок!   \n","2        2  Хорошо. Хорошо. Помогло бы, если бы я подошел ...   \n","3        3                           Росс, чего ты так долго?   \n","4        4                                        Так кто ты?   \n","...    ...                                                ...   \n","3081  3081  Дело не только в барабанах. Каждые пять минут ...   \n","3082  3082  Кажется, я случайно использовал коробки Моники...   \n","3083  3083  ну знаете, вот почему через несколько лет расп...   \n","3084  3084  Он переспал с тобой, а потом никогда тебе не з...   \n","3085  3085  Я знаю, разве он не классный? Так приятно нако...   \n","\n","                                        friend_response  \n","0                          Меня застрелят. Любой совет?  \n","1                          Боже мой, как ты это сделал?  \n","2                            Это было бы очень полезно!  \n","3       Простите, это как будто не для быстрого отдыха!  \n","4     Ну, наши имена действительно Моника и Чендлер....  \n","...                                                 ...  \n","3081  Понимаете, именно так нормальные люди должны р...  \n","3082   Боже, все испорчено! Папа, она будет раздавлена!  \n","3083                               Ой, это так здорово.  \n","3084    А я просто хотела нового папу для Дэви и Бекки.  \n","3085  Ну, может он скоро уедет, как в классную поезд...  \n","\n","[3086 rows x 3 columns]"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["df_test"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Category</th>\n","    </tr>\n","    <tr>\n","      <th>Id</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ФИБИ</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>МОНИКА</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>РЕЙЧЕЛ</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>РОСС</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ЧЕНДЛЕР</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3081</th>\n","      <td>ФИБИ</td>\n","    </tr>\n","    <tr>\n","      <th>3082</th>\n","      <td>МОНИКА</td>\n","    </tr>\n","    <tr>\n","      <th>3083</th>\n","      <td>РЕЙЧЕЛ</td>\n","    </tr>\n","    <tr>\n","      <th>3084</th>\n","      <td>МОНИКА</td>\n","    </tr>\n","    <tr>\n","      <th>3085</th>\n","      <td>РЕЙЧЕЛ</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3086 rows × 1 columns</p>\n","</div>"],"text/plain":["     Category\n","Id           \n","0        ФИБИ\n","1      МОНИКА\n","2      РЕЙЧЕЛ\n","3        РОСС\n","4     ЧЕНДЛЕР\n","...       ...\n","3081     ФИБИ\n","3082   МОНИКА\n","3083   РЕЙЧЕЛ\n","3084   МОНИКА\n","3085   РЕЙЧЕЛ\n","\n","[3086 rows x 1 columns]"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["answers = pd.DataFrame(\n","    names_to_cats.inverse_transform(preds), \n","    index=df_test.Id, columns=[\"Category\"])\n","answers.to_csv('submission1.csv')\n","answers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bxhl6QhJlbfE"},"outputs":[],"source":["from transformers import get_constant_schedule\n","opti = AdamW(net.parameters(), lr=LEARNING_RATE, weight_decay=1e-2)\n","lr_scheduler = get_constant_schedule(optimizer=opti)\n","train_set = FriendsDataset(dataframe=df_train.iloc[5000:], tokenizer=tokenizer, max_length=MAX_LEN)\n","\n","val_set = FriendsDataset(dataframe=df_val, tokenizer=tokenizer, max_length=MAX_LEN)\n","# Creating instances of training and validation dataloaders\n","train_loader = DataLoader(train_set, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=2)\n","val_loader = DataLoader(val_set, batch_size=VALID_BATCH_SIZE, shuffle=False, num_workers=2)\n","\n","train_bert(net, criterion, opti, LEARNING_RATE, lr_scheduler, train_loader, val_loader, EPOCHS, iters_to_accumulate)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":939,"status":"ok","timestamp":1636033344945,"user":{"displayName":"Andrew Argatkiny","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04422905123959661601"},"user_tz":-420},"id":"r6o9XkFVL6qD","outputId":"2c4ae139-f975-4f8c-e7b0-88897023e72e"},"outputs":[{"data":{"text/plain":["(1.7879175646551724, 0.1861051115910727)"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["val_loss, val_accuracy"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":268,"status":"ok","timestamp":1636042247525,"user":{"displayName":"Andrew Argatkiny","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04422905123959661601"},"user_tz":-420},"id":"BnAB6Udp9Z-1"},"outputs":[],"source":["import gc\n","gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":448,"status":"ok","timestamp":1636024184045,"user":{"displayName":"Andrew Argatkiny","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04422905123959661601"},"user_tz":-420},"id":"VEInewq0pYTk","outputId":"f8944075-b0f8-41a9-8378-f5fc6d1f90fb"},"outputs":[{"data":{"text/plain":["8506.769408"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["torch.cuda.get_device_properties(0).total_memory / 1e6"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11448,"status":"ok","timestamp":1635932769348,"user":{"displayName":"Andrew Argatkiny","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04422905123959661601"},"user_tz":-420},"id":"zYNBGqTwpoHn","outputId":"74b194fd-8d8b-4c9f-a35c-9d95859f2c6d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Gen RAM Free: 11.1 GB  | Proc size: 5.1 GB\n","GPU RAM Free: 10137MB | Used: 1304MB | Util  11% | Total 11441MB\n"]}],"source":["# Check that we are using 100% of GPU memory footprint support libraries/code\n","# from https://github.com/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb\n","!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n","!pip -q install gputil\n","!pip -q install psutil\n","!pip -q install humanize\n","import psutil\n","import humanize\n","import os\n","import GPUtil as GPU\n","GPUs = GPU.getGPUs()\n","# XXX: only one GPU on Colab and isn’t guaranteed\n","gpu = GPUs[0]\n","def printm():\n"," process = psutil.Process(os.getpid())\n"," print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n"," print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n","printm()\n"," #!kill -9 -1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":452,"status":"ok","timestamp":1635932801898,"user":{"displayName":"Andrew Argatkiny","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04422905123959661601"},"user_tz":-420},"id":"e9gyehyY6n8C","outputId":"0e7574a4-b8d7-44fa-8f06-717912d52d5a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Gen RAM Free: 11.1 GB  | Proc size: 5.1 GB\n","GPU RAM Free: 10137MB | Used: 1304MB | Util  11% | Total 11441MB\n"]}],"source":["printm()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v1kSrbCvMNDf"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNFzSvllZcK0G034IIhIOS7","collapsed_sections":[],"mount_file_id":"144h_eAZbCarOnrPb3zxIWY7HYm27TGhi","name":"FriendsPredictNLP","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0344eb1638a44fc0bd8be3d2b5aafc99":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2cbc04f6abc4b45b5374f424dd88efa","placeholder":"​","style":"IPY_MODEL_91647e75ea5e4c16a11046fda73ba026","value":" 521/521 [00:00&lt;00:00, 12.4kB/s]"}},"0f3925eaafb9438ab0f6b27c356cfe99":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_82b0759acdb845ea9859d2f96ce0e5d3","placeholder":"​","style":"IPY_MODEL_487d23c4e2b64adda4a945df313d1ed1","value":" 1.70M/1.70M [00:00&lt;00:00, 7.10MB/s]"}},"121075183d2441ddb2e99f849d2e9686":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f4f04df6382427288c4e9422e4fbf7d","placeholder":"​","style":"IPY_MODEL_e3aeb80a52bf4ccdb69330b3ebce4355","value":" 683M/683M [00:23&lt;00:00, 32.0MB/s]"}},"158f6d85beec4b33bdc422ee9e115808":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2e6a2c5306764e31bea3a9e37121ff8c","placeholder":"​","style":"IPY_MODEL_faaf710ce2cc471c9370811df2f54ba6","value":" 655/655 [00:00&lt;00:00, 15.7kB/s]"}},"192f33925dd045d792b38a2444794a10":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"21bf688381ef4a8495a3a6981a84ef8d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_939389df612a4f23b8ab42d6dccd6c8d","max":323,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6c7f0e73512743efb870026d6677531e","value":323}},"269978945c9c48d280cd1eb1ddf2f7fc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d185bee7972497296fc31ba5c0d54d6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e6a2c5306764e31bea3a9e37121ff8c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"317c759beaf6494db054eae9162ccfb2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32acd792c0bf45419a87561dc4ccdb4c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d8a373f8b1a4da3841d55ed5fd62935":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b968066f8f924c309860b111ae0816a4","IPY_MODEL_98ff0c5dbe8049c588ef631dfbc08ef8","IPY_MODEL_121075183d2441ddb2e99f849d2e9686"],"layout":"IPY_MODEL_5435b27f9fab4fcd9f0cf0fe1f11db71"}},"4718ae66701049229c66c8506b3efe1b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_65cbb712ea2f4dab90ba6600d8440c0b","placeholder":"​","style":"IPY_MODEL_d70b870ad108499e8651b57c9e9be34b","value":"Downloading: 100%"}},"487d23c4e2b64adda4a945df313d1ed1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4e0b4d1769974ba88e49d4fb02e2c4a2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"52e9ba2a4d974b15ae4b6ab1232dc256":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5435b27f9fab4fcd9f0cf0fe1f11db71":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54feb1dd180f4bf8b47c18713336d41c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58564c36a43e4f3ab99a0e153cb5e480":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"58c863f270ac42c3b195987575a8bc9a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5e3622f69800454d9704f414226d5869":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5fa6528c3aa5487e87edb8797ebf465e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"632a7169fbd74159b11d92ab6cbacfa7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6502e372ae624b618f6248ee2ea778b6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65cbb712ea2f4dab90ba6600d8440c0b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c7b0a0b13b3450fa83c21af7c8525a2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c7f0e73512743efb870026d6677531e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6cd8338fff6a4f469d417911e8b8142b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_54feb1dd180f4bf8b47c18713336d41c","placeholder":"​","style":"IPY_MODEL_7148c15a118641d3a9488fd981cc3bf3","value":"Downloading: 100%"}},"7148c15a118641d3a9488fd981cc3bf3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"717056921033432aa1612afdd7aab763":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d900d48dde840e0824e409582df0dd2","placeholder":"​","style":"IPY_MODEL_58564c36a43e4f3ab99a0e153cb5e480","value":"Downloading: 100%"}},"73c9f7d056704fd2ac03cb76d6f36df5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d83773323bd4e90bc86003c4220a662","placeholder":"​","style":"IPY_MODEL_df27936694364ed583b70df9a0870a08","value":"Downloading: 100%"}},"769e20765ef64788b8de218a9853c99e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"77b7765f0fe04c8a976230fb2eb01065":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"784667efc3604b52a7a3e90afeeba909":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79f29a0c5f01482dbbb7700bd7b573a7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4718ae66701049229c66c8506b3efe1b","IPY_MODEL_bddc64019a20420589041d59a8ac1e86","IPY_MODEL_158f6d85beec4b33bdc422ee9e115808"],"layout":"IPY_MODEL_d469fb245e2244848f21d12c9897e0e8"}},"7d83773323bd4e90bc86003c4220a662":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f4f04df6382427288c4e9422e4fbf7d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82b0759acdb845ea9859d2f96ce0e5d3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"830958dc67714696a237fc7a051931bc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"85aa7a96758841a3bb30895cf30d9912":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88b743167cd44b318de6a3998f466556":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d900d48dde840e0824e409582df0dd2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e60d43dcd7e4df9a554b7b8ed28c56d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a7c1b532fc64473eab127d4f73c4ffd7","placeholder":"​","style":"IPY_MODEL_192f33925dd045d792b38a2444794a10","value":"Downloading: 100%"}},"91647e75ea5e4c16a11046fda73ba026":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"939389df612a4f23b8ab42d6dccd6c8d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"972de0d98911484dad5f2c7b560d973c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_269978945c9c48d280cd1eb1ddf2f7fc","max":521,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f543e115e78f40f7bc9813a76b5ba862","value":521}},"98ff0c5dbe8049c588ef631dfbc08ef8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e55762599f664a0fa5288de998af11c1","max":716133354,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ef630b6fc27b4750839918ef7ac13c6c","value":716133354}},"a56c41e3f2744ba6b2f5cd408a86e6d7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8e60d43dcd7e4df9a554b7b8ed28c56d","IPY_MODEL_ebbe8ae171e94413be8fcc944c05b28b","IPY_MODEL_c280900ff8ba4cc8a2dbf30daad33a4b"],"layout":"IPY_MODEL_d15f42cd6e9d427c99f12b45ba2b04e1"}},"a7c1b532fc64473eab127d4f73c4ffd7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a825221a71434b82be79352ad867c68a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a89df2c0f77a445eb9a12550fcaed50d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6cd8338fff6a4f469d417911e8b8142b","IPY_MODEL_edd582160bd349c68d35b5461b9771c2","IPY_MODEL_f1868ee80eff44af8596b13875f7cddd"],"layout":"IPY_MODEL_ee730afc554f486591437c1c9de33490"}},"adef346525b5428cb77ce85b67b7de02":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f32062e1b8f74b2ba3ebb65244b8345d","IPY_MODEL_21bf688381ef4a8495a3a6981a84ef8d","IPY_MODEL_d601368d185f4c2f8de3b222976c42ef"],"layout":"IPY_MODEL_632a7169fbd74159b11d92ab6cbacfa7"}},"b968066f8f924c309860b111ae0816a4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_88b743167cd44b318de6a3998f466556","placeholder":"​","style":"IPY_MODEL_77b7765f0fe04c8a976230fb2eb01065","value":"Downloading: 100%"}},"b9b60187d34644bda94b224017bc450d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bddc64019a20420589041d59a8ac1e86":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d185bee7972497296fc31ba5c0d54d6","max":655,"min":0,"orientation":"horizontal","style":"IPY_MODEL_830958dc67714696a237fc7a051931bc","value":655}},"c280900ff8ba4cc8a2dbf30daad33a4b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_317c759beaf6494db054eae9162ccfb2","placeholder":"​","style":"IPY_MODEL_4e0b4d1769974ba88e49d4fb02e2c4a2","value":" 1.70M/1.70M [00:00&lt;00:00, 3.07MB/s]"}},"d15f42cd6e9d427c99f12b45ba2b04e1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3ce46e504bd4daf9ac3763a7cdc222f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d469fb245e2244848f21d12c9897e0e8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5ed4335e6bb418aaebd5316b616f54c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d601368d185f4c2f8de3b222976c42ef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_32acd792c0bf45419a87561dc4ccdb4c","placeholder":"​","style":"IPY_MODEL_769e20765ef64788b8de218a9853c99e","value":" 323/323 [00:00&lt;00:00, 7.17kB/s]"}},"d70b870ad108499e8651b57c9e9be34b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"df27936694364ed583b70df9a0870a08":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e2cbc04f6abc4b45b5374f424dd88efa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3aeb80a52bf4ccdb69330b3ebce4355":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e55762599f664a0fa5288de998af11c1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e87825600783430391ed6af6be32c278":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_73c9f7d056704fd2ac03cb76d6f36df5","IPY_MODEL_972de0d98911484dad5f2c7b560d973c","IPY_MODEL_0344eb1638a44fc0bd8be3d2b5aafc99"],"layout":"IPY_MODEL_b9b60187d34644bda94b224017bc450d"}},"e9d9038ad82440aebdb0fb714add647d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_717056921033432aa1612afdd7aab763","IPY_MODEL_f010001c717b439797f738bb4353b74d","IPY_MODEL_0f3925eaafb9438ab0f6b27c356cfe99"],"layout":"IPY_MODEL_85aa7a96758841a3bb30895cf30d9912"}},"ebbe8ae171e94413be8fcc944c05b28b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6502e372ae624b618f6248ee2ea778b6","max":1780720,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d5ed4335e6bb418aaebd5316b616f54c","value":1780720}},"edd582160bd349c68d35b5461b9771c2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_784667efc3604b52a7a3e90afeeba909","max":112,"min":0,"orientation":"horizontal","style":"IPY_MODEL_58c863f270ac42c3b195987575a8bc9a","value":112}},"ee730afc554f486591437c1c9de33490":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef630b6fc27b4750839918ef7ac13c6c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f010001c717b439797f738bb4353b74d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d3ce46e504bd4daf9ac3763a7cdc222f","max":1780720,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5fa6528c3aa5487e87edb8797ebf465e","value":1780720}},"f1868ee80eff44af8596b13875f7cddd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c7b0a0b13b3450fa83c21af7c8525a2","placeholder":"​","style":"IPY_MODEL_5e3622f69800454d9704f414226d5869","value":" 112/112 [00:00&lt;00:00, 2.29kB/s]"}},"f32062e1b8f74b2ba3ebb65244b8345d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_52e9ba2a4d974b15ae4b6ab1232dc256","placeholder":"​","style":"IPY_MODEL_a825221a71434b82be79352ad867c68a","value":"Downloading: 100%"}},"f543e115e78f40f7bc9813a76b5ba862":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"faaf710ce2cc471c9370811df2f54ba6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
