{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":304,"status":"ok","timestamp":1636037468369,"user":{"displayName":"Andrew Argatkiny","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04422905123959661601"},"user_tz":-420},"id":"jfX9Mo2IRKkS","outputId":"72476e94-f6d5-4f3f-9289-95aba0fefb20"},"outputs":[{"name":"stdout","output_type":"stream","text":["Python 3.9.7\n"]}],"source":["! python -V"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformers\n","  Downloading transformers-4.12.3-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 626 kB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/andrew/anaconda3/envs/nn/lib/python3.9/site-packages (from transformers) (1.21.2)\n","Collecting filelock\n","  Downloading filelock-3.3.2-py3-none-any.whl (9.7 kB)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (661 kB)\n","\u001b[K     |████████████████████████████████| 661 kB 9.8 MB/s \n","\u001b[?25hCollecting regex!=2019.12.17\n","  Downloading regex-2021.11.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (762 kB)\n","\u001b[K     |████████████████████████████████| 762 kB 9.7 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.1.0-py3-none-any.whl (59 kB)\n","\u001b[K     |████████████████████████████████| 59 kB 4.7 MB/s \n","\u001b[?25hCollecting requests\n","  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n","\u001b[K     |████████████████████████████████| 62 kB 712 kB/s \n","\u001b[?25hCollecting tqdm>=4.27\n","  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n","\u001b[K     |████████████████████████████████| 76 kB 3.7 MB/s \n","\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 9.8 MB/s \n","\u001b[?25hCollecting packaging>=20.0\n","  Downloading packaging-21.2-py3-none-any.whl (40 kB)\n","\u001b[K     |████████████████████████████████| 40 kB 4.5 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 10.4 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /home/andrew/anaconda3/envs/nn/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Collecting pyparsing<3,>=2.0.2\n","  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 4.4 MB/s \n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /home/andrew/anaconda3/envs/nn/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n","Collecting charset-normalizer~=2.0.0\n","  Downloading charset_normalizer-2.0.7-py3-none-any.whl (38 kB)\n","Collecting urllib3<1.27,>=1.21.1\n","  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n","\u001b[K     |████████████████████████████████| 138 kB 10.4 MB/s \n","\u001b[?25hCollecting idna<4,>=2.5\n","  Downloading idna-3.3-py3-none-any.whl (61 kB)\n","\u001b[K     |████████████████████████████████| 61 kB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: joblib in /home/andrew/anaconda3/envs/nn/lib/python3.9/site-packages (from sacremoses->transformers) (1.1.0)\n","Collecting click\n","  Downloading click-8.0.3-py3-none-any.whl (97 kB)\n","\u001b[K     |████████████████████████████████| 97 kB 4.7 MB/s \n","\u001b[?25hRequirement already satisfied: six in /home/andrew/anaconda3/envs/nn/lib/python3.9/site-packages (from sacremoses->transformers) (1.16.0)\n","Installing collected packages: urllib3, pyparsing, idna, charset-normalizer, tqdm, requests, regex, pyyaml, packaging, filelock, click, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyparsing\n","    Found existing installation: pyparsing 3.0.4\n","    Uninstalling pyparsing-3.0.4:\n","      Successfully uninstalled pyparsing-3.0.4\n","Successfully installed charset-normalizer-2.0.7 click-8.0.3 filelock-3.3.2 huggingface-hub-0.1.0 idna-3.3 packaging-21.2 pyparsing-2.4.7 pyyaml-6.0 regex-2021.11.2 requests-2.26.0 sacremoses-0.0.46 tokenizers-0.10.3 tqdm-4.62.3 transformers-4.12.3 urllib3-1.26.7\n"]}],"source":["! pip install transformers"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":28083,"status":"ok","timestamp":1636037505252,"user":{"displayName":"Andrew Argatkiny","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04422905123959661601"},"user_tz":-420},"id":"JplCyr0VKrhP"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /home/andrew/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["import random \n","import time\n","import copy\n","\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import torch\n","import torch.nn as nn\n","from torch.cuda.amp import autocast, GradScaler\n","\n","from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n","from transformers import BertTokenizer, BertModel, AdamW\n","from transformers import RobertaTokenizer, RobertaModel\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","\n","from torch.utils.data import Dataset, DataLoader\n","from word_aug import get_aug_dataset\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["torch.cuda.is_available()"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":209,"referenced_widgets":["adef346525b5428cb77ce85b67b7de02","632a7169fbd74159b11d92ab6cbacfa7","f32062e1b8f74b2ba3ebb65244b8345d","21bf688381ef4a8495a3a6981a84ef8d","d601368d185f4c2f8de3b222976c42ef","a825221a71434b82be79352ad867c68a","52e9ba2a4d974b15ae4b6ab1232dc256","6c7f0e73512743efb870026d6677531e","939389df612a4f23b8ab42d6dccd6c8d","769e20765ef64788b8de218a9853c99e","32acd792c0bf45419a87561dc4ccdb4c","79f29a0c5f01482dbbb7700bd7b573a7","d469fb245e2244848f21d12c9897e0e8","4718ae66701049229c66c8506b3efe1b","bddc64019a20420589041d59a8ac1e86","158f6d85beec4b33bdc422ee9e115808","d70b870ad108499e8651b57c9e9be34b","65cbb712ea2f4dab90ba6600d8440c0b","830958dc67714696a237fc7a051931bc","2d185bee7972497296fc31ba5c0d54d6","faaf710ce2cc471c9370811df2f54ba6","2e6a2c5306764e31bea3a9e37121ff8c","a56c41e3f2744ba6b2f5cd408a86e6d7","d15f42cd6e9d427c99f12b45ba2b04e1","8e60d43dcd7e4df9a554b7b8ed28c56d","ebbe8ae171e94413be8fcc944c05b28b","c280900ff8ba4cc8a2dbf30daad33a4b","192f33925dd045d792b38a2444794a10","a7c1b532fc64473eab127d4f73c4ffd7","d5ed4335e6bb418aaebd5316b616f54c","6502e372ae624b618f6248ee2ea778b6","4e0b4d1769974ba88e49d4fb02e2c4a2","317c759beaf6494db054eae9162ccfb2","a89df2c0f77a445eb9a12550fcaed50d","ee730afc554f486591437c1c9de33490","6cd8338fff6a4f469d417911e8b8142b","edd582160bd349c68d35b5461b9771c2","f1868ee80eff44af8596b13875f7cddd","7148c15a118641d3a9488fd981cc3bf3","54feb1dd180f4bf8b47c18713336d41c","58c863f270ac42c3b195987575a8bc9a","784667efc3604b52a7a3e90afeeba909","5e3622f69800454d9704f414226d5869","6c7b0a0b13b3450fa83c21af7c8525a2","e9d9038ad82440aebdb0fb714add647d","85aa7a96758841a3bb30895cf30d9912","717056921033432aa1612afdd7aab763","f010001c717b439797f738bb4353b74d","0f3925eaafb9438ab0f6b27c356cfe99","58564c36a43e4f3ab99a0e153cb5e480","8d900d48dde840e0824e409582df0dd2","5fa6528c3aa5487e87edb8797ebf465e","d3ce46e504bd4daf9ac3763a7cdc222f","487d23c4e2b64adda4a945df313d1ed1","82b0759acdb845ea9859d2f96ce0e5d3","e87825600783430391ed6af6be32c278","b9b60187d34644bda94b224017bc450d","73c9f7d056704fd2ac03cb76d6f36df5","972de0d98911484dad5f2c7b560d973c","0344eb1638a44fc0bd8be3d2b5aafc99","df27936694364ed583b70df9a0870a08","7d83773323bd4e90bc86003c4220a662","f543e115e78f40f7bc9813a76b5ba862","269978945c9c48d280cd1eb1ddf2f7fc","91647e75ea5e4c16a11046fda73ba026","e2cbc04f6abc4b45b5374f424dd88efa"]},"executionInfo":{"elapsed":4421,"status":"ok","timestamp":1636037509668,"user":{"displayName":"Andrew Argatkiny","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04422905123959661601"},"user_tz":-420},"id":"o24cy8ORGJjU","outputId":"2c57e157-d12e-41c3-9631-cd59f18b134b"},"outputs":[],"source":["#CHECKPOINT = \"sberbank-ai/sbert_large_nlu_ru\"\n","MAX_LEN = 256\n","TRAIN_BATCH_SIZE = 16\n","VALID_BATCH_SIZE = 16\n","EPOCHS = 3\n","LEARNING_RATE = 2e-05\n","#tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n","CHECKPOINT = \"bert-base-uncased\"\n","tokenizer = BertTokenizer.from_pretrained(CHECKPOINT)\n","\n","def get_individual_labels(df):\n","    labels = pd.get_dummies(df.label).rename({\n","        \"ДЖОУИ\": \"Joey\", \"МОНИКА\": \"Monica\", \"РЕЙЧЕЛ\": \"Rachel\", \"РОСС\": \"Ross\", \n","        \"ФИБИ\": \"Phoebe\", \"ЧЕНДЛЕР\": \"Chandler\"\n","    }, axis=1)\n","    return pd.concat([df, labels], axis=1)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1636037511385,"user":{"displayName":"Andrew Argatkiny","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04422905123959661601"},"user_tz":-420},"id":"iuShp08vL62l","outputId":"885c5ad6-61d9-40c9-ae33-5be1da37a048"},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/andrew/ml/friends-classification\n","mkdir: cannot create directory ‘models’: File exists\n","english\n","example_eng_ru.csv\n","fb_model_translate_en_ru_2_friend_response_test.csv\n","fb_model_translate_en_ru_2_friend_response_train.csv\n","fb_model_translate_en_ru_2_friend_response_val.csv\n","fb_model_translate_en_ru_2_other_speaker_test.csv\n","fb_model_translate_en_ru_2_other_speaker_train.csv\n","fb_model_translate_en_ru_2_other_speaker_val.csv\n","fb_model_translate_ru_en_1_friend_response_test.csv\n","fb_model_translate_ru_en_1_friend_response_train.csv\n","fb_model_translate_ru_en_1_friend_response_val.csv\n","fb_model_translate_ru_en_1_other_speaker_test.csv\n","fb_model_translate_ru_en_1_other_speaker_train.csv\n","fb_model_translate_ru_en_1_other_speaker_val.csv\n","final_model.pt\n","final_submission.csv\n","helsinki_model_translate_ru_en_1_friend_response_test.csv\n","helsinki_model_translate_ru_en_1_friend_response_train.csv\n","helsinki_model_translate_ru_en_1_friend_response_val.csv\n","helsinki_model_translate_ru_en_1_other_speaker_test.csv\n","helsinki_model_translate_ru_en_1_other_speaker_train.csv\n","helsinki_model_translate_ru_en_1_other_speaker_val.csv\n","models\n","submission1.csv\n","test.csv\n","test_data_eng_fb_model.csv\n","test_data_eng_google_model.csv\n","test_data_eng_helsinki_model.csv\n","test_data_rus_fb_model.csv\n","train_data.csv\n","train_data_eng_fb_model.csv\n","train_data_eng_google_model.csv\n","train_data_eng_helsinki_model.csv\n","train_data_rus_fb_model.csv\n","val_data.csv\n","val_data_eng_fb_model.csv\n","val_data_eng_google_model.csv\n","val_data_eng_helsinki_model.csv\n","val_data_rus_fb_model.csv\n"]}],"source":["%cd friends-classification/\n","!mkdir models\n","! ls"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":459,"status":"ok","timestamp":1636037511841,"user":{"displayName":"Andrew Argatkiny","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04422905123959661601"},"user_tz":-420},"id":"acv7o-iyMo2n","outputId":"e1180782-7199-4b5b-ef05-41f2f3d4d114"},"outputs":[{"name":"stdout","output_type":"stream","text":["РОСС       0.176569\n","РЕЙЧЕЛ     0.176089\n","ЧЕНДЛЕР    0.170568\n","ДЖОУИ      0.166287\n","МОНИКА     0.160525\n","ФИБИ       0.149962\n","Name: label, dtype: float64\n","\n","РОСС       0.176746\n","РЕЙЧЕЛ     0.176026\n","ЧЕНДЛЕР    0.170626\n","ДЖОУИ      0.166307\n","МОНИКА     0.160547\n","ФИБИ       0.149748\n","Name: label, dtype: float64\n"]}],"source":["df_train = pd.read_csv('english/df_train_eng.csv').rename({'Category': 'label'}, axis=1)\n","df_train.other_speaker.fillna('', inplace=True)\n","df_train.friend_response.fillna('', inplace=True)\n","df_val = pd.read_csv('english/df_val_eng.csv')\n","df_val.other_speaker.fillna('', inplace=True)\n","df_val.friend_response.fillna('', inplace=True)\n","df_test = pd.read_csv('english/df_test_eng.csv')\n","df_test.other_speaker.fillna('', inplace=True)\n","df_test.friend_response.fillna('', inplace=True)\n","\n","df_train = get_individual_labels(df_train)\n","df_val = get_individual_labels(df_val)\n","\n","# Encoding target variable\n","names_to_cats = LabelEncoder()\n","df_train['label_code'] = names_to_cats.fit_transform(df_train.label)\n","df_val['label_code'] = names_to_cats.transform(df_val.label)\n","df_fb_train = pd.read_csv('train_data_rus_fb_model.csv')\n","df_full = pd.concat([df_train, df_val])\n","print(df_train[\"label\"].value_counts()/df_train.shape[0])\n","print()\n","print(df_val[\"label\"].value_counts()/df_val.shape[0])"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":264,"status":"ok","timestamp":1636041805080,"user":{"displayName":"Andrew Argatkiny","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04422905123959661601"},"user_tz":-420},"id":"6fwxjlLEGzIT"},"outputs":[],"source":["class FriendsDataset(Dataset):\n","    def __init__(self, dataframe, tokenizer, max_length=512, padding='max_length', \n","                 with_labels=True):\n","\n","        self.dataframe = dataframe  # pandas dataframe\n","        #Initialize the tokenizer\n","        self.tokenizer = tokenizer  \n","        self.padding = padding\n","        self.max_length = max_length\n","        \n","        self.with_labels = with_labels \n","        if 'label' not in self.dataframe.columns:\n","          self.with_labels = False\n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","\n","    def __getitem__(self, index):\n","\n","        # Selecting sentence1 and sentence2 at the specified index in the data frame\n","        sent1 = self.dataframe.other_speaker.iloc[index]\n","        sent2 = self.dataframe.friend_response.iloc[index]\n","\n","        # Tokenize the pair of sentences to get token ids, attention masks and token type ids\n","        encoded_pair = self.tokenizer(sent1, sent2, \n","                                      padding=self.padding,  # Pad to max_length\n","                                      truncation=True,  # Truncate to max_length\n","                                      max_length=self.max_length,  \n","                                      return_tensors='pt')  # Return torch.Tensor objects\n","        \n","        token_ids = encoded_pair['input_ids'].squeeze(0)  # tensor of token ids\n","        attn_masks = encoded_pair['attention_mask'].squeeze(0)  # binary tensor with \"0\" for padded values and \"1\" for the other values\n","        token_type_ids = encoded_pair['token_type_ids'].squeeze(0)  # binary tensor with \"0\" for the 1st sentence tokens & \"1\" for the 2nd sentence tokens\n","\n","        if self.with_labels:  # True if the dataset has labels\n","            label = self.dataframe.label_code.iloc[index]\n","            #label = self.dataframe.Phoebe.iloc[index]\n","            return token_ids, attn_masks, token_type_ids, label  \n","        else:\n","            return token_ids, attn_masks, token_type_ids"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1636037511843,"user":{"displayName":"Andrew Argatkiny","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04422905123959661601"},"user_tz":-420},"id":"jLmUmq46a0D6"},"outputs":[],"source":["class SentencePairClassifier(nn.Module):\n","\n","    def __init__(self, model=CHECKPOINT, freeze_model=True):\n","        super(SentencePairClassifier, self).__init__()\n","        #  Instantiating BERT-based model object\n","        # self.pretrained_layer = AutoModel.from_pretrained(CHECKPOINT)\n","        self.pretrained_layer = BertModel.from_pretrained(CHECKPOINT)\n","        #self.pretrained_layer = RobertaModel.from_pretrained(CHECKPOINT)\n","\n","        hidden_size = self.pretrained_layer.config.hidden_size\n","\n","        # Freeze model layers and only train the classification layer weights\n","        if freeze_model:\n","            for p in self.pretrained_layer.parameters():\n","                p.requires_grad = False\n","            print('All parameters frozen')\n","        # Classification layer\n","        self.cls_layer = nn.Linear(hidden_size, 6)\n","\n","        self.dropout = nn.Dropout(p=0.3)\n","\n","    @autocast()  # run in mixed precision\n","    def forward(self, input_ids, attn_masks, token_type_ids):\n","        '''\n","        Inputs:\n","            -input_ids : Tensor  containing token ids\n","            -attn_masks : Tensor containing attention masks to be used to focus on non-padded values\n","            -token_type_ids : Tensor containing token type ids to be used to identify sentence1 and sentence2\n","        '''\n","\n","        # Feeding the inputs to the BERT-based model to obtain contextualized representations\n","        output = self.pretrained_layer(input_ids, attn_masks, token_type_ids)\n","\n","        # Feeding to the classifier layer the last layer hidden-state of the [CLS] token further processed by a\n","        # Linear Layer and a Tanh activation. The Linear layer weights were trained from the sentence order prediction (ALBERT) or next sentence prediction (BERT)\n","        # objective during pre-training.\n","        logits = self.cls_layer(self.dropout(output.pooler_output))\n","\n","        return logits"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1636037511843,"user":{"displayName":"Andrew Argatkiny","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04422905123959661601"},"user_tz":-420},"id":"JKiUIjwGiWEv"},"outputs":[],"source":["def set_seed(seed):\n","    \"\"\" Set all seeds to make results reproducible \"\"\"\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    #os.environ['PYTHONHASHSEED'] = str(seed)\n","    \n","@autocast()\n","def evaluate_loss(net, device, criterion, dataloader):\n","    net.eval()\n","    n_correct = 0\n","    mean_loss = 0\n","    count = 0\n","\n","    with torch.no_grad():\n","        for it, (seq, attn_masks, token_type_ids, labels) in enumerate(tqdm(dataloader)):\n","            seq, attn_masks, token_type_ids, labels = \\\n","                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)\n","            logits = net(seq, attn_masks, token_type_ids)\n","            mean_loss += criterion(logits.squeeze(-1), labels).item()\n","            count += 1\n","            max_logits, argmax_idx = torch.max(logits.data, dim=1)\n","            n_correct += calcuate_accu(argmax_idx, labels)\n","    del logits\n","    return mean_loss / count, n_correct / len(dataloader.dataset)\n","  \n","# Function to calcuate the accuracy of the model\n","def calcuate_accu(big_idx, targets):\n","    n_correct = (big_idx==targets).sum().item()\n","    return n_correct"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":271,"status":"ok","timestamp":1636040507768,"user":{"displayName":"Andrew Argatkiny","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04422905123959661601"},"user_tz":-420},"id":"NNBTL2IC8PMJ"},"outputs":[],"source":["def train_bert(net, criterion, opti, lr, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate):\n","\n","    best_loss = np.Inf\n","    best_acc = 0\n","    best_ep = 1\n","    n_iterations = len(train_loader)\n","    batch_size = train_loader.batch_size\n","    print_every = 1000 // batch_size  # print the training loss this many times per epoch\n","    print_eval_iters = 10000 // batch_size\n","    scaler = GradScaler()\n","\n","    for ep in range(epochs):\n","        net.train()\n","        curr_loss = 0.0\n","        curr_n_correct = 0.\n","        trailing_loss = 0.\n","        trailing_n_correct = 0.\n","        curr_n_tr_examples = 0\n","        trainling_n_tr_examples = 0\n","\n","        for it, (seq, attn_masks, token_type_ids, labels) in enumerate(tqdm(train_loader)):\n","            # Converting to cuda tensors\n","            seq, attn_masks, token_type_ids, labels = \\\n","                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)\n","  \n","            # Enables autocasting for the forward pass (model + loss)\n","            with autocast():\n","                # Obtaining the logits from the model\n","                pooled = net(seq, attn_masks, token_type_ids)\n","\n","                # Computing loss\n","                loss = criterion(pooled.squeeze(-1), labels)\n","                #print(loss, type(loss))\n","                loss = loss / iters_to_accumulate  # Normalize the loss because it is averaged\n","                # Computing accuracy\n","                #print(pooled.squeeze(-1), labels)\n","                curr_loss += loss.item() \n","                big_val, big_idx = torch.max(pooled.data, dim=1)\n","                n_correct = calcuate_accu(big_idx, labels)\n","                curr_n_correct += n_correct\n","\n","            trailing_loss += loss.item() \n","            trailing_n_correct += n_correct\n","            curr_n_tr_examples += labels.size(0)\n","            trainling_n_tr_examples += labels.size(0)\n","\n","            # Backpropagating the gradients\n","            # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n","            scaler.scale(loss).backward()\n","\n","            if (it + 1) % iters_to_accumulate == 0:\n","                # Optimization step\n","                # scaler.step() first unscales the gradients of the optimizer's assigned params.\n","                # If these gradients do not contain infs or NaNs, opti.step() is then called,\n","                # otherwise, opti.step() is skipped.\n","                scaler.step(opti)\n","                # Updates the scale for next iteration.\n","                scaler.update()\n","                # Adjust the learning rate based on the number of iterations.\n","                lr_scheduler.step()\n","                # Clear gradients\n","                opti.zero_grad()\n","\n","            if (it + 1) % print_every == 0:  # Print training loss information\n","                print()\n","                print(\"Batch {}/{} of epoch {} complete. Loss per last {} samples:: {} \"\n","                      .format(it+1, n_iterations, ep+1, curr_n_tr_examples, curr_loss / print_every))\n","                accu_step = (curr_n_correct*100) / curr_n_tr_examples \n","                #print(f\"Training Loss per 5000 steps: {loss_step}\")\n","                print(f\"Training Accuracy per last {curr_n_tr_examples} samples: {accu_step}\")\n","                curr_loss = 0.0\n","                curr_n_tr_examples = 0\n","                curr_n_correct = 0\n","\n","\n","            if (it + 1) % print_eval_iters == 0 or it ==  n_iterations - 1:\n","                del pooled, loss\n","                print(\"Epoch {}, batch {} complete! Training Loss : {}\"\n","                .format(ep+1, it+1, trailing_loss / (it+1)))\n","                print(\"Epoch {}, batch {} complete! Training Accuracy : {}\"\n","                .format(ep+1, it+1, trailing_n_correct / trainling_n_tr_examples))\n","                with autocast():\n","                    val_loss, val_accuracy = evaluate_loss(net, device, criterion, val_loader)  # Compute validation loss\n","                #print()\n","                print(\"Epoch {}, batch {} complete! Validation Loss : {}\".format(ep+1, it+1, val_loss))\n","                print(\"Epoch {}, batch {} complete! Validation Accuracy : {}\".format(ep+1, it+1,val_accuracy))\n","                net.train()\n","                #if val_loss < best_loss:\n","                if val_accuracy > best_acc:\n","                    print(\"Validation loss changed from {} to {}\".format(best_loss, val_loss))\n","                    print(\"Best validation accuracy improved from {} to {}\".format(best_acc, val_accuracy))\n","                    print()\n","                    #net_copy = copy.deepcopy(net)  # save a copy of the model\n","                    best_loss = val_loss\n","                    best_acc = val_accuracy\n","                    best_ep = ep + 1\n","                    # Saving the model\n","                    path_to_model='models/{}_lr_{}_val_acc_{}_ep_{}.pt'.format(time.ctime(), lr, round(best_acc, 4), best_ep)\n","                    torch.save(net.state_dict(), path_to_model)\n","                    print(\"The model has been saved in {}\".format(path_to_model))\n","\n","    torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["3d8a373f8b1a4da3841d55ed5fd62935","5435b27f9fab4fcd9f0cf0fe1f11db71","b968066f8f924c309860b111ae0816a4","98ff0c5dbe8049c588ef631dfbc08ef8","121075183d2441ddb2e99f849d2e9686","77b7765f0fe04c8a976230fb2eb01065","88b743167cd44b318de6a3998f466556","ef630b6fc27b4750839918ef7ac13c6c","e55762599f664a0fa5288de998af11c1","e3aeb80a52bf4ccdb69330b3ebce4355","7f4f04df6382427288c4e9422e4fbf7d"]},"executionInfo":{"elapsed":1839309,"status":"ok","timestamp":1636039351577,"user":{"displayName":"Andrew Argatkiny","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04422905123959661601"},"user_tz":-420},"id":"tV_3pDzJ81v9","outputId":"5413e47a-47a4-4216-9f14-592c6d734425"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading training data...\n","Reading validation data...\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["cuda:0\n"]}],"source":["from transformers import get_linear_schedule_with_warmup\n","from transformers import get_constant_schedule\n","from sklearn.utils import compute_class_weight\n","#  Set all seeds to make reproducible results\n","set_seed(1)\n","\n","# Creating instances of training and validation set\n","print(\"Reading training data...\")\n","#train_set = FriendsDataset(dataframe=df_train, tokenizer=tokenizer, max_length=MAX_LEN)\n","train_set = FriendsDataset(dataframe=df_full, tokenizer=tokenizer, max_length=MAX_LEN)\n","\n","print(\"Reading validation data...\")\n","val_set = FriendsDataset(dataframe=df_val, tokenizer=tokenizer, max_length=MAX_LEN)\n","# Creating instances of training and validation dataloaders\n","train_loader = DataLoader(train_set, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=2)\n","val_loader = DataLoader(val_set, batch_size=VALID_BATCH_SIZE, shuffle=False, num_workers=2)\n","\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","net = SentencePairClassifier(model=CHECKPOINT, freeze_model=False)\n","print(device)\n","\n","if torch.cuda.device_count() > 1:  # if multiple GPUs\n","    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n","    net = nn.DataParallel(net)\n","\n","net.to(device)\n","\n","\"\"\"class_weights = compute_class_weight(\n","    'balanced', \n","    classes=np.unique(df_train.Phoebe), y=df_train.Phoebe)\n","class_weights = torch.tensor(class_weights, dtype=torch.float)\n","class_weights = class_weights.to(device)\n","criterion = nn.CrossEntropyLoss(weight=class_weights)\"\"\"\n","criterion = nn.CrossEntropyLoss()\n","\n","opti = AdamW(net.parameters(), lr=LEARNING_RATE, weight_decay=1e-2)\n","num_warmup_steps = 0 # The number of steps for the warmup phase.\n","iters_to_accumulate = 2\n","num_training_steps = EPOCHS * len(train_loader)  # The total number of training steps\n","t_total = (len(train_loader) // iters_to_accumulate) * EPOCHS  # Necessary to take into account Gradient accumulation\n","#lr_scheduler = get_linear_schedule_with_warmup(optimizer=opti, num_warmup_steps=num_warmup_steps, num_training_steps=t_total)\n","lr_scheduler = get_constant_schedule(optimizer=opti)\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  3%|▎         | 57/1736 [00:39<19:24,  1.44it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","  4%|▎         | 62/1736 [00:43<19:23,  1.44it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 62/1736 of epoch 1 complete. Loss per last 992 samples:: 0.9178887644121724 \n","Training Accuracy per last 992 samples: 17.842741935483872\n"]},{"name":"stderr","output_type":"stream","text":["  7%|▋         | 124/1736 [01:26<18:56,  1.42it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 124/1736 of epoch 1 complete. Loss per last 992 samples:: 0.9073033486643145 \n","Training Accuracy per last 992 samples: 16.129032258064516\n"]},{"name":"stderr","output_type":"stream","text":[" 11%|█         | 186/1736 [02:09<18:00,  1.43it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 186/1736 of epoch 1 complete. Loss per last 992 samples:: 0.9004467379662299 \n","Training Accuracy per last 992 samples: 20.56451612903226\n"]},{"name":"stderr","output_type":"stream","text":[" 14%|█▍        | 248/1736 [02:53<17:16,  1.44it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 248/1736 of epoch 1 complete. Loss per last 992 samples:: 0.9012170607043851 \n","Training Accuracy per last 992 samples: 19.052419354838708\n"]},{"name":"stderr","output_type":"stream","text":[" 18%|█▊        | 310/1736 [03:36<16:49,  1.41it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 310/1736 of epoch 1 complete. Loss per last 992 samples:: 0.9055953487273185 \n","Training Accuracy per last 992 samples: 17.036290322580644\n"]},{"name":"stderr","output_type":"stream","text":[" 21%|██▏       | 372/1736 [04:19<15:42,  1.45it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 372/1736 of epoch 1 complete. Loss per last 992 samples:: 0.9012470860635081 \n","Training Accuracy per last 992 samples: 18.14516129032258\n"]},{"name":"stderr","output_type":"stream","text":[" 25%|██▌       | 434/1736 [05:03<15:12,  1.43it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 434/1736 of epoch 1 complete. Loss per last 992 samples:: 0.8970258159022177 \n","Training Accuracy per last 992 samples: 19.556451612903224\n"]},{"name":"stderr","output_type":"stream","text":[" 29%|██▊       | 496/1736 [05:46<14:25,  1.43it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 496/1736 of epoch 1 complete. Loss per last 992 samples:: 0.8920076431766633 \n","Training Accuracy per last 992 samples: 19.858870967741936\n"]},{"name":"stderr","output_type":"stream","text":[" 32%|███▏      | 558/1736 [06:30<13:38,  1.44it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 558/1736 of epoch 1 complete. Loss per last 992 samples:: 0.8923851751512096 \n","Training Accuracy per last 992 samples: 18.548387096774192\n"]},{"name":"stderr","output_type":"stream","text":[" 35%|███▍      | 599/1736 [06:58<13:04,  1.45it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 36%|███▌      | 620/1736 [07:13<13:05,  1.42it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 620/1736 of epoch 1 complete. Loss per last 992 samples:: 0.8952678557365171 \n","Training Accuracy per last 992 samples: 19.052419354838708\n"]},{"name":"stderr","output_type":"stream","text":[" 36%|███▌      | 624/1736 [07:16<13:02,  1.42it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 625 complete! Training Loss : 0.90090078125\n","Epoch 1, batch 625 complete! Training Accuracy : 0.186\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 174/174 [00:42<00:00,  4.12it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 625 complete! Validation Loss : 1.7678532744276112\n","Epoch 1, batch 625 complete! Validation Accuracy : 0.2361411087113031\n","Validation loss changed from inf to 1.7678532744276112\n","Best validation accuracy improved from 0 to 0.2361411087113031\n","\n"]},{"name":"stderr","output_type":"stream","text":[" 36%|███▌      | 625/1736 [07:59<4:11:52, 13.60s/it]"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Thu Nov 25 01:41:25 2021_lr_2e-05_val_acc_0.2361_ep_1.pt\n"]},{"name":"stderr","output_type":"stream","text":[" 39%|███▉      | 682/1736 [08:39<11:48,  1.49it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 682/1736 of epoch 1 complete. Loss per last 992 samples:: 0.8871952179939516 \n","Training Accuracy per last 992 samples: 19.95967741935484\n"]},{"name":"stderr","output_type":"stream","text":[" 39%|███▉      | 683/1736 [08:40<11:54,  1.47it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 43%|████▎     | 744/1736 [09:19<10:26,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 744/1736 of epoch 1 complete. Loss per last 992 samples:: 0.8869520618069556 \n","Training Accuracy per last 992 samples: 21.27016129032258\n"]},{"name":"stderr","output_type":"stream","text":[" 46%|████▋     | 806/1736 [10:00<10:13,  1.52it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 806/1736 of epoch 1 complete. Loss per last 992 samples:: 0.8788038684475806 \n","Training Accuracy per last 992 samples: 24.39516129032258\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 868/1736 [10:40<09:48,  1.47it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 868/1736 of epoch 1 complete. Loss per last 992 samples:: 0.8805738879788306 \n","Training Accuracy per last 992 samples: 22.47983870967742\n"]},{"name":"stderr","output_type":"stream","text":[" 54%|█████▎    | 930/1736 [11:20<08:27,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 930/1736 of epoch 1 complete. Loss per last 992 samples:: 0.8839281143680695 \n","Training Accuracy per last 992 samples: 21.975806451612904\n"]},{"name":"stderr","output_type":"stream","text":[" 57%|█████▋    | 992/1736 [11:59<07:52,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 992/1736 of epoch 1 complete. Loss per last 992 samples:: 0.8799002862745716 \n","Training Accuracy per last 992 samples: 22.883064516129032\n"]},{"name":"stderr","output_type":"stream","text":[" 61%|██████    | 1054/1736 [12:38<07:10,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1054/1736 of epoch 1 complete. Loss per last 992 samples:: 0.867401369156376 \n","Training Accuracy per last 992 samples: 23.891129032258064\n"]},{"name":"stderr","output_type":"stream","text":[" 64%|██████▍   | 1116/1736 [13:19<06:30,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1116/1736 of epoch 1 complete. Loss per last 992 samples:: 0.861328371109501 \n","Training Accuracy per last 992 samples: 26.20967741935484\n"]},{"name":"stderr","output_type":"stream","text":[" 68%|██████▊   | 1178/1736 [13:58<05:52,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1178/1736 of epoch 1 complete. Loss per last 992 samples:: 0.8709022768082157 \n","Training Accuracy per last 992 samples: 24.798387096774192\n"]},{"name":"stderr","output_type":"stream","text":[" 71%|███████▏  | 1240/1736 [14:39<05:45,  1.44it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1240/1736 of epoch 1 complete. Loss per last 992 samples:: 0.8571747810609879 \n","Training Accuracy per last 992 samples: 27.318548387096776\n"]},{"name":"stderr","output_type":"stream","text":[" 72%|███████▏  | 1249/1736 [14:45<05:36,  1.45it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 1250 complete! Training Loss : 0.8880179809570312\n","Epoch 1, batch 1250 complete! Training Accuracy : 0.2109\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 174/174 [00:39<00:00,  4.41it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 1250 complete! Validation Loss : 1.6848887828574783\n","Epoch 1, batch 1250 complete! Validation Accuracy : 0.28725701943844495\n","Validation loss changed from 1.7678532744276112 to 1.6848887828574783\n","Best validation accuracy improved from 0.2361411087113031 to 0.28725701943844495\n","\n"]},{"name":"stderr","output_type":"stream","text":[" 72%|███████▏  | 1250/1736 [15:26<1:43:34, 12.79s/it]"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Thu Nov 25 01:48:52 2021_lr_2e-05_val_acc_0.2873_ep_1.pt\n"]},{"name":"stderr","output_type":"stream","text":[" 75%|███████▌  | 1302/1736 [16:00<04:35,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1302/1736 of epoch 1 complete. Loss per last 992 samples:: 0.860778316374748 \n","Training Accuracy per last 992 samples: 27.116935483870968\n"]},{"name":"stderr","output_type":"stream","text":[" 79%|███████▊  | 1364/1736 [16:39<03:54,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1364/1736 of epoch 1 complete. Loss per last 992 samples:: 0.8420511061145414 \n","Training Accuracy per last 992 samples: 28.830645161290324\n"]},{"name":"stderr","output_type":"stream","text":[" 80%|███████▉  | 1385/1736 [16:52<03:39,  1.60it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 82%|████████▏ | 1426/1736 [17:17<03:15,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1426/1736 of epoch 1 complete. Loss per last 992 samples:: 0.8563557286416331 \n","Training Accuracy per last 992 samples: 27.116935483870968\n"]},{"name":"stderr","output_type":"stream","text":[" 86%|████████▌ | 1488/1736 [17:56<02:36,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1488/1736 of epoch 1 complete. Loss per last 992 samples:: 0.8507258507513231 \n","Training Accuracy per last 992 samples: 28.326612903225808\n"]},{"name":"stderr","output_type":"stream","text":[" 89%|████████▉ | 1550/1736 [18:35<01:57,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1550/1736 of epoch 1 complete. Loss per last 992 samples:: 0.8475580523090978 \n","Training Accuracy per last 992 samples: 29.737903225806452\n"]},{"name":"stderr","output_type":"stream","text":[" 93%|█████████▎| 1612/1736 [19:14<01:17,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1612/1736 of epoch 1 complete. Loss per last 992 samples:: 0.8457604685137349 \n","Training Accuracy per last 992 samples: 32.15725806451613\n"]},{"name":"stderr","output_type":"stream","text":[" 96%|█████████▋| 1674/1736 [19:53<00:39,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1674/1736 of epoch 1 complete. Loss per last 992 samples:: 0.831741456062563 \n","Training Accuracy per last 992 samples: 30.342741935483872\n"]},{"name":"stderr","output_type":"stream","text":["100%|█████████▉| 1735/1736 [20:32<00:00,  1.60it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1736/1736 of epoch 1 complete. Loss per last 987 samples:: 0.8162143739961809 \n","Training Accuracy per last 987 samples: 32.21884498480243\n","Epoch 1, batch 1736 complete! Training Loss : 0.875561800866907\n","Epoch 1, batch 1736 complete! Training Accuracy : 0.23456123294083756\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 174/174 [00:38<00:00,  4.56it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 1736 complete! Validation Loss : 1.5834934279836457\n","Epoch 1, batch 1736 complete! Validation Accuracy : 0.3624910007199424\n","Validation loss changed from 1.6848887828574783 to 1.5834934279836457\n","Best validation accuracy improved from 0.28725701943844495 to 0.3624910007199424\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1736/1736 [21:11<00:00,  1.37it/s]\n"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Thu Nov 25 01:54:36 2021_lr_2e-05_val_acc_0.3625_ep_1.pt\n"]},{"name":"stderr","output_type":"stream","text":["  4%|▎         | 62/1736 [00:39<17:34,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 62/1736 of epoch 2 complete. Loss per last 992 samples:: 0.8000272935436618 \n","Training Accuracy per last 992 samples: 34.274193548387096\n"]},{"name":"stderr","output_type":"stream","text":["  7%|▋         | 124/1736 [01:17<16:59,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 124/1736 of epoch 2 complete. Loss per last 992 samples:: 0.7992842274327432 \n","Training Accuracy per last 992 samples: 36.59274193548387\n"]},{"name":"stderr","output_type":"stream","text":[" 11%|█         | 186/1736 [01:56<16:15,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 186/1736 of epoch 2 complete. Loss per last 992 samples:: 0.7878429658951298 \n","Training Accuracy per last 992 samples: 34.778225806451616\n"]},{"name":"stderr","output_type":"stream","text":[" 14%|█▍        | 248/1736 [02:35<15:35,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 248/1736 of epoch 2 complete. Loss per last 992 samples:: 0.8054039862848097 \n","Training Accuracy per last 992 samples: 34.57661290322581\n"]},{"name":"stderr","output_type":"stream","text":[" 18%|█▊        | 310/1736 [03:14<14:56,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 310/1736 of epoch 2 complete. Loss per last 992 samples:: 0.7902487477948589 \n","Training Accuracy per last 992 samples: 34.57661290322581\n"]},{"name":"stderr","output_type":"stream","text":[" 18%|█▊        | 321/1736 [03:21<14:45,  1.60it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 21%|██        | 368/1736 [03:51<14:21,  1.59it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 21%|██▏       | 372/1736 [03:53<14:20,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 372/1736 of epoch 2 complete. Loss per last 992 samples:: 0.8074448493219191 \n","Training Accuracy per last 992 samples: 35.28225806451613\n"]},{"name":"stderr","output_type":"stream","text":[" 25%|██▌       | 434/1736 [04:32<13:39,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 434/1736 of epoch 2 complete. Loss per last 992 samples:: 0.7980702307916456 \n","Training Accuracy per last 992 samples: 36.189516129032256\n"]},{"name":"stderr","output_type":"stream","text":[" 28%|██▊       | 478/1736 [05:00<13:12,  1.59it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 29%|██▊       | 496/1736 [05:12<13:21,  1.55it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 496/1736 of epoch 2 complete. Loss per last 992 samples:: 0.7881771825974987 \n","Training Accuracy per last 992 samples: 34.475806451612904\n"]},{"name":"stderr","output_type":"stream","text":[" 32%|███▏      | 558/1736 [05:51<12:21,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 558/1736 of epoch 2 complete. Loss per last 992 samples:: 0.7779814812444872 \n","Training Accuracy per last 992 samples: 36.895161290322584\n"]},{"name":"stderr","output_type":"stream","text":[" 36%|███▌      | 620/1736 [06:30<11:42,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 620/1736 of epoch 2 complete. Loss per last 992 samples:: 0.7796370290940807 \n","Training Accuracy per last 992 samples: 36.49193548387097\n"]},{"name":"stderr","output_type":"stream","text":[" 36%|███▌      | 624/1736 [06:32<11:38,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, batch 625 complete! Training Loss : 0.7937084930419922\n","Epoch 2, batch 625 complete! Training Accuracy : 0.3538\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 174/174 [00:38<00:00,  4.57it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, batch 625 complete! Validation Loss : 1.4725390816556996\n","Epoch 2, batch 625 complete! Validation Accuracy : 0.4272858171346292\n","Validation loss changed from 1.5834934279836457 to 1.4725390816556996\n","Best validation accuracy improved from 0.3624910007199424 to 0.4272858171346292\n","\n"]},{"name":"stderr","output_type":"stream","text":[" 36%|███▌      | 625/1736 [07:12<3:47:23, 12.28s/it]"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Thu Nov 25 02:01:49 2021_lr_2e-05_val_acc_0.4273_ep_2.pt\n"]},{"name":"stderr","output_type":"stream","text":[" 39%|███▉      | 682/1736 [07:48<11:03,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 682/1736 of epoch 2 complete. Loss per last 992 samples:: 0.7766850379205519 \n","Training Accuracy per last 992 samples: 36.99596774193548\n"]},{"name":"stderr","output_type":"stream","text":[" 43%|████▎     | 744/1736 [08:27<10:25,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 744/1736 of epoch 2 complete. Loss per last 992 samples:: 0.7861016796481225 \n","Training Accuracy per last 992 samples: 34.87903225806452\n"]},{"name":"stderr","output_type":"stream","text":[" 46%|████▋     | 806/1736 [09:06<10:11,  1.52it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 806/1736 of epoch 2 complete. Loss per last 992 samples:: 0.7747120395783456 \n","Training Accuracy per last 992 samples: 38.104838709677416\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 868/1736 [09:45<09:07,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 868/1736 of epoch 2 complete. Loss per last 992 samples:: 0.7909242568477508 \n","Training Accuracy per last 992 samples: 34.67741935483871\n"]},{"name":"stderr","output_type":"stream","text":[" 54%|█████▎    | 930/1736 [10:24<08:27,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 930/1736 of epoch 2 complete. Loss per last 992 samples:: 0.7803074313748267 \n","Training Accuracy per last 992 samples: 36.895161290322584\n"]},{"name":"stderr","output_type":"stream","text":[" 57%|█████▋    | 992/1736 [11:03<07:48,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 992/1736 of epoch 2 complete. Loss per last 992 samples:: 0.7687013995262885 \n","Training Accuracy per last 992 samples: 38.70967741935484\n"]},{"name":"stderr","output_type":"stream","text":[" 61%|██████    | 1054/1736 [11:42<07:09,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1054/1736 of epoch 2 complete. Loss per last 992 samples:: 0.7762295199978736 \n","Training Accuracy per last 992 samples: 36.49193548387097\n"]},{"name":"stderr","output_type":"stream","text":[" 64%|██████▍   | 1116/1736 [12:21<06:30,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1116/1736 of epoch 2 complete. Loss per last 992 samples:: 0.7791134926580614 \n","Training Accuracy per last 992 samples: 37.903225806451616\n"]},{"name":"stderr","output_type":"stream","text":[" 68%|██████▊   | 1178/1736 [13:00<05:51,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1178/1736 of epoch 2 complete. Loss per last 992 samples:: 0.7670601875551285 \n","Training Accuracy per last 992 samples: 39.11290322580645\n"]},{"name":"stderr","output_type":"stream","text":[" 71%|███████▏  | 1240/1736 [13:39<05:12,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1240/1736 of epoch 2 complete. Loss per last 992 samples:: 0.7768972766014838 \n","Training Accuracy per last 992 samples: 37.903225806451616\n"]},{"name":"stderr","output_type":"stream","text":[" 72%|███████▏  | 1249/1736 [13:45<05:36,  1.45it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, batch 1250 complete! Training Loss : 0.7850242980957032\n","Epoch 2, batch 1250 complete! Training Accuracy : 0.3634\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 174/174 [00:39<00:00,  4.41it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, batch 1250 complete! Validation Loss : 1.3773712159573346\n","Epoch 2, batch 1250 complete! Validation Accuracy : 0.46184305255579555\n","Validation loss changed from 1.4725390816556996 to 1.3773712159573346\n","Best validation accuracy improved from 0.4272858171346292 to 0.46184305255579555\n","\n"]},{"name":"stderr","output_type":"stream","text":[" 72%|███████▏  | 1250/1736 [14:26<1:43:25, 12.77s/it]"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Thu Nov 25 02:09:03 2021_lr_2e-05_val_acc_0.4618_ep_2.pt\n"]},{"name":"stderr","output_type":"stream","text":[" 75%|███████▌  | 1302/1736 [15:01<04:33,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1302/1736 of epoch 2 complete. Loss per last 992 samples:: 0.748767206745763 \n","Training Accuracy per last 992 samples: 40.32258064516129\n"]},{"name":"stderr","output_type":"stream","text":[" 79%|███████▊  | 1364/1736 [15:41<04:09,  1.49it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1364/1736 of epoch 2 complete. Loss per last 992 samples:: 0.7647725382158833 \n","Training Accuracy per last 992 samples: 36.189516129032256\n"]},{"name":"stderr","output_type":"stream","text":[" 82%|████████▏ | 1426/1736 [16:22<03:26,  1.50it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1426/1736 of epoch 2 complete. Loss per last 992 samples:: 0.7705930586784117 \n","Training Accuracy per last 992 samples: 37.5\n"]},{"name":"stderr","output_type":"stream","text":[" 86%|████████▌ | 1488/1736 [17:02<02:38,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1488/1736 of epoch 2 complete. Loss per last 992 samples:: 0.7679192327683971 \n","Training Accuracy per last 992 samples: 37.096774193548384\n"]},{"name":"stderr","output_type":"stream","text":[" 86%|████████▌ | 1495/1736 [17:06<02:33,  1.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 89%|████████▉ | 1550/1736 [17:41<01:58,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1550/1736 of epoch 2 complete. Loss per last 992 samples:: 0.7555833631946195 \n","Training Accuracy per last 992 samples: 39.11290322580645\n"]},{"name":"stderr","output_type":"stream","text":[" 93%|█████████▎| 1612/1736 [18:21<01:19,  1.55it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1612/1736 of epoch 2 complete. Loss per last 992 samples:: 0.7504806826191563 \n","Training Accuracy per last 992 samples: 39.818548387096776\n"]},{"name":"stderr","output_type":"stream","text":[" 96%|█████████▋| 1674/1736 [19:01<00:39,  1.56it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1674/1736 of epoch 2 complete. Loss per last 992 samples:: 0.7405143860847719 \n","Training Accuracy per last 992 samples: 39.516129032258064\n"]},{"name":"stderr","output_type":"stream","text":["100%|█████████▉| 1735/1736 [19:40<00:00,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1736/1736 of epoch 2 complete. Loss per last 987 samples:: 0.7672035963304581 \n","Training Accuracy per last 987 samples: 38.90577507598784\n","Epoch 2, batch 1736 complete! Training Loss : 0.777738727869526\n","Epoch 2, batch 1736 complete! Training Accuracy : 0.36937812826329625\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 174/174 [00:38<00:00,  4.55it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, batch 1736 complete! Validation Loss : 1.3173391428487053\n","Epoch 2, batch 1736 complete! Validation Accuracy : 0.49388048956083513\n","Validation loss changed from 1.3773712159573346 to 1.3173391428487053\n","Best validation accuracy improved from 0.46184305255579555 to 0.49388048956083513\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1736/1736 [20:20<00:00,  1.42it/s]\n"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Thu Nov 25 02:14:56 2021_lr_2e-05_val_acc_0.4939_ep_2.pt\n"]},{"name":"stderr","output_type":"stream","text":["  4%|▎         | 62/1736 [00:39<17:36,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 62/1736 of epoch 3 complete. Loss per last 992 samples:: 0.6879654699756254 \n","Training Accuracy per last 992 samples: 45.96774193548387\n"]},{"name":"stderr","output_type":"stream","text":["  6%|▋         | 110/1736 [01:09<17:07,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","  7%|▋         | 124/1736 [01:18<17:00,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 124/1736 of epoch 3 complete. Loss per last 992 samples:: 0.6506985695131363 \n","Training Accuracy per last 992 samples: 49.899193548387096\n"]},{"name":"stderr","output_type":"stream","text":[" 11%|█         | 186/1736 [01:57<16:23,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 186/1736 of epoch 3 complete. Loss per last 992 samples:: 0.6641638817325715 \n","Training Accuracy per last 992 samples: 49.193548387096776\n"]},{"name":"stderr","output_type":"stream","text":[" 14%|█▍        | 248/1736 [02:36<15:39,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 248/1736 of epoch 3 complete. Loss per last 992 samples:: 0.6668615956460276 \n","Training Accuracy per last 992 samples: 48.185483870967744\n"]},{"name":"stderr","output_type":"stream","text":[" 18%|█▊        | 310/1736 [03:15<15:06,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 310/1736 of epoch 3 complete. Loss per last 992 samples:: 0.6688145668275894 \n","Training Accuracy per last 992 samples: 48.891129032258064\n"]},{"name":"stderr","output_type":"stream","text":[" 21%|██▏       | 372/1736 [03:55<14:22,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 372/1736 of epoch 3 complete. Loss per last 992 samples:: 0.6854231203756025 \n","Training Accuracy per last 992 samples: 47.17741935483871\n"]},{"name":"stderr","output_type":"stream","text":[" 25%|██▍       | 428/1736 [04:31<13:52,  1.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 25%|██▌       | 434/1736 [04:34<13:47,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 434/1736 of epoch 3 complete. Loss per last 992 samples:: 0.682131951855075 \n","Training Accuracy per last 992 samples: 47.58064516129032\n"]},{"name":"stderr","output_type":"stream","text":[" 29%|██▊       | 496/1736 [05:14<13:04,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 496/1736 of epoch 3 complete. Loss per last 992 samples:: 0.6876486809022965 \n","Training Accuracy per last 992 samples: 47.07661290322581\n"]},{"name":"stderr","output_type":"stream","text":[" 32%|███▏      | 558/1736 [05:53<12:26,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 558/1736 of epoch 3 complete. Loss per last 992 samples:: 0.6989043297306183 \n","Training Accuracy per last 992 samples: 44.354838709677416\n"]},{"name":"stderr","output_type":"stream","text":[" 36%|███▌      | 620/1736 [06:32<11:46,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 620/1736 of epoch 3 complete. Loss per last 992 samples:: 0.6689984875340615 \n","Training Accuracy per last 992 samples: 48.48790322580645\n"]},{"name":"stderr","output_type":"stream","text":[" 36%|███▌      | 624/1736 [06:35<11:47,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 3, batch 625 complete! Training Loss : 0.675451163482666\n","Epoch 3, batch 625 complete! Training Accuracy : 0.4777\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 174/174 [00:38<00:00,  4.55it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3, batch 625 complete! Validation Loss : 1.1724395519015434\n","Epoch 3, batch 625 complete! Validation Accuracy : 0.5521958243340532\n","Validation loss changed from 1.3173391428487053 to 1.1724395519015434\n","Best validation accuracy improved from 0.49388048956083513 to 0.5521958243340532\n","\n"]},{"name":"stderr","output_type":"stream","text":[" 36%|███▌      | 625/1736 [07:14<3:48:22, 12.33s/it]"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Thu Nov 25 02:22:11 2021_lr_2e-05_val_acc_0.5522_ep_3.pt\n"]},{"name":"stderr","output_type":"stream","text":[" 39%|███▊      | 670/1736 [07:43<11:36,  1.53it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 39%|███▉      | 682/1736 [07:51<11:20,  1.55it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 682/1736 of epoch 3 complete. Loss per last 992 samples:: 0.6605496022009081 \n","Training Accuracy per last 992 samples: 47.278225806451616\n"]},{"name":"stderr","output_type":"stream","text":[" 43%|████▎     | 744/1736 [08:31<10:27,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 744/1736 of epoch 3 complete. Loss per last 992 samples:: 0.6772006942379859 \n","Training Accuracy per last 992 samples: 48.185483870967744\n"]},{"name":"stderr","output_type":"stream","text":[" 46%|████▋     | 806/1736 [09:10<09:46,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 806/1736 of epoch 3 complete. Loss per last 992 samples:: 0.6913964363836473 \n","Training Accuracy per last 992 samples: 45.060483870967744\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 868/1736 [09:49<09:07,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 868/1736 of epoch 3 complete. Loss per last 992 samples:: 0.6821748825811571 \n","Training Accuracy per last 992 samples: 47.681451612903224\n"]},{"name":"stderr","output_type":"stream","text":[" 54%|█████▎    | 930/1736 [10:28<08:27,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 930/1736 of epoch 3 complete. Loss per last 992 samples:: 0.6853434962611045 \n","Training Accuracy per last 992 samples: 45.96774193548387\n"]},{"name":"stderr","output_type":"stream","text":[" 56%|█████▌    | 976/1736 [10:56<07:58,  1.59it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 57%|█████▋    | 992/1736 [11:06<07:48,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 992/1736 of epoch 3 complete. Loss per last 992 samples:: 0.675171475256643 \n","Training Accuracy per last 992 samples: 45.766129032258064\n"]},{"name":"stderr","output_type":"stream","text":[" 61%|██████    | 1054/1736 [11:47<07:11,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1054/1736 of epoch 3 complete. Loss per last 992 samples:: 0.6856210923963978 \n","Training Accuracy per last 992 samples: 45.66532258064516\n"]},{"name":"stderr","output_type":"stream","text":[" 64%|██████▍   | 1116/1736 [12:27<06:31,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1116/1736 of epoch 3 complete. Loss per last 992 samples:: 0.6488228228784376 \n","Training Accuracy per last 992 samples: 49.29435483870968\n"]},{"name":"stderr","output_type":"stream","text":[" 68%|██████▊   | 1178/1736 [13:06<05:52,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1178/1736 of epoch 3 complete. Loss per last 992 samples:: 0.6858382071218183 \n","Training Accuracy per last 992 samples: 45.96774193548387\n"]},{"name":"stderr","output_type":"stream","text":[" 71%|███████▏  | 1240/1736 [13:45<05:13,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1240/1736 of epoch 3 complete. Loss per last 992 samples:: 0.7008220764898485 \n","Training Accuracy per last 992 samples: 45.26209677419355\n"]},{"name":"stderr","output_type":"stream","text":[" 72%|███████▏  | 1249/1736 [13:51<05:06,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 3, batch 1250 complete! Training Loss : 0.6775103603363037\n","Epoch 3, batch 1250 complete! Training Accuracy : 0.4718\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 174/174 [00:38<00:00,  4.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3, batch 1250 complete! Validation Loss : 1.0959558904856102\n","Epoch 3, batch 1250 complete! Validation Accuracy : 0.6069114470842333\n","Validation loss changed from 1.1724395519015434 to 1.0959558904856102\n","Best validation accuracy improved from 0.5521958243340532 to 0.6069114470842333\n","\n"]},{"name":"stderr","output_type":"stream","text":[" 72%|███████▏  | 1250/1736 [14:31<1:40:46, 12.44s/it]"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Thu Nov 25 02:29:28 2021_lr_2e-05_val_acc_0.6069_ep_3.pt\n"]},{"name":"stderr","output_type":"stream","text":[" 75%|███████▌  | 1302/1736 [15:04<04:34,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1302/1736 of epoch 3 complete. Loss per last 992 samples:: 0.6675734519958496 \n","Training Accuracy per last 992 samples: 46.774193548387096\n"]},{"name":"stderr","output_type":"stream","text":[" 79%|███████▊  | 1364/1736 [15:43<03:55,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1364/1736 of epoch 3 complete. Loss per last 992 samples:: 0.6623791263949487 \n","Training Accuracy per last 992 samples: 49.69758064516129\n"]},{"name":"stderr","output_type":"stream","text":[" 82%|████████▏ | 1426/1736 [16:23<03:16,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1426/1736 of epoch 3 complete. Loss per last 992 samples:: 0.6969256939426545 \n","Training Accuracy per last 992 samples: 45.86693548387097\n"]},{"name":"stderr","output_type":"stream","text":[" 86%|████████▌ | 1488/1736 [17:02<02:36,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1488/1736 of epoch 3 complete. Loss per last 992 samples:: 0.6636506588228287 \n","Training Accuracy per last 992 samples: 47.58064516129032\n"]},{"name":"stderr","output_type":"stream","text":[" 89%|████████▉ | 1550/1736 [17:41<01:57,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1550/1736 of epoch 3 complete. Loss per last 992 samples:: 0.6677527273854902 \n","Training Accuracy per last 992 samples: 46.471774193548384\n"]},{"name":"stderr","output_type":"stream","text":[" 93%|█████████▎| 1612/1736 [18:20<01:19,  1.56it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1612/1736 of epoch 3 complete. Loss per last 992 samples:: 0.6687367346978956 \n","Training Accuracy per last 992 samples: 45.96774193548387\n"]},{"name":"stderr","output_type":"stream","text":[" 96%|█████████▋| 1674/1736 [19:00<00:39,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1674/1736 of epoch 3 complete. Loss per last 992 samples:: 0.6845279585930609 \n","Training Accuracy per last 992 samples: 46.471774193548384\n"]},{"name":"stderr","output_type":"stream","text":["100%|█████████▉| 1735/1736 [19:38<00:00,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1736/1736 of epoch 3 complete. Loss per last 987 samples:: 0.667423673214451 \n","Training Accuracy per last 987 samples: 48.02431610942249\n","Epoch 3, batch 1736 complete! Training Loss : 0.6761971951767047\n","Epoch 3, batch 1736 complete! Training Accuracy : 0.4713550106225919\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 174/174 [00:38<00:00,  4.54it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3, batch 1736 complete! Validation Loss : 0.9934600447786266\n","Epoch 3, batch 1736 complete! Validation Accuracy : 0.6587473002159827\n","Validation loss changed from 1.0959558904856102 to 0.9934600447786266\n","Best validation accuracy improved from 0.6069114470842333 to 0.6587473002159827\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1736/1736 [20:18<00:00,  1.43it/s]"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Thu Nov 25 02:35:15 2021_lr_2e-05_val_acc_0.6587_ep_3.pt\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Training on all availible data\n","train_bert(net, criterion, opti, LEARNING_RATE, lr_scheduler, train_loader, val_loader, EPOCHS, iters_to_accumulate)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["torch.save(net.state_dict(), 'models/orig_final_model.pt')"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["#net.load_state_dict(torch.load('models/orig_final_model.pt'))"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 193/193 [00:46<00:00,  4.18it/s]\n"]}],"source":["test_set = FriendsDataset(dataframe=df_test, tokenizer=tokenizer, max_length=MAX_LEN)\n","test_loader = DataLoader(test_set, batch_size=VALID_BATCH_SIZE, shuffle=False, num_workers=1)\n","def predict(net, device, dataloader):\n","    net.eval()\n","    predictions = []\n","\n","    with torch.no_grad():\n","        for it, (seq, attn_masks, token_type_ids) in enumerate(tqdm(dataloader)):\n","            seq, attn_masks, token_type_ids = \\\n","                seq.to(device), attn_masks.to(device), token_type_ids.to(device)\n","            logits = net(seq, attn_masks, token_type_ids)\n","            max_logits, argmax_idx = torch.max(logits.data, dim=1)\n","            predictions.extend(argmax_idx.tolist())\n","    del logits\n","    return predictions\n","preds = predict(net, device, test_loader)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Id</th>\n","      <th>other_speaker</th>\n","      <th>friend_response</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>But I'm unemployed, my music is all I really h...</td>\n","      <td>All right, I'm gonna do it! I'm gonna get shot...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Check this out. Five hundred and seventeen boxes!</td>\n","      <td>Oh my God, how did you do that?</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>Okay. Okay. Would, would it help if I went ove...</td>\n","      <td>Yeah! Yeah! That would be very helpful! Yeah.</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>Ross, what is taking you so long?</td>\n","      <td>I'm sorry, it's almost as if this wasn't built...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>So who are you?</td>\n","      <td>Well, our names really are Monica and Chandler...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3081</th>\n","      <td>3081</td>\n","      <td>3081</td>\n","      <td>It's not just the drum noise. Every five minut...</td>\n","      <td>Yes, thank you. You see, this is how normal pe...</td>\n","    </tr>\n","    <tr>\n","      <th>3082</th>\n","      <td>3082</td>\n","      <td>3082</td>\n","      <td>I think I accidentally used Monica’s boxes to ...</td>\n","      <td>Oh no. Dad! Dad! What. Oh God everything’s rui...</td>\n","    </tr>\n","    <tr>\n","      <th>3083</th>\n","      <td>3083</td>\n","      <td>3083</td>\n","      <td>so y'know, that’s why, within a few years, tha...</td>\n","      <td>Oh, this is so great.</td>\n","    </tr>\n","    <tr>\n","      <th>3084</th>\n","      <td>3084</td>\n","      <td>3084</td>\n","      <td>He slept with you and then never called you.</td>\n","      <td>And I just wanted a new daddy for Davy and Becky.</td>\n","    </tr>\n","    <tr>\n","      <th>3085</th>\n","      <td>3085</td>\n","      <td>3085</td>\n","      <td>I know, isn't he great? It's so nice to finall...</td>\n","      <td>Well maybe he'll get to go soon, like on a cla...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3086 rows × 4 columns</p>\n","</div>"],"text/plain":["      Unnamed: 0    Id                                      other_speaker  \\\n","0              0     0  But I'm unemployed, my music is all I really h...   \n","1              1     1  Check this out. Five hundred and seventeen boxes!   \n","2              2     2  Okay. Okay. Would, would it help if I went ove...   \n","3              3     3                  Ross, what is taking you so long?   \n","4              4     4                                    So who are you?   \n","...          ...   ...                                                ...   \n","3081        3081  3081  It's not just the drum noise. Every five minut...   \n","3082        3082  3082  I think I accidentally used Monica’s boxes to ...   \n","3083        3083  3083  so y'know, that’s why, within a few years, tha...   \n","3084        3084  3084       He slept with you and then never called you.   \n","3085        3085  3085  I know, isn't he great? It's so nice to finall...   \n","\n","                                        friend_response  \n","0     All right, I'm gonna do it! I'm gonna get shot...  \n","1                       Oh my God, how did you do that?  \n","2         Yeah! Yeah! That would be very helpful! Yeah.  \n","3     I'm sorry, it's almost as if this wasn't built...  \n","4     Well, our names really are Monica and Chandler...  \n","...                                                 ...  \n","3081  Yes, thank you. You see, this is how normal pe...  \n","3082  Oh no. Dad! Dad! What. Oh God everything’s rui...  \n","3083                              Oh, this is so great.  \n","3084  And I just wanted a new daddy for Davy and Becky.  \n","3085  Well maybe he'll get to go soon, like on a cla...  \n","\n","[3086 rows x 4 columns]"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["df_test"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>other_speaker</th>\n","      <th>friend_response</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>Но я безработный, моя музыка - это все, что у ...</td>\n","      <td>Меня застрелят. Любой совет?</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>Посмотри. Пятьсот семнадцать коробок!</td>\n","      <td>Боже мой, как ты это сделал?</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>Хорошо. Хорошо. Помогло бы, если бы я подошел ...</td>\n","      <td>Это было бы очень полезно!</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>Росс, чего ты так долго?</td>\n","      <td>Простите, это как будто не для быстрого отдыха!</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>Так кто ты?</td>\n","      <td>Ну, наши имена действительно Моника и Чендлер....</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3081</th>\n","      <td>3081</td>\n","      <td>Дело не только в барабанах. Каждые пять минут ...</td>\n","      <td>Понимаете, именно так нормальные люди должны р...</td>\n","    </tr>\n","    <tr>\n","      <th>3082</th>\n","      <td>3082</td>\n","      <td>Кажется, я случайно использовал коробки Моники...</td>\n","      <td>Боже, все испорчено! Папа, она будет раздавлена!</td>\n","    </tr>\n","    <tr>\n","      <th>3083</th>\n","      <td>3083</td>\n","      <td>ну знаете, вот почему через несколько лет расп...</td>\n","      <td>Ой, это так здорово.</td>\n","    </tr>\n","    <tr>\n","      <th>3084</th>\n","      <td>3084</td>\n","      <td>Он переспал с тобой, а потом никогда тебе не з...</td>\n","      <td>А я просто хотела нового папу для Дэви и Бекки.</td>\n","    </tr>\n","    <tr>\n","      <th>3085</th>\n","      <td>3085</td>\n","      <td>Я знаю, разве он не классный? Так приятно нако...</td>\n","      <td>Ну, может он скоро уедет, как в классную поезд...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3086 rows × 3 columns</p>\n","</div>"],"text/plain":["        Id                                      other_speaker  \\\n","0        0  Но я безработный, моя музыка - это все, что у ...   \n","1        1              Посмотри. Пятьсот семнадцать коробок!   \n","2        2  Хорошо. Хорошо. Помогло бы, если бы я подошел ...   \n","3        3                           Росс, чего ты так долго?   \n","4        4                                        Так кто ты?   \n","...    ...                                                ...   \n","3081  3081  Дело не только в барабанах. Каждые пять минут ...   \n","3082  3082  Кажется, я случайно использовал коробки Моники...   \n","3083  3083  ну знаете, вот почему через несколько лет расп...   \n","3084  3084  Он переспал с тобой, а потом никогда тебе не з...   \n","3085  3085  Я знаю, разве он не классный? Так приятно нако...   \n","\n","                                        friend_response  \n","0                          Меня застрелят. Любой совет?  \n","1                          Боже мой, как ты это сделал?  \n","2                            Это было бы очень полезно!  \n","3       Простите, это как будто не для быстрого отдыха!  \n","4     Ну, наши имена действительно Моника и Чендлер....  \n","...                                                 ...  \n","3081  Понимаете, именно так нормальные люди должны р...  \n","3082   Боже, все испорчено! Папа, она будет раздавлена!  \n","3083                               Ой, это так здорово.  \n","3084    А я просто хотела нового папу для Дэви и Бекки.  \n","3085  Ну, может он скоро уедет, как в классную поезд...  \n","\n","[3086 rows x 3 columns]"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["df_test"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Category</th>\n","    </tr>\n","    <tr>\n","      <th>Id</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ДЖОУИ</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>МОНИКА</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>РЕЙЧЕЛ</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>РОСС</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>МОНИКА</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3081</th>\n","      <td>ЧЕНДЛЕР</td>\n","    </tr>\n","    <tr>\n","      <th>3082</th>\n","      <td>РОСС</td>\n","    </tr>\n","    <tr>\n","      <th>3083</th>\n","      <td>РЕЙЧЕЛ</td>\n","    </tr>\n","    <tr>\n","      <th>3084</th>\n","      <td>ЧЕНДЛЕР</td>\n","    </tr>\n","    <tr>\n","      <th>3085</th>\n","      <td>ФИБИ</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3086 rows × 1 columns</p>\n","</div>"],"text/plain":["     Category\n","Id           \n","0       ДЖОУИ\n","1      МОНИКА\n","2      РЕЙЧЕЛ\n","3        РОСС\n","4      МОНИКА\n","...       ...\n","3081  ЧЕНДЛЕР\n","3082     РОСС\n","3083   РЕЙЧЕЛ\n","3084  ЧЕНДЛЕР\n","3085     ФИБИ\n","\n","[3086 rows x 1 columns]"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["answers = pd.DataFrame(\n","    names_to_cats.inverse_transform(preds), \n","    index=df_test.Id, columns=[\"Category\"])\n","answers.to_csv('submission1_orig.csv')\n","answers"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Category</th>\n","    </tr>\n","    <tr>\n","      <th>Id</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ФИБИ</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>МОНИКА</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>РЕЙЧЕЛ</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>РОСС</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ЧЕНДЛЕР</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3081</th>\n","      <td>ФИБИ</td>\n","    </tr>\n","    <tr>\n","      <th>3082</th>\n","      <td>МОНИКА</td>\n","    </tr>\n","    <tr>\n","      <th>3083</th>\n","      <td>РЕЙЧЕЛ</td>\n","    </tr>\n","    <tr>\n","      <th>3084</th>\n","      <td>МОНИКА</td>\n","    </tr>\n","    <tr>\n","      <th>3085</th>\n","      <td>РЕЙЧЕЛ</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3086 rows × 1 columns</p>\n","</div>"],"text/plain":["     Category\n","Id           \n","0        ФИБИ\n","1      МОНИКА\n","2      РЕЙЧЕЛ\n","3        РОСС\n","4     ЧЕНДЛЕР\n","...       ...\n","3081     ФИБИ\n","3082   МОНИКА\n","3083   РЕЙЧЕЛ\n","3084   МОНИКА\n","3085   РЕЙЧЕЛ\n","\n","[3086 rows x 1 columns]"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["answers = pd.DataFrame(\n","    names_to_cats.inverse_transform(preds), \n","    index=df_test.Id, columns=[\"Category\"])\n","answers.to_csv('submission1_orig.csv')\n","answers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bxhl6QhJlbfE"},"outputs":[],"source":["from transformers import get_constant_schedule\n","opti = AdamW(net.parameters(), lr=LEARNING_RATE, weight_decay=1e-2)\n","lr_scheduler = get_constant_schedule(optimizer=opti)\n","train_set = FriendsDataset(dataframe=df_train.iloc[5000:], tokenizer=tokenizer, max_length=MAX_LEN)\n","\n","val_set = FriendsDataset(dataframe=df_val, tokenizer=tokenizer, max_length=MAX_LEN)\n","# Creating instances of training and validation dataloaders\n","train_loader = DataLoader(train_set, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=2)\n","val_loader = DataLoader(val_set, batch_size=VALID_BATCH_SIZE, shuffle=False, num_workers=2)\n","\n","train_bert(net, criterion, opti, LEARNING_RATE, lr_scheduler, train_loader, val_loader, EPOCHS, iters_to_accumulate)"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":268,"status":"ok","timestamp":1636042247525,"user":{"displayName":"Andrew Argatkiny","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04422905123959661601"},"user_tz":-420},"id":"BnAB6Udp9Z-1"},"outputs":[],"source":["import gc\n","gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":448,"status":"ok","timestamp":1636024184045,"user":{"displayName":"Andrew Argatkiny","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04422905123959661601"},"user_tz":-420},"id":"VEInewq0pYTk","outputId":"f8944075-b0f8-41a9-8378-f5fc6d1f90fb"},"outputs":[{"data":{"text/plain":["8506.769408"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["torch.cuda.get_device_properties(0).total_memory / 1e6"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  4%|▍         | 62/1563 [00:43<17:32,  1.43it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 62/1563 of epoch 1 complete. Loss per last 992 samples:: 0.913238279281124 \n","Training Accuracy per last 992 samples: 19.556451612903224\n"]},{"name":"stderr","output_type":"stream","text":["  8%|▊         | 124/1563 [01:25<16:41,  1.44it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 124/1563 of epoch 1 complete. Loss per last 992 samples:: 0.9004580590032762 \n","Training Accuracy per last 992 samples: 17.641129032258064\n"]},{"name":"stderr","output_type":"stream","text":[" 12%|█▏        | 186/1563 [02:09<16:10,  1.42it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 186/1563 of epoch 1 complete. Loss per last 992 samples:: 0.899505615234375 \n","Training Accuracy per last 992 samples: 17.338709677419356\n"]},{"name":"stderr","output_type":"stream","text":[" 16%|█▌        | 248/1563 [02:51<15:13,  1.44it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 248/1563 of epoch 1 complete. Loss per last 992 samples:: 0.9008006434286794 \n","Training Accuracy per last 992 samples: 18.75\n"]},{"name":"stderr","output_type":"stream","text":[" 16%|█▋        | 256/1563 [02:57<14:23,  1.51it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 19%|█▉        | 301/1563 [03:25<13:10,  1.60it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 20%|█▉        | 310/1563 [03:31<13:11,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 310/1563 of epoch 1 complete. Loss per last 992 samples:: 0.8999879898563508 \n","Training Accuracy per last 992 samples: 16.93548387096774\n"]},{"name":"stderr","output_type":"stream","text":[" 24%|██▍       | 372/1563 [04:10<12:32,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 372/1563 of epoch 1 complete. Loss per last 992 samples:: 0.8955442367061492 \n","Training Accuracy per last 992 samples: 19.35483870967742\n"]},{"name":"stderr","output_type":"stream","text":[" 28%|██▊       | 434/1563 [04:49<11:53,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 434/1563 of epoch 1 complete. Loss per last 992 samples:: 0.8951455393145161 \n","Training Accuracy per last 992 samples: 18.548387096774192\n"]},{"name":"stderr","output_type":"stream","text":[" 32%|███▏      | 496/1563 [05:28<11:12,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 496/1563 of epoch 1 complete. Loss per last 992 samples:: 0.895750476467994 \n","Training Accuracy per last 992 samples: 19.455645161290324\n"]},{"name":"stderr","output_type":"stream","text":[" 36%|███▌      | 558/1563 [06:07<10:34,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 558/1563 of epoch 1 complete. Loss per last 992 samples:: 0.8903459118258569 \n","Training Accuracy per last 992 samples: 20.866935483870968\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|███▉      | 620/1563 [06:46<09:55,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 620/1563 of epoch 1 complete. Loss per last 992 samples:: 0.8890636813256049 \n","Training Accuracy per last 992 samples: 21.370967741935484\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|███▉      | 624/1563 [06:48<09:54,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 625 complete! Training Loss : 0.8979001708984375\n","Epoch 1, batch 625 complete! Training Accuracy : 0.1898\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 174/174 [00:38<00:00,  4.56it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 625 complete! Validation Loss : 1.7738734456314438\n","Epoch 1, batch 625 complete! Validation Accuracy : 0.21526277897768178\n","Validation loss changed from inf to 1.7738734456314438\n","Best validation accuracy improved from 0 to 0.21526277897768178\n","\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|███▉      | 625/1563 [07:28<3:12:19, 12.30s/it]"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Wed Nov 24 23:29:33 2021_lr_2e-05_val_acc_0.2153_ep_1.pt\n"]},{"name":"stderr","output_type":"stream","text":[" 44%|████▎     | 682/1563 [08:04<09:16,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 682/1563 of epoch 1 complete. Loss per last 992 samples:: 0.8921690910093246 \n","Training Accuracy per last 992 samples: 20.766129032258064\n"]},{"name":"stderr","output_type":"stream","text":[" 48%|████▊     | 744/1563 [08:43<08:37,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 744/1563 of epoch 1 complete. Loss per last 992 samples:: 0.8860958468529486 \n","Training Accuracy per last 992 samples: 21.975806451612904\n"]},{"name":"stderr","output_type":"stream","text":[" 52%|█████▏    | 806/1563 [09:22<07:58,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 806/1563 of epoch 1 complete. Loss per last 992 samples:: 0.8756868916173135 \n","Training Accuracy per last 992 samples: 23.68951612903226\n"]},{"name":"stderr","output_type":"stream","text":[" 56%|█████▌    | 868/1563 [10:01<07:31,  1.54it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 868/1563 of epoch 1 complete. Loss per last 992 samples:: 0.873778066327495 \n","Training Accuracy per last 992 samples: 25.302419354838708\n"]},{"name":"stderr","output_type":"stream","text":[" 60%|█████▉    | 930/1563 [10:42<06:41,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 930/1563 of epoch 1 complete. Loss per last 992 samples:: 0.8720855712890625 \n","Training Accuracy per last 992 samples: 25.100806451612904\n"]},{"name":"stderr","output_type":"stream","text":[" 63%|██████▎   | 992/1563 [11:22<06:00,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 992/1563 of epoch 1 complete. Loss per last 992 samples:: 0.8710652012978831 \n","Training Accuracy per last 992 samples: 23.487903225806452\n"]},{"name":"stderr","output_type":"stream","text":[" 67%|██████▋   | 1054/1563 [12:01<05:22,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1054/1563 of epoch 1 complete. Loss per last 992 samples:: 0.8707061275359123 \n","Training Accuracy per last 992 samples: 24.495967741935484\n"]},{"name":"stderr","output_type":"stream","text":[" 71%|███████▏  | 1116/1563 [12:40<04:42,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1116/1563 of epoch 1 complete. Loss per last 992 samples:: 0.8523943501134073 \n","Training Accuracy per last 992 samples: 29.33467741935484\n"]},{"name":"stderr","output_type":"stream","text":[" 75%|███████▌  | 1178/1563 [13:19<04:03,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1178/1563 of epoch 1 complete. Loss per last 992 samples:: 0.86222900882844 \n","Training Accuracy per last 992 samples: 26.108870967741936\n"]},{"name":"stderr","output_type":"stream","text":[" 79%|███████▉  | 1240/1563 [13:58<03:24,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1240/1563 of epoch 1 complete. Loss per last 992 samples:: 0.8570504957629789 \n","Training Accuracy per last 992 samples: 26.108870967741936\n"]},{"name":"stderr","output_type":"stream","text":[" 80%|███████▉  | 1249/1563 [14:04<03:17,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 1250 complete! Training Loss : 0.8843230895996094\n","Epoch 1, batch 1250 complete! Training Accuracy : 0.2186\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 174/174 [00:38<00:00,  4.56it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 1250 complete! Validation Loss : 1.6809274958468032\n","Epoch 1, batch 1250 complete! Validation Accuracy : 0.30417566594672424\n","Validation loss changed from 1.7738734456314438 to 1.6809274958468032\n","Best validation accuracy improved from 0.21526277897768178 to 0.30417566594672424\n","\n"]},{"name":"stderr","output_type":"stream","text":[" 80%|███████▉  | 1250/1563 [14:43<1:04:10, 12.30s/it]"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Wed Nov 24 23:36:48 2021_lr_2e-05_val_acc_0.3042_ep_1.pt\n"]},{"name":"stderr","output_type":"stream","text":[" 83%|████████▎ | 1290/1563 [15:09<02:53,  1.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 83%|████████▎ | 1302/1563 [15:16<02:45,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1302/1563 of epoch 1 complete. Loss per last 992 samples:: 0.8526414440524194 \n","Training Accuracy per last 992 samples: 28.225806451612904\n"]},{"name":"stderr","output_type":"stream","text":[" 85%|████████▌ | 1333/1563 [15:36<02:24,  1.59it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 87%|████████▋ | 1364/1563 [15:55<02:06,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1364/1563 of epoch 1 complete. Loss per last 992 samples:: 0.8346786499023438 \n","Training Accuracy per last 992 samples: 29.838709677419356\n"]},{"name":"stderr","output_type":"stream","text":[" 91%|█████████ | 1426/1563 [16:35<01:26,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1426/1563 of epoch 1 complete. Loss per last 992 samples:: 0.8506345441264491 \n","Training Accuracy per last 992 samples: 26.91532258064516\n"]},{"name":"stderr","output_type":"stream","text":[" 95%|█████████▌| 1488/1563 [17:14<00:47,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1488/1563 of epoch 1 complete. Loss per last 992 samples:: 0.847123299875567 \n","Training Accuracy per last 992 samples: 26.20967741935484\n"]},{"name":"stderr","output_type":"stream","text":[" 99%|█████████▉| 1550/1563 [17:53<00:08,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1550/1563 of epoch 1 complete. Loss per last 992 samples:: 0.813750482374622 \n","Training Accuracy per last 992 samples: 32.358870967741936\n"]},{"name":"stderr","output_type":"stream","text":["100%|█████████▉| 1562/1563 [18:00<00:00,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 1563 complete! Training Loss : 0.8752698355276312\n","Epoch 1, batch 1563 complete! Training Accuracy : 0.2324250790221262\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 174/174 [00:38<00:00,  4.55it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, batch 1563 complete! Validation Loss : 1.6280372005769577\n","Epoch 1, batch 1563 complete! Validation Accuracy : 0.3369330453563715\n","Validation loss changed from 1.6809274958468032 to 1.6280372005769577\n","Best validation accuracy improved from 0.30417566594672424 to 0.3369330453563715\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1563/1563 [18:39<00:00,  1.40it/s]\n"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Wed Nov 24 23:40:44 2021_lr_2e-05_val_acc_0.3369_ep_1.pt\n"]},{"name":"stderr","output_type":"stream","text":["  4%|▍         | 62/1563 [00:39<15:49,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 62/1563 of epoch 2 complete. Loss per last 992 samples:: 0.8078534526209677 \n","Training Accuracy per last 992 samples: 33.971774193548384\n"]},{"name":"stderr","output_type":"stream","text":["  8%|▊         | 124/1563 [01:18<15:10,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 124/1563 of epoch 2 complete. Loss per last 992 samples:: 0.8096378695580267 \n","Training Accuracy per last 992 samples: 34.17338709677419\n"]},{"name":"stderr","output_type":"stream","text":["  9%|▉         | 138/1563 [01:27<14:59,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 12%|█▏        | 186/1563 [01:57<14:32,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 186/1563 of epoch 2 complete. Loss per last 992 samples:: 0.8061800310688634 \n","Training Accuracy per last 992 samples: 34.778225806451616\n"]},{"name":"stderr","output_type":"stream","text":[" 13%|█▎        | 201/1563 [02:07<14:18,  1.59it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 16%|█▌        | 248/1563 [02:36<13:53,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 248/1563 of epoch 2 complete. Loss per last 992 samples:: 0.7881897957094254 \n","Training Accuracy per last 992 samples: 35.181451612903224\n"]},{"name":"stderr","output_type":"stream","text":[" 20%|█▉        | 310/1563 [03:15<13:11,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 310/1563 of epoch 2 complete. Loss per last 992 samples:: 0.8017726713611234 \n","Training Accuracy per last 992 samples: 33.064516129032256\n"]},{"name":"stderr","output_type":"stream","text":[" 24%|██▍       | 372/1563 [03:54<12:33,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 372/1563 of epoch 2 complete. Loss per last 992 samples:: 0.8028790873865927 \n","Training Accuracy per last 992 samples: 33.770161290322584\n"]},{"name":"stderr","output_type":"stream","text":[" 28%|██▊       | 434/1563 [04:34<11:51,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 434/1563 of epoch 2 complete. Loss per last 992 samples:: 0.7818398014191659 \n","Training Accuracy per last 992 samples: 35.98790322580645\n"]},{"name":"stderr","output_type":"stream","text":[" 32%|███▏      | 496/1563 [05:13<11:20,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 496/1563 of epoch 2 complete. Loss per last 992 samples:: 0.7809099689606698 \n","Training Accuracy per last 992 samples: 35.483870967741936\n"]},{"name":"stderr","output_type":"stream","text":[" 36%|███▌      | 558/1563 [05:52<10:34,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 558/1563 of epoch 2 complete. Loss per last 992 samples:: 0.7891100606610698 \n","Training Accuracy per last 992 samples: 34.67741935483871\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|███▉      | 620/1563 [06:31<09:57,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 620/1563 of epoch 2 complete. Loss per last 992 samples:: 0.7921706168882309 \n","Training Accuracy per last 992 samples: 37.903225806451616\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|███▉      | 624/1563 [06:34<09:59,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, batch 625 complete! Training Loss : 0.7959734130859375\n","Epoch 2, batch 625 complete! Training Accuracy : 0.3487\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 174/174 [00:38<00:00,  4.55it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, batch 625 complete! Validation Loss : 1.57643024400733\n","Epoch 2, batch 625 complete! Validation Accuracy : 0.3545716342692585\n","Validation loss changed from 1.6280372005769577 to 1.57643024400733\n","Best validation accuracy improved from 0.3369330453563715 to 0.3545716342692585\n","\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|███▉      | 625/1563 [07:14<3:12:36, 12.32s/it]"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Wed Nov 24 23:47:58 2021_lr_2e-05_val_acc_0.3546_ep_2.pt\n"]},{"name":"stderr","output_type":"stream","text":[" 44%|████▎     | 682/1563 [07:50<09:18,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 682/1563 of epoch 2 complete. Loss per last 992 samples:: 0.7881764750326833 \n","Training Accuracy per last 992 samples: 35.78629032258065\n"]},{"name":"stderr","output_type":"stream","text":[" 48%|████▊     | 744/1563 [08:29<08:36,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 744/1563 of epoch 2 complete. Loss per last 992 samples:: 0.7537336041850429 \n","Training Accuracy per last 992 samples: 40.725806451612904\n"]},{"name":"stderr","output_type":"stream","text":[" 49%|████▉     | 765/1563 [08:42<08:22,  1.59it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 52%|█████▏    | 806/1563 [09:08<08:00,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 806/1563 of epoch 2 complete. Loss per last 992 samples:: 0.7786685574439264 \n","Training Accuracy per last 992 samples: 35.38306451612903\n"]},{"name":"stderr","output_type":"stream","text":[" 56%|█████▌    | 868/1563 [09:47<07:19,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 868/1563 of epoch 2 complete. Loss per last 992 samples:: 0.7805727066532258 \n","Training Accuracy per last 992 samples: 36.99596774193548\n"]},{"name":"stderr","output_type":"stream","text":[" 60%|█████▉    | 930/1563 [10:26<06:40,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 930/1563 of epoch 2 complete. Loss per last 992 samples:: 0.7670226250925372 \n","Training Accuracy per last 992 samples: 39.516129032258064\n"]},{"name":"stderr","output_type":"stream","text":[" 63%|██████▎   | 992/1563 [11:05<06:00,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 992/1563 of epoch 2 complete. Loss per last 992 samples:: 0.7665022265526557 \n","Training Accuracy per last 992 samples: 38.40725806451613\n"]},{"name":"stderr","output_type":"stream","text":[" 67%|██████▋   | 1054/1563 [11:44<05:22,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1054/1563 of epoch 2 complete. Loss per last 992 samples:: 0.7538205731299615 \n","Training Accuracy per last 992 samples: 39.314516129032256\n"]},{"name":"stderr","output_type":"stream","text":[" 71%|███████▏  | 1116/1563 [12:24<04:42,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1116/1563 of epoch 2 complete. Loss per last 992 samples:: 0.7677207454558341 \n","Training Accuracy per last 992 samples: 38.104838709677416\n"]},{"name":"stderr","output_type":"stream","text":[" 75%|███████▌  | 1178/1563 [13:03<04:03,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1178/1563 of epoch 2 complete. Loss per last 992 samples:: 0.779358156265751 \n","Training Accuracy per last 992 samples: 36.49193548387097\n"]},{"name":"stderr","output_type":"stream","text":[" 79%|███████▉  | 1240/1563 [13:42<03:24,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1240/1563 of epoch 2 complete. Loss per last 992 samples:: 0.7556150344110304 \n","Training Accuracy per last 992 samples: 39.01209677419355\n"]},{"name":"stderr","output_type":"stream","text":[" 80%|███████▉  | 1249/1563 [13:47<03:18,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, batch 1250 complete! Training Loss : 0.7824455329895019\n","Epoch 2, batch 1250 complete! Training Accuracy : 0.36435\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 174/174 [00:38<00:00,  4.55it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, batch 1250 complete! Validation Loss : 1.5400352217685218\n","Epoch 2, batch 1250 complete! Validation Accuracy : 0.36285097192224625\n","Validation loss changed from 1.57643024400733 to 1.5400352217685218\n","Best validation accuracy improved from 0.3545716342692585 to 0.36285097192224625\n","\n"]},{"name":"stderr","output_type":"stream","text":[" 80%|███████▉  | 1250/1563 [14:27<1:04:18, 12.33s/it]"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Wed Nov 24 23:55:12 2021_lr_2e-05_val_acc_0.3629_ep_2.pt\n"]},{"name":"stderr","output_type":"stream","text":[" 83%|████████▎ | 1302/1563 [15:00<02:45,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1302/1563 of epoch 2 complete. Loss per last 992 samples:: 0.7614023454727665 \n","Training Accuracy per last 992 samples: 40.221774193548384\n"]},{"name":"stderr","output_type":"stream","text":[" 87%|████████▋ | 1364/1563 [15:39<02:05,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1364/1563 of epoch 2 complete. Loss per last 992 samples:: 0.7488531297253024 \n","Training Accuracy per last 992 samples: 40.42338709677419\n"]},{"name":"stderr","output_type":"stream","text":[" 91%|█████████ | 1426/1563 [16:18<01:26,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1426/1563 of epoch 2 complete. Loss per last 992 samples:: 0.7741169775685957 \n","Training Accuracy per last 992 samples: 37.19758064516129\n"]},{"name":"stderr","output_type":"stream","text":[" 93%|█████████▎| 1460/1563 [16:40<01:04,  1.59it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 95%|█████████▌| 1488/1563 [16:57<00:47,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1488/1563 of epoch 2 complete. Loss per last 992 samples:: 0.7606095652426442 \n","Training Accuracy per last 992 samples: 38.306451612903224\n"]},{"name":"stderr","output_type":"stream","text":[" 99%|█████████▉| 1550/1563 [17:37<00:08,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1550/1563 of epoch 2 complete. Loss per last 992 samples:: 0.7647923346488706 \n","Training Accuracy per last 992 samples: 37.80241935483871\n"]},{"name":"stderr","output_type":"stream","text":["100%|█████████▉| 1562/1563 [17:44<00:00,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, batch 1563 complete! Training Loss : 0.7780847128430621\n","Epoch 2, batch 1563 complete! Training Accuracy : 0.36938342735966073\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 174/174 [00:38<00:00,  4.55it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, batch 1563 complete! Validation Loss : 1.5354859431584675\n","Epoch 2, batch 1563 complete! Validation Accuracy : 0.37329013678905687\n","Validation loss changed from 1.5400352217685218 to 1.5354859431584675\n","Best validation accuracy improved from 0.36285097192224625 to 0.37329013678905687\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1563/1563 [18:23<00:00,  1.42it/s]\n"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Wed Nov 24 23:59:08 2021_lr_2e-05_val_acc_0.3733_ep_2.pt\n"]},{"name":"stderr","output_type":"stream","text":["  4%|▍         | 62/1563 [00:39<15:48,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 62/1563 of epoch 3 complete. Loss per last 992 samples:: 0.6825685039643319 \n","Training Accuracy per last 992 samples: 45.46370967741935\n"]},{"name":"stderr","output_type":"stream","text":["  8%|▊         | 124/1563 [01:18<15:09,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 124/1563 of epoch 3 complete. Loss per last 992 samples:: 0.6759223937988281 \n","Training Accuracy per last 992 samples: 46.37096774193548\n"]},{"name":"stderr","output_type":"stream","text":[" 12%|█▏        | 186/1563 [01:57<14:29,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 186/1563 of epoch 3 complete. Loss per last 992 samples:: 0.6751911563258017 \n","Training Accuracy per last 992 samples: 48.79032258064516\n"]},{"name":"stderr","output_type":"stream","text":[" 16%|█▌        | 248/1563 [02:36<13:51,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 248/1563 of epoch 3 complete. Loss per last 992 samples:: 0.6516752089223554 \n","Training Accuracy per last 992 samples: 51.310483870967744\n"]},{"name":"stderr","output_type":"stream","text":[" 20%|█▉        | 310/1563 [03:15<13:11,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 310/1563 of epoch 3 complete. Loss per last 992 samples:: 0.6781766491551553 \n","Training Accuracy per last 992 samples: 47.58064516129032\n"]},{"name":"stderr","output_type":"stream","text":[" 24%|██▍       | 372/1563 [03:54<12:33,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 372/1563 of epoch 3 complete. Loss per last 992 samples:: 0.6727600405293126 \n","Training Accuracy per last 992 samples: 47.681451612903224\n"]},{"name":"stderr","output_type":"stream","text":[" 27%|██▋       | 427/1563 [04:29<11:55,  1.59it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 28%|██▊       | 434/1563 [04:33<11:56,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 434/1563 of epoch 3 complete. Loss per last 992 samples:: 0.6803347218421197 \n","Training Accuracy per last 992 samples: 47.479838709677416\n"]},{"name":"stderr","output_type":"stream","text":[" 32%|███▏      | 496/1563 [05:13<11:16,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 496/1563 of epoch 3 complete. Loss per last 992 samples:: 0.6703915288371425 \n","Training Accuracy per last 992 samples: 48.79032258064516\n"]},{"name":"stderr","output_type":"stream","text":[" 36%|███▌      | 558/1563 [05:52<10:35,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 558/1563 of epoch 3 complete. Loss per last 992 samples:: 0.6720982674629458 \n","Training Accuracy per last 992 samples: 46.67338709677419\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|███▉      | 620/1563 [06:31<09:55,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 620/1563 of epoch 3 complete. Loss per last 992 samples:: 0.6618139205440399 \n","Training Accuracy per last 992 samples: 48.58870967741935\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|███▉      | 624/1563 [06:33<09:55,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 3, batch 625 complete! Training Loss : 0.6721667770385742\n","Epoch 3, batch 625 complete! Training Accuracy : 0.4783\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 174/174 [00:38<00:00,  4.56it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3, batch 625 complete! Validation Loss : 1.5608694676695198\n","Epoch 3, batch 625 complete! Validation Accuracy : 0.3804895608351332\n","Validation loss changed from 1.5354859431584675 to 1.5608694676695198\n","Best validation accuracy improved from 0.37329013678905687 to 0.3804895608351332\n","\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|███▉      | 625/1563 [07:13<3:12:44, 12.33s/it]"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Thu Nov 25 00:06:21 2021_lr_2e-05_val_acc_0.3805_ep_3.pt\n"]},{"name":"stderr","output_type":"stream","text":[" 44%|████▎     | 682/1563 [07:49<09:16,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 682/1563 of epoch 3 complete. Loss per last 992 samples:: 0.6685508989518688 \n","Training Accuracy per last 992 samples: 48.185483870967744\n"]},{"name":"stderr","output_type":"stream","text":[" 48%|████▊     | 744/1563 [08:28<08:38,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 744/1563 of epoch 3 complete. Loss per last 992 samples:: 0.6534895896911621 \n","Training Accuracy per last 992 samples: 47.983870967741936\n"]},{"name":"stderr","output_type":"stream","text":[" 52%|█████▏    | 806/1563 [09:07<07:58,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 806/1563 of epoch 3 complete. Loss per last 992 samples:: 0.6842802416893744 \n","Training Accuracy per last 992 samples: 46.471774193548384\n"]},{"name":"stderr","output_type":"stream","text":[" 56%|█████▌    | 868/1563 [09:47<07:20,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 868/1563 of epoch 3 complete. Loss per last 992 samples:: 0.6765977259605161 \n","Training Accuracy per last 992 samples: 45.46370967741935\n"]},{"name":"stderr","output_type":"stream","text":[" 60%|█████▉    | 930/1563 [10:26<06:42,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 930/1563 of epoch 3 complete. Loss per last 992 samples:: 0.6835372217239872 \n","Training Accuracy per last 992 samples: 45.766129032258064\n"]},{"name":"stderr","output_type":"stream","text":[" 60%|█████▉    | 931/1563 [10:26<06:41,  1.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 63%|██████▎   | 992/1563 [11:05<06:02,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 992/1563 of epoch 3 complete. Loss per last 992 samples:: 0.6742876498929916 \n","Training Accuracy per last 992 samples: 48.99193548387097\n"]},{"name":"stderr","output_type":"stream","text":[" 67%|██████▋   | 1054/1563 [11:45<05:23,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1054/1563 of epoch 3 complete. Loss per last 992 samples:: 0.6710429806863109 \n","Training Accuracy per last 992 samples: 46.37096774193548\n"]},{"name":"stderr","output_type":"stream","text":[" 71%|███████▏  | 1116/1563 [12:24<04:47,  1.55it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1116/1563 of epoch 3 complete. Loss per last 992 samples:: 0.6879153866921702 \n","Training Accuracy per last 992 samples: 47.07661290322581\n"]},{"name":"stderr","output_type":"stream","text":[" 75%|███████▌  | 1178/1563 [13:04<04:10,  1.54it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1178/1563 of epoch 3 complete. Loss per last 992 samples:: 0.6649544392862627 \n","Training Accuracy per last 992 samples: 50.403225806451616\n"]},{"name":"stderr","output_type":"stream","text":[" 77%|███████▋  | 1203/1563 [13:20<03:48,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 79%|███████▉  | 1240/1563 [13:43<03:25,  1.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1240/1563 of epoch 3 complete. Loss per last 992 samples:: 0.6748255837348199 \n","Training Accuracy per last 992 samples: 47.78225806451613\n"]},{"name":"stderr","output_type":"stream","text":[" 80%|███████▉  | 1249/1563 [13:49<03:19,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 3, batch 1250 complete! Training Loss : 0.6733337162017822\n","Epoch 3, batch 1250 complete! Training Accuracy : 0.47655\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 174/174 [00:38<00:00,  4.52it/s]\n"," 80%|███████▉  | 1250/1563 [14:28<1:03:37, 12.20s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 3, batch 1250 complete! Validation Loss : 1.5664778817659137\n","Epoch 3, batch 1250 complete! Validation Accuracy : 0.3696904247660187\n"]},{"name":"stderr","output_type":"stream","text":[" 83%|████████▎ | 1302/1563 [15:01<02:46,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1302/1563 of epoch 3 complete. Loss per last 992 samples:: 0.6740102921762774 \n","Training Accuracy per last 992 samples: 47.479838709677416\n"]},{"name":"stderr","output_type":"stream","text":[" 87%|████████▋ | 1364/1563 [15:41<02:06,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1364/1563 of epoch 3 complete. Loss per last 992 samples:: 0.6539135440703361 \n","Training Accuracy per last 992 samples: 49.596774193548384\n"]},{"name":"stderr","output_type":"stream","text":[" 91%|█████████ | 1426/1563 [16:20<01:26,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1426/1563 of epoch 3 complete. Loss per last 992 samples:: 0.6623226288826235 \n","Training Accuracy per last 992 samples: 49.193548387096776\n"]},{"name":"stderr","output_type":"stream","text":[" 95%|█████████▌| 1488/1563 [16:59<00:47,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1488/1563 of epoch 3 complete. Loss per last 992 samples:: 0.682469775599818 \n","Training Accuracy per last 992 samples: 46.068548387096776\n"]},{"name":"stderr","output_type":"stream","text":[" 99%|█████████▉| 1550/1563 [17:39<00:08,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1550/1563 of epoch 3 complete. Loss per last 992 samples:: 0.695514602045859 \n","Training Accuracy per last 992 samples: 43.44758064516129\n"]},{"name":"stderr","output_type":"stream","text":["100%|█████████▉| 1562/1563 [17:46<00:00,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 3, batch 1563 complete! Training Loss : 0.6729513828524091\n","Epoch 3, batch 1563 complete! Training Accuracy : 0.4755331492817989\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 174/174 [00:38<00:00,  4.54it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3, batch 1563 complete! Validation Loss : 1.5438008157686256\n","Epoch 3, batch 1563 complete! Validation Accuracy : 0.3930885529157667\n","Validation loss changed from 1.5608694676695198 to 1.5438008157686256\n","Best validation accuracy improved from 0.3804895608351332 to 0.3930885529157667\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1563/1563 [18:25<00:00,  1.41it/s]\n"]},{"name":"stdout","output_type":"stream","text":["The model has been saved in models/Thu Nov 25 00:17:34 2021_lr_2e-05_val_acc_0.3931_ep_3.pt\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 1/1563 [00:00<18:52,  1.38it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","  3%|▎         | 45/1563 [00:28<16:24,  1.54it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","  4%|▍         | 62/1563 [00:39<16:00,  1.56it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 62/1563 of epoch 4 complete. Loss per last 992 samples:: 0.5344237204520933 \n","Training Accuracy per last 992 samples: 61.08870967741935\n"]},{"name":"stderr","output_type":"stream","text":["  8%|▊         | 124/1563 [01:18<15:16,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 124/1563 of epoch 4 complete. Loss per last 992 samples:: 0.5134004008385443 \n","Training Accuracy per last 992 samples: 60.98790322580645\n"]},{"name":"stderr","output_type":"stream","text":[" 12%|█▏        | 186/1563 [01:58<14:38,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 186/1563 of epoch 4 complete. Loss per last 992 samples:: 0.5185778448658604 \n","Training Accuracy per last 992 samples: 62.29838709677419\n"]},{"name":"stderr","output_type":"stream","text":[" 16%|█▌        | 248/1563 [02:37<13:57,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 248/1563 of epoch 4 complete. Loss per last 992 samples:: 0.5098814195202243 \n","Training Accuracy per last 992 samples: 61.59274193548387\n"]},{"name":"stderr","output_type":"stream","text":[" 17%|█▋        | 263/1563 [02:47<13:43,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 20%|█▉        | 310/1563 [03:17<13:18,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 310/1563 of epoch 4 complete. Loss per last 992 samples:: 0.5058845935329315 \n","Training Accuracy per last 992 samples: 61.391129032258064\n"]},{"name":"stderr","output_type":"stream","text":[" 24%|██▍       | 372/1563 [03:57<12:40,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 372/1563 of epoch 4 complete. Loss per last 992 samples:: 0.5206207229245093 \n","Training Accuracy per last 992 samples: 61.391129032258064\n"]},{"name":"stderr","output_type":"stream","text":[" 28%|██▊       | 434/1563 [04:36<12:01,  1.56it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 434/1563 of epoch 4 complete. Loss per last 992 samples:: 0.5093132757371471 \n","Training Accuracy per last 992 samples: 62.70161290322581\n"]},{"name":"stderr","output_type":"stream","text":[" 32%|███▏      | 496/1563 [05:15<11:19,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 496/1563 of epoch 4 complete. Loss per last 992 samples:: 0.5088711323276642 \n","Training Accuracy per last 992 samples: 62.399193548387096\n"]},{"name":"stderr","output_type":"stream","text":[" 36%|███▌      | 558/1563 [05:55<10:39,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 558/1563 of epoch 4 complete. Loss per last 992 samples:: 0.5183439639306837 \n","Training Accuracy per last 992 samples: 60.38306451612903\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|███▉      | 620/1563 [06:34<10:01,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 620/1563 of epoch 4 complete. Loss per last 992 samples:: 0.5141992415151289 \n","Training Accuracy per last 992 samples: 61.99596774193548\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|███▉      | 624/1563 [06:37<10:03,  1.56it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 4, batch 625 complete! Training Loss : 0.5151514812469482\n","Epoch 4, batch 625 complete! Training Accuracy : 0.6167\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 174/174 [00:38<00:00,  4.48it/s]\n"," 40%|███▉      | 625/1563 [07:16<3:12:00, 12.28s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 4, batch 625 complete! Validation Loss : 1.7097248928300266\n","Epoch 4, batch 625 complete! Validation Accuracy : 0.37688984881209503\n"]},{"name":"stderr","output_type":"stream","text":[" 44%|████▎     | 682/1563 [07:53<09:20,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 682/1563 of epoch 4 complete. Loss per last 992 samples:: 0.5164726357306203 \n","Training Accuracy per last 992 samples: 61.29032258064516\n"]},{"name":"stderr","output_type":"stream","text":[" 45%|████▍     | 702/1563 [08:06<09:05,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 48%|████▊     | 744/1563 [08:32<08:38,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 744/1563 of epoch 4 complete. Loss per last 992 samples:: 0.5200393469102921 \n","Training Accuracy per last 992 samples: 62.19758064516129\n"]},{"name":"stderr","output_type":"stream","text":[" 52%|█████▏    | 806/1563 [09:11<07:59,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 806/1563 of epoch 4 complete. Loss per last 992 samples:: 0.5438617044879545 \n","Training Accuracy per last 992 samples: 59.475806451612904\n"]},{"name":"stderr","output_type":"stream","text":[" 56%|█████▌    | 868/1563 [09:51<07:22,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 868/1563 of epoch 4 complete. Loss per last 992 samples:: 0.5232942219703428 \n","Training Accuracy per last 992 samples: 60.58467741935484\n"]},{"name":"stderr","output_type":"stream","text":[" 60%|█████▉    | 930/1563 [10:30<06:43,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 930/1563 of epoch 4 complete. Loss per last 992 samples:: 0.5032825316152265 \n","Training Accuracy per last 992 samples: 61.99596774193548\n"]},{"name":"stderr","output_type":"stream","text":[" 63%|██████▎   | 992/1563 [11:09<06:03,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 992/1563 of epoch 4 complete. Loss per last 992 samples:: 0.5462102543923163 \n","Training Accuracy per last 992 samples: 58.770161290322584\n"]},{"name":"stderr","output_type":"stream","text":[" 67%|██████▋   | 1054/1563 [11:49<05:24,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1054/1563 of epoch 4 complete. Loss per last 992 samples:: 0.5545949243730114 \n","Training Accuracy per last 992 samples: 58.568548387096776\n"]},{"name":"stderr","output_type":"stream","text":[" 71%|███████▏  | 1116/1563 [12:28<04:44,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1116/1563 of epoch 4 complete. Loss per last 992 samples:: 0.5180700171378351 \n","Training Accuracy per last 992 samples: 61.59274193548387\n"]},{"name":"stderr","output_type":"stream","text":[" 75%|███████▌  | 1178/1563 [13:08<04:07,  1.56it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1178/1563 of epoch 4 complete. Loss per last 992 samples:: 0.5270636812333138 \n","Training Accuracy per last 992 samples: 59.778225806451616\n"]},{"name":"stderr","output_type":"stream","text":[" 79%|███████▉  | 1240/1563 [13:47<03:25,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1240/1563 of epoch 4 complete. Loss per last 992 samples:: 0.5302376631767519 \n","Training Accuracy per last 992 samples: 59.07258064516129\n"]},{"name":"stderr","output_type":"stream","text":[" 80%|███████▉  | 1249/1563 [13:53<03:18,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 4, batch 1250 complete! Training Loss : 0.5215600643157959\n","Epoch 4, batch 1250 complete! Training Accuracy : 0.61\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 174/174 [00:38<00:00,  4.52it/s]\n"," 80%|███████▉  | 1250/1563 [14:32<1:03:36, 12.19s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 4, batch 1250 complete! Validation Loss : 1.70805256257112\n","Epoch 4, batch 1250 complete! Validation Accuracy : 0.3804895608351332\n"]},{"name":"stderr","output_type":"stream","text":[" 83%|████████▎ | 1302/1563 [15:05<02:46,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1302/1563 of epoch 4 complete. Loss per last 992 samples:: 0.5342865951599614 \n","Training Accuracy per last 992 samples: 61.391129032258064\n"]},{"name":"stderr","output_type":"stream","text":[" 87%|████████▋ | 1364/1563 [15:45<02:06,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1364/1563 of epoch 4 complete. Loss per last 992 samples:: 0.5321467076578448 \n","Training Accuracy per last 992 samples: 59.67741935483871\n"]},{"name":"stderr","output_type":"stream","text":[" 91%|█████████ | 1426/1563 [16:24<01:27,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1426/1563 of epoch 4 complete. Loss per last 992 samples:: 0.5034539141962605 \n","Training Accuracy per last 992 samples: 61.79435483870968\n"]},{"name":"stderr","output_type":"stream","text":[" 95%|█████████▌| 1488/1563 [17:03<00:47,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1488/1563 of epoch 4 complete. Loss per last 992 samples:: 0.5417321055166183 \n","Training Accuracy per last 992 samples: 60.483870967741936\n"]},{"name":"stderr","output_type":"stream","text":[" 99%|█████████▉| 1550/1563 [17:43<00:08,  1.56it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1550/1563 of epoch 4 complete. Loss per last 992 samples:: 0.5453714843719236 \n","Training Accuracy per last 992 samples: 58.66935483870968\n"]},{"name":"stderr","output_type":"stream","text":["100%|█████████▉| 1562/1563 [17:50<00:00,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 4, batch 1563 complete! Training Loss : 0.5236004481160023\n","Epoch 4, batch 1563 complete! Training Accuracy : 0.60869043332133\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 174/174 [00:38<00:00,  4.52it/s]\n","100%|██████████| 1563/1563 [18:29<00:00,  1.41it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4, batch 1563 complete! Validation Loss : 1.7014721782728173\n","Epoch 4, batch 1563 complete! Validation Accuracy : 0.3804895608351332\n"]},{"name":"stderr","output_type":"stream","text":["  4%|▍         | 62/1563 [00:40<15:57,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 62/1563 of epoch 5 complete. Loss per last 992 samples:: 0.3376353036972784 \n","Training Accuracy per last 992 samples: 77.31854838709677\n"]},{"name":"stderr","output_type":"stream","text":["  8%|▊         | 124/1563 [01:19<15:14,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 124/1563 of epoch 5 complete. Loss per last 992 samples:: 0.3257069472343691 \n","Training Accuracy per last 992 samples: 77.31854838709677\n"]},{"name":"stderr","output_type":"stream","text":[" 12%|█▏        | 186/1563 [01:58<14:33,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 186/1563 of epoch 5 complete. Loss per last 992 samples:: 0.36724968302634453 \n","Training Accuracy per last 992 samples: 73.79032258064517\n"]},{"name":"stderr","output_type":"stream","text":[" 15%|█▍        | 231/1563 [02:27<14:04,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 16%|█▌        | 248/1563 [02:38<13:55,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 248/1563 of epoch 5 complete. Loss per last 992 samples:: 0.3524282017061787 \n","Training Accuracy per last 992 samples: 74.8991935483871\n"]},{"name":"stderr","output_type":"stream","text":[" 20%|█▉        | 310/1563 [03:17<13:20,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 310/1563 of epoch 5 complete. Loss per last 992 samples:: 0.33469848382857537 \n","Training Accuracy per last 992 samples: 78.125\n"]},{"name":"stderr","output_type":"stream","text":[" 24%|██▍       | 372/1563 [03:56<12:32,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 372/1563 of epoch 5 complete. Loss per last 992 samples:: 0.3605522955617597 \n","Training Accuracy per last 992 samples: 74.79838709677419\n"]},{"name":"stderr","output_type":"stream","text":[" 28%|██▊       | 434/1563 [04:36<11:55,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 434/1563 of epoch 5 complete. Loss per last 992 samples:: 0.3437589003193763 \n","Training Accuracy per last 992 samples: 75.80645161290323\n"]},{"name":"stderr","output_type":"stream","text":[" 32%|███▏      | 496/1563 [05:15<11:15,  1.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 496/1563 of epoch 5 complete. Loss per last 992 samples:: 0.3379732899127468 \n","Training Accuracy per last 992 samples: 74.49596774193549\n"]},{"name":"stderr","output_type":"stream","text":[" 36%|███▌      | 558/1563 [05:54<10:40,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 558/1563 of epoch 5 complete. Loss per last 992 samples:: 0.34491656672570015 \n","Training Accuracy per last 992 samples: 74.49596774193549\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|███▉      | 620/1563 [06:33<10:00,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 620/1563 of epoch 5 complete. Loss per last 992 samples:: 0.3490165404735073 \n","Training Accuracy per last 992 samples: 75.60483870967742\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|███▉      | 624/1563 [06:36<10:00,  1.56it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 5, batch 625 complete! Training Loss : 0.34588309478759766\n","Epoch 5, batch 625 complete! Training Accuracy : 0.7561\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 174/174 [00:38<00:00,  4.52it/s]\n"," 40%|███▉      | 625/1563 [07:15<3:10:26, 12.18s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 5, batch 625 complete! Validation Loss : 1.9962554471246128\n","Epoch 5, batch 625 complete! Validation Accuracy : 0.37329013678905687\n"]},{"name":"stderr","output_type":"stream","text":[" 44%|████▎     | 682/1563 [07:51<09:21,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 682/1563 of epoch 5 complete. Loss per last 992 samples:: 0.36432146929925485 \n","Training Accuracy per last 992 samples: 73.58870967741936\n"]},{"name":"stderr","output_type":"stream","text":[" 48%|████▊     | 744/1563 [08:31<08:41,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 744/1563 of epoch 5 complete. Loss per last 992 samples:: 0.3534245010345213 \n","Training Accuracy per last 992 samples: 73.99193548387096\n"]},{"name":"stderr","output_type":"stream","text":[" 52%|█████▏    | 806/1563 [09:10<08:02,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 806/1563 of epoch 5 complete. Loss per last 992 samples:: 0.36217169800112325 \n","Training Accuracy per last 992 samples: 73.58870967741936\n"]},{"name":"stderr","output_type":"stream","text":[" 56%|█████▌    | 868/1563 [09:50<07:22,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 868/1563 of epoch 5 complete. Loss per last 992 samples:: 0.33570841050917105 \n","Training Accuracy per last 992 samples: 74.6975806451613\n"]},{"name":"stderr","output_type":"stream","text":[" 60%|█████▉    | 930/1563 [10:29<06:43,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 930/1563 of epoch 5 complete. Loss per last 992 samples:: 0.3696897077944971 \n","Training Accuracy per last 992 samples: 72.68145161290323\n"]},{"name":"stderr","output_type":"stream","text":[" 63%|██████▎   | 992/1563 [11:08<06:04,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 992/1563 of epoch 5 complete. Loss per last 992 samples:: 0.3597254954999493 \n","Training Accuracy per last 992 samples: 74.59677419354838\n"]},{"name":"stderr","output_type":"stream","text":[" 67%|██████▋   | 1054/1563 [11:48<05:23,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1054/1563 of epoch 5 complete. Loss per last 992 samples:: 0.3668857514858246 \n","Training Accuracy per last 992 samples: 74.19354838709677\n"]},{"name":"stderr","output_type":"stream","text":[" 71%|███████▏  | 1114/1563 [12:26<04:45,  1.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 71%|███████▏  | 1116/1563 [12:27<04:47,  1.55it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1116/1563 of epoch 5 complete. Loss per last 992 samples:: 0.3576075598116844 \n","Training Accuracy per last 992 samples: 73.99193548387096\n"]},{"name":"stderr","output_type":"stream","text":[" 73%|███████▎  | 1141/1563 [12:43<04:27,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 75%|███████▌  | 1178/1563 [13:07<04:05,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1178/1563 of epoch 5 complete. Loss per last 992 samples:: 0.35787985113359266 \n","Training Accuracy per last 992 samples: 73.48790322580645\n"]},{"name":"stderr","output_type":"stream","text":[" 79%|███████▉  | 1240/1563 [13:46<03:25,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1240/1563 of epoch 5 complete. Loss per last 992 samples:: 0.36496986256491754 \n","Training Accuracy per last 992 samples: 72.98387096774194\n"]},{"name":"stderr","output_type":"stream","text":[" 80%|███████▉  | 1249/1563 [13:52<03:19,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 5, batch 1250 complete! Training Loss : 0.3523771286249161\n","Epoch 5, batch 1250 complete! Training Accuracy : 0.7472\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 174/174 [00:38<00:00,  4.53it/s]\n"," 80%|███████▉  | 1250/1563 [14:31<1:03:30, 12.17s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 5, batch 1250 complete! Validation Loss : 2.040891580883114\n","Epoch 5, batch 1250 complete! Validation Accuracy : 0.3675305975521958\n"]},{"name":"stderr","output_type":"stream","text":[" 83%|████████▎ | 1302/1563 [15:04<02:46,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1302/1563 of epoch 5 complete. Loss per last 992 samples:: 0.35971549730147084 \n","Training Accuracy per last 992 samples: 72.68145161290323\n"]},{"name":"stderr","output_type":"stream","text":[" 87%|████████▋ | 1364/1563 [15:43<02:07,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1364/1563 of epoch 5 complete. Loss per last 992 samples:: 0.34919326439980536 \n","Training Accuracy per last 992 samples: 74.19354838709677\n"]},{"name":"stderr","output_type":"stream","text":[" 91%|█████████ | 1426/1563 [16:23<01:27,  1.56it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1426/1563 of epoch 5 complete. Loss per last 992 samples:: 0.3601541898904308 \n","Training Accuracy per last 992 samples: 73.58870967741936\n"]},{"name":"stderr","output_type":"stream","text":[" 95%|█████████▍| 1481/1563 [16:58<00:51,  1.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"," 95%|█████████▌| 1488/1563 [17:02<00:47,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1488/1563 of epoch 5 complete. Loss per last 992 samples:: 0.33938891753073663 \n","Training Accuracy per last 992 samples: 74.6975806451613\n"]},{"name":"stderr","output_type":"stream","text":[" 99%|█████████▉| 1550/1563 [17:42<00:08,  1.56it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Batch 1550/1563 of epoch 5 complete. Loss per last 992 samples:: 0.35248026636339 \n","Training Accuracy per last 992 samples: 73.99193548387096\n"]},{"name":"stderr","output_type":"stream","text":["100%|█████████▉| 1562/1563 [17:49<00:00,  1.57it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 5, batch 1563 complete! Training Loss : 0.35191390614286877\n","Epoch 5, batch 1563 complete! Training Accuracy : 0.7455287480494538\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 174/174 [00:38<00:00,  4.52it/s]\n","100%|██████████| 1563/1563 [18:28<00:00,  1.41it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 5, batch 1563 complete! Validation Loss : 2.06698082712875\n","Epoch 5, batch 1563 complete! Validation Accuracy : 0.3675305975521958\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Training perfomance during usual train-eval on hold-out loop\n","train_bert(net, criterion, opti, LEARNING_RATE, lr_scheduler, train_loader, val_loader, EPOCHS, iters_to_accumulate)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11448,"status":"ok","timestamp":1635932769348,"user":{"displayName":"Andrew Argatkiny","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04422905123959661601"},"user_tz":-420},"id":"zYNBGqTwpoHn","outputId":"74b194fd-8d8b-4c9f-a35c-9d95859f2c6d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Gen RAM Free: 11.1 GB  | Proc size: 5.1 GB\n","GPU RAM Free: 10137MB | Used: 1304MB | Util  11% | Total 11441MB\n"]}],"source":["# Check that we are using 100% of GPU memory footprint support libraries/code\n","# from https://github.com/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb\n","!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n","!pip -q install gputil\n","!pip -q install psutil\n","!pip -q install humanize\n","import psutil\n","import humanize\n","import os\n","import GPUtil as GPU\n","GPUs = GPU.getGPUs()\n","# XXX: only one GPU on Colab and isn’t guaranteed\n","gpu = GPUs[0]\n","def printm():\n"," process = psutil.Process(os.getpid())\n"," print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n"," print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n","printm()\n"," #!kill -9 -1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":452,"status":"ok","timestamp":1635932801898,"user":{"displayName":"Andrew Argatkiny","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04422905123959661601"},"user_tz":-420},"id":"e9gyehyY6n8C","outputId":"0e7574a4-b8d7-44fa-8f06-717912d52d5a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Gen RAM Free: 11.1 GB  | Proc size: 5.1 GB\n","GPU RAM Free: 10137MB | Used: 1304MB | Util  11% | Total 11441MB\n"]}],"source":["printm()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v1kSrbCvMNDf"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNFzSvllZcK0G034IIhIOS7","collapsed_sections":[],"mount_file_id":"144h_eAZbCarOnrPb3zxIWY7HYm27TGhi","name":"FriendsPredictNLP","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0344eb1638a44fc0bd8be3d2b5aafc99":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2cbc04f6abc4b45b5374f424dd88efa","placeholder":"​","style":"IPY_MODEL_91647e75ea5e4c16a11046fda73ba026","value":" 521/521 [00:00&lt;00:00, 12.4kB/s]"}},"0f3925eaafb9438ab0f6b27c356cfe99":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_82b0759acdb845ea9859d2f96ce0e5d3","placeholder":"​","style":"IPY_MODEL_487d23c4e2b64adda4a945df313d1ed1","value":" 1.70M/1.70M [00:00&lt;00:00, 7.10MB/s]"}},"121075183d2441ddb2e99f849d2e9686":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f4f04df6382427288c4e9422e4fbf7d","placeholder":"​","style":"IPY_MODEL_e3aeb80a52bf4ccdb69330b3ebce4355","value":" 683M/683M [00:23&lt;00:00, 32.0MB/s]"}},"158f6d85beec4b33bdc422ee9e115808":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2e6a2c5306764e31bea3a9e37121ff8c","placeholder":"​","style":"IPY_MODEL_faaf710ce2cc471c9370811df2f54ba6","value":" 655/655 [00:00&lt;00:00, 15.7kB/s]"}},"192f33925dd045d792b38a2444794a10":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"21bf688381ef4a8495a3a6981a84ef8d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_939389df612a4f23b8ab42d6dccd6c8d","max":323,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6c7f0e73512743efb870026d6677531e","value":323}},"269978945c9c48d280cd1eb1ddf2f7fc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d185bee7972497296fc31ba5c0d54d6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e6a2c5306764e31bea3a9e37121ff8c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"317c759beaf6494db054eae9162ccfb2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32acd792c0bf45419a87561dc4ccdb4c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d8a373f8b1a4da3841d55ed5fd62935":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b968066f8f924c309860b111ae0816a4","IPY_MODEL_98ff0c5dbe8049c588ef631dfbc08ef8","IPY_MODEL_121075183d2441ddb2e99f849d2e9686"],"layout":"IPY_MODEL_5435b27f9fab4fcd9f0cf0fe1f11db71"}},"4718ae66701049229c66c8506b3efe1b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_65cbb712ea2f4dab90ba6600d8440c0b","placeholder":"​","style":"IPY_MODEL_d70b870ad108499e8651b57c9e9be34b","value":"Downloading: 100%"}},"487d23c4e2b64adda4a945df313d1ed1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4e0b4d1769974ba88e49d4fb02e2c4a2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"52e9ba2a4d974b15ae4b6ab1232dc256":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5435b27f9fab4fcd9f0cf0fe1f11db71":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54feb1dd180f4bf8b47c18713336d41c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58564c36a43e4f3ab99a0e153cb5e480":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"58c863f270ac42c3b195987575a8bc9a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5e3622f69800454d9704f414226d5869":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5fa6528c3aa5487e87edb8797ebf465e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"632a7169fbd74159b11d92ab6cbacfa7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6502e372ae624b618f6248ee2ea778b6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65cbb712ea2f4dab90ba6600d8440c0b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c7b0a0b13b3450fa83c21af7c8525a2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c7f0e73512743efb870026d6677531e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6cd8338fff6a4f469d417911e8b8142b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_54feb1dd180f4bf8b47c18713336d41c","placeholder":"​","style":"IPY_MODEL_7148c15a118641d3a9488fd981cc3bf3","value":"Downloading: 100%"}},"7148c15a118641d3a9488fd981cc3bf3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"717056921033432aa1612afdd7aab763":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d900d48dde840e0824e409582df0dd2","placeholder":"​","style":"IPY_MODEL_58564c36a43e4f3ab99a0e153cb5e480","value":"Downloading: 100%"}},"73c9f7d056704fd2ac03cb76d6f36df5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d83773323bd4e90bc86003c4220a662","placeholder":"​","style":"IPY_MODEL_df27936694364ed583b70df9a0870a08","value":"Downloading: 100%"}},"769e20765ef64788b8de218a9853c99e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"77b7765f0fe04c8a976230fb2eb01065":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"784667efc3604b52a7a3e90afeeba909":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79f29a0c5f01482dbbb7700bd7b573a7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4718ae66701049229c66c8506b3efe1b","IPY_MODEL_bddc64019a20420589041d59a8ac1e86","IPY_MODEL_158f6d85beec4b33bdc422ee9e115808"],"layout":"IPY_MODEL_d469fb245e2244848f21d12c9897e0e8"}},"7d83773323bd4e90bc86003c4220a662":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f4f04df6382427288c4e9422e4fbf7d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82b0759acdb845ea9859d2f96ce0e5d3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"830958dc67714696a237fc7a051931bc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"85aa7a96758841a3bb30895cf30d9912":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88b743167cd44b318de6a3998f466556":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d900d48dde840e0824e409582df0dd2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e60d43dcd7e4df9a554b7b8ed28c56d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a7c1b532fc64473eab127d4f73c4ffd7","placeholder":"​","style":"IPY_MODEL_192f33925dd045d792b38a2444794a10","value":"Downloading: 100%"}},"91647e75ea5e4c16a11046fda73ba026":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"939389df612a4f23b8ab42d6dccd6c8d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"972de0d98911484dad5f2c7b560d973c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_269978945c9c48d280cd1eb1ddf2f7fc","max":521,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f543e115e78f40f7bc9813a76b5ba862","value":521}},"98ff0c5dbe8049c588ef631dfbc08ef8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e55762599f664a0fa5288de998af11c1","max":716133354,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ef630b6fc27b4750839918ef7ac13c6c","value":716133354}},"a56c41e3f2744ba6b2f5cd408a86e6d7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8e60d43dcd7e4df9a554b7b8ed28c56d","IPY_MODEL_ebbe8ae171e94413be8fcc944c05b28b","IPY_MODEL_c280900ff8ba4cc8a2dbf30daad33a4b"],"layout":"IPY_MODEL_d15f42cd6e9d427c99f12b45ba2b04e1"}},"a7c1b532fc64473eab127d4f73c4ffd7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a825221a71434b82be79352ad867c68a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a89df2c0f77a445eb9a12550fcaed50d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6cd8338fff6a4f469d417911e8b8142b","IPY_MODEL_edd582160bd349c68d35b5461b9771c2","IPY_MODEL_f1868ee80eff44af8596b13875f7cddd"],"layout":"IPY_MODEL_ee730afc554f486591437c1c9de33490"}},"adef346525b5428cb77ce85b67b7de02":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f32062e1b8f74b2ba3ebb65244b8345d","IPY_MODEL_21bf688381ef4a8495a3a6981a84ef8d","IPY_MODEL_d601368d185f4c2f8de3b222976c42ef"],"layout":"IPY_MODEL_632a7169fbd74159b11d92ab6cbacfa7"}},"b968066f8f924c309860b111ae0816a4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_88b743167cd44b318de6a3998f466556","placeholder":"​","style":"IPY_MODEL_77b7765f0fe04c8a976230fb2eb01065","value":"Downloading: 100%"}},"b9b60187d34644bda94b224017bc450d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bddc64019a20420589041d59a8ac1e86":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d185bee7972497296fc31ba5c0d54d6","max":655,"min":0,"orientation":"horizontal","style":"IPY_MODEL_830958dc67714696a237fc7a051931bc","value":655}},"c280900ff8ba4cc8a2dbf30daad33a4b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_317c759beaf6494db054eae9162ccfb2","placeholder":"​","style":"IPY_MODEL_4e0b4d1769974ba88e49d4fb02e2c4a2","value":" 1.70M/1.70M [00:00&lt;00:00, 3.07MB/s]"}},"d15f42cd6e9d427c99f12b45ba2b04e1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3ce46e504bd4daf9ac3763a7cdc222f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d469fb245e2244848f21d12c9897e0e8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5ed4335e6bb418aaebd5316b616f54c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d601368d185f4c2f8de3b222976c42ef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_32acd792c0bf45419a87561dc4ccdb4c","placeholder":"​","style":"IPY_MODEL_769e20765ef64788b8de218a9853c99e","value":" 323/323 [00:00&lt;00:00, 7.17kB/s]"}},"d70b870ad108499e8651b57c9e9be34b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"df27936694364ed583b70df9a0870a08":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e2cbc04f6abc4b45b5374f424dd88efa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3aeb80a52bf4ccdb69330b3ebce4355":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e55762599f664a0fa5288de998af11c1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e87825600783430391ed6af6be32c278":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_73c9f7d056704fd2ac03cb76d6f36df5","IPY_MODEL_972de0d98911484dad5f2c7b560d973c","IPY_MODEL_0344eb1638a44fc0bd8be3d2b5aafc99"],"layout":"IPY_MODEL_b9b60187d34644bda94b224017bc450d"}},"e9d9038ad82440aebdb0fb714add647d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_717056921033432aa1612afdd7aab763","IPY_MODEL_f010001c717b439797f738bb4353b74d","IPY_MODEL_0f3925eaafb9438ab0f6b27c356cfe99"],"layout":"IPY_MODEL_85aa7a96758841a3bb30895cf30d9912"}},"ebbe8ae171e94413be8fcc944c05b28b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6502e372ae624b618f6248ee2ea778b6","max":1780720,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d5ed4335e6bb418aaebd5316b616f54c","value":1780720}},"edd582160bd349c68d35b5461b9771c2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_784667efc3604b52a7a3e90afeeba909","max":112,"min":0,"orientation":"horizontal","style":"IPY_MODEL_58c863f270ac42c3b195987575a8bc9a","value":112}},"ee730afc554f486591437c1c9de33490":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef630b6fc27b4750839918ef7ac13c6c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f010001c717b439797f738bb4353b74d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d3ce46e504bd4daf9ac3763a7cdc222f","max":1780720,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5fa6528c3aa5487e87edb8797ebf465e","value":1780720}},"f1868ee80eff44af8596b13875f7cddd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c7b0a0b13b3450fa83c21af7c8525a2","placeholder":"​","style":"IPY_MODEL_5e3622f69800454d9704f414226d5869","value":" 112/112 [00:00&lt;00:00, 2.29kB/s]"}},"f32062e1b8f74b2ba3ebb65244b8345d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_52e9ba2a4d974b15ae4b6ab1232dc256","placeholder":"​","style":"IPY_MODEL_a825221a71434b82be79352ad867c68a","value":"Downloading: 100%"}},"f543e115e78f40f7bc9813a76b5ba862":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"faaf710ce2cc471c9370811df2f54ba6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
